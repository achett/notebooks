{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Path_seg_paper_code_replication.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "#!pip install tensorflow-gpu"
      ],
      "metadata": {
        "id": "9xGcENyoHGoD"
      },
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#!pip install --upgrade https://storage.googleapis.com/tensorflow/mac/cpu/tensorflow-1.12.0-py3-none-any.whl"
      ],
      "metadata": {
        "id": "64ku1lcQIFDn"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#!pip install tensorflow --upgrade"
      ],
      "metadata": {
        "id": "00UpEw9iL8iz"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#!pip install tensorflow-hub"
      ],
      "metadata": {
        "id": "2MC8OYGyKHIa"
      },
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install tf-slim"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vFvLp5I7UDT0",
        "outputId": "b427a6fc-d93e-4e39-efd0-4cef106038ec"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting tf-slim\n",
            "  Downloading tf_slim-1.1.0-py2.py3-none-any.whl (352 kB)\n",
            "\u001b[?25l\r\u001b[K     |█                               | 10 kB 17.0 MB/s eta 0:00:01\r\u001b[K     |█▉                              | 20 kB 22.0 MB/s eta 0:00:01\r\u001b[K     |██▉                             | 30 kB 14.8 MB/s eta 0:00:01\r\u001b[K     |███▊                            | 40 kB 10.9 MB/s eta 0:00:01\r\u001b[K     |████▋                           | 51 kB 3.9 MB/s eta 0:00:01\r\u001b[K     |█████▋                          | 61 kB 4.6 MB/s eta 0:00:01\r\u001b[K     |██████▌                         | 71 kB 4.8 MB/s eta 0:00:01\r\u001b[K     |███████▌                        | 81 kB 4.6 MB/s eta 0:00:01\r\u001b[K     |████████▍                       | 92 kB 5.1 MB/s eta 0:00:01\r\u001b[K     |█████████▎                      | 102 kB 4.4 MB/s eta 0:00:01\r\u001b[K     |██████████▎                     | 112 kB 4.4 MB/s eta 0:00:01\r\u001b[K     |███████████▏                    | 122 kB 4.4 MB/s eta 0:00:01\r\u001b[K     |████████████                    | 133 kB 4.4 MB/s eta 0:00:01\r\u001b[K     |█████████████                   | 143 kB 4.4 MB/s eta 0:00:01\r\u001b[K     |██████████████                  | 153 kB 4.4 MB/s eta 0:00:01\r\u001b[K     |███████████████                 | 163 kB 4.4 MB/s eta 0:00:01\r\u001b[K     |███████████████▉                | 174 kB 4.4 MB/s eta 0:00:01\r\u001b[K     |████████████████▊               | 184 kB 4.4 MB/s eta 0:00:01\r\u001b[K     |█████████████████▊              | 194 kB 4.4 MB/s eta 0:00:01\r\u001b[K     |██████████████████▋             | 204 kB 4.4 MB/s eta 0:00:01\r\u001b[K     |███████████████████▌            | 215 kB 4.4 MB/s eta 0:00:01\r\u001b[K     |████████████████████▌           | 225 kB 4.4 MB/s eta 0:00:01\r\u001b[K     |█████████████████████▍          | 235 kB 4.4 MB/s eta 0:00:01\r\u001b[K     |██████████████████████▍         | 245 kB 4.4 MB/s eta 0:00:01\r\u001b[K     |███████████████████████▎        | 256 kB 4.4 MB/s eta 0:00:01\r\u001b[K     |████████████████████████▏       | 266 kB 4.4 MB/s eta 0:00:01\r\u001b[K     |█████████████████████████▏      | 276 kB 4.4 MB/s eta 0:00:01\r\u001b[K     |██████████████████████████      | 286 kB 4.4 MB/s eta 0:00:01\r\u001b[K     |███████████████████████████     | 296 kB 4.4 MB/s eta 0:00:01\r\u001b[K     |████████████████████████████    | 307 kB 4.4 MB/s eta 0:00:01\r\u001b[K     |████████████████████████████▉   | 317 kB 4.4 MB/s eta 0:00:01\r\u001b[K     |█████████████████████████████▉  | 327 kB 4.4 MB/s eta 0:00:01\r\u001b[K     |██████████████████████████████▊ | 337 kB 4.4 MB/s eta 0:00:01\r\u001b[K     |███████████████████████████████▋| 348 kB 4.4 MB/s eta 0:00:01\r\u001b[K     |████████████████████████████████| 352 kB 4.4 MB/s \n",
            "\u001b[?25hRequirement already satisfied: absl-py>=0.2.2 in /usr/local/lib/python3.7/dist-packages (from tf-slim) (1.0.0)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.7/dist-packages (from absl-py>=0.2.2->tf-slim) (1.15.0)\n",
            "Installing collected packages: tf-slim\n",
            "Successfully installed tf-slim-1.1.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "19n_APMrFSyW"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "#import tensorflow as tf\n",
        "import tensorflow.compat.v1 as tf\n",
        "tf.disable_v2_behavior()\n",
        "import random\n",
        "import os, sys\n",
        "import tf_slim\n",
        "\n",
        "import warnings\n",
        "warnings.filterwarnings(\"ignore\")\n",
        "\n",
        "from tensorflow.python.ops.rnn import _transpose_batch_time\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "#performance metrics\n",
        "from sklearn.metrics import roc_auc_score, average_precision_score\n",
        "from sklearn.metrics import normalized_mutual_info_score, homogeneity_score, adjusted_rand_score\n",
        "from sklearn.metrics.cluster import contingency_matrix"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#user defined\n",
        "from utils_log import save_logging, load_logging\n",
        "from data_loader import import_data\n",
        "from class_AC_TPC import AC_TPC, initialize_embedding"
      ],
      "metadata": {
        "id": "fcPorzxNFa09"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def f_get_minibatch(mb_size, x, y):\n",
        "    idx = range(np.shape(x)[0])\n",
        "    idx = random.sample(idx, mb_size)\n",
        "\n",
        "    x_mb   = x[idx].astype(float)    \n",
        "    y_mb   = y[idx].astype(float)    \n",
        "\n",
        "    return x_mb, y_mb\n",
        "\n",
        "### PERFORMANCE METRICS:\n",
        "def f_get_prediction_scores(y_true_, y_pred_):\n",
        "    if np.sum(y_true_) == 0: #no label for running roc_auc_curves\n",
        "        auroc_ = -1.\n",
        "        auprc_ = -1.\n",
        "    else:\n",
        "        auroc_ = roc_auc_score(y_true_, y_pred_)\n",
        "        auprc_ = average_precision_score(y_true_, y_pred_)\n",
        "    return (auroc_, auprc_)\n",
        "\n",
        "\n",
        "def purity_score(y_true, y_pred):\n",
        "    # compute contingency matrix (also called confusion matrix)\n",
        "    c_matrix = contingency_matrix(y_true, y_pred)\n",
        "    # return purity\n",
        "    return np.sum(np.amax(c_matrix, axis=0)) / np.sum(c_matrix)"
      ],
      "metadata": {
        "id": "DrNh1z6HTTVl"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "data_name = 'sample'\n",
        "data_x, data_y, y_type = import_data(data_name = data_name)"
      ],
      "metadata": {
        "id": "snnt08CKM3Wv"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "seed = 1234\n",
        "\n",
        "tr_data_x,te_data_x, tr_data_y,te_data_y = train_test_split(\n",
        "    data_x, data_y, test_size=0.2, random_state=seed\n",
        ")\n",
        "\n",
        "tr_data_x,va_data_x, tr_data_y,va_data_y = train_test_split(\n",
        "    tr_data_x, tr_data_y, test_size=0.2, random_state=seed\n",
        ")"
      ],
      "metadata": {
        "id": "OC-FkT3cJiYV"
      },
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "data_x.shape"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "W1o9QkUtlqsG",
        "outputId": "70bc66f6-997f-4695-c8cc-a3ca4fa45e80"
      },
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(1346, 23, 35)"
            ]
          },
          "metadata": {},
          "execution_count": 17
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "K = 6\n",
        "\n",
        "h_dim_FC   = 50 #for fully_connected layers\n",
        "h_dim_RNN  = 50\n",
        "\n",
        "x_dim = np.shape(data_x)[2]\n",
        "y_dim = np.shape(data_y)[2]\n",
        "  \n",
        "    \n",
        "num_layer_encoder    = 1\n",
        "num_layer_selector   = 2\n",
        "num_layer_predictor  = 2\n",
        "\n",
        "z_dim = h_dim_RNN * num_layer_encoder\n",
        "\n",
        "max_length = np.shape(data_x)[1]\n",
        "\n",
        "rnn_type          = 'LSTM' #GRU, LSTM\n",
        "\n",
        "\n",
        "input_dims ={\n",
        "    'x_dim': x_dim,\n",
        "    'y_dim': y_dim,\n",
        "    'y_type': y_type,\n",
        "    'max_cluster': K,\n",
        "    'max_length': max_length    \n",
        "}\n",
        "\n",
        "network_settings ={\n",
        "    'h_dim_encoder': h_dim_RNN,\n",
        "    'num_layers_encoder': num_layer_encoder,\n",
        "    'rnn_type': rnn_type,\n",
        "    'rnn_activate_fn': tf.nn.tanh,\n",
        "\n",
        "    'h_dim_selector': h_dim_FC,\n",
        "    'num_layers_selector': num_layer_selector,\n",
        "    \n",
        "    'h_dim_predictor': h_dim_FC,\n",
        "    'num_layers_predictor': num_layer_predictor,\n",
        "    \n",
        "    'fc_activate_fn': tf.nn.relu\n",
        "}"
      ],
      "metadata": {
        "id": "k82CWNWLTP_-"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "lr_rate    = 0.001\n",
        "keep_prob  = 0.7\n",
        "mb_size    = 128\n",
        "\n",
        "#ITERATION  = 10000\n",
        "ITERATION  = 5000\n",
        "check_step = 1000\n",
        "\n",
        "save_path = './{}/proposed/init/'.format(data_name)\n",
        "\n",
        "if not os.path.exists(save_path + '/models/'):\n",
        "    os.makedirs(save_path + '/models/')"
      ],
      "metadata": {
        "id": "wgDHyylUTVxr"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def log(x): \n",
        "    return tf.log(x + 1e-8)\n",
        "\n",
        "def div(x, y):\n",
        "    return tf.div(x, (y + 1e-8))\n",
        "\n",
        "def get_seq_length(sequence):\n",
        "    used = tf.sign(tf.reduce_max(tf.abs(sequence), 2))\n",
        "    tmp_length = tf.reduce_sum(used, 1)\n",
        "    tmp_length = tf.cast(tmp_length, tf.int32)\n",
        "    return tmp_length"
      ],
      "metadata": {
        "id": "n0aJzRUmMuRv"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#from tensorflow.contrib.layers import fully_connected as FC_Net\n",
        "from tf_slim.layers import fully_connected as FC_Net\n",
        "\n",
        "### CONSTRUCT MULTICELL FOR MULTI-LAYER RNNS\n",
        "def create_rnn_cell(num_units, num_layers, keep_prob, RNN_type, activation_fn): \n",
        "    '''\n",
        "        GOAL         : create multi-cell (including a single cell) to construct multi-layer RNN\n",
        "        num_units    : number of units in each layer\n",
        "        num_layers   : number of layers in MulticellRNN\n",
        "        keep_prob    : keep probabilty [0, 1]  (if None, dropout is not employed)\n",
        "        RNN_type     : either 'LSTM' or 'GRU'\n",
        "    '''\n",
        "    if activation_fn == 'None':\n",
        "        activation_fn = tf.nn.tanh\n",
        "\n",
        "    cells = []\n",
        "    for _ in range(num_layers):\n",
        "        if RNN_type == 'GRU':\n",
        "            cell = tf.nn.rnn_cell.GRUCell(num_units, activation=activation_fn)\n",
        "        elif RNN_type == 'LSTM':\n",
        "            cell = tf.nn.rnn_cell.LSTMCell(num_units, activation=activation_fn, state_is_tuple=True)\n",
        "            # cell = tf.contrib.rnn.LSTMCell(num_units, activation=activation_fn)\n",
        "        else:\n",
        "        \tprint('ERROR: WRONG RNN TYPE')\n",
        "        if not keep_prob is None:\n",
        "            cell = tf.nn.rnn_cell.DropoutWrapper(cell, input_keep_prob=keep_prob, output_keep_prob=keep_prob) # state_keep_prob=keep_prob\n",
        "        cells.append(cell)\n",
        "    cell = tf.nn.rnn_cell.MultiRNNCell(cells)\n",
        "    \n",
        "    return cell\n",
        "\n",
        "\n",
        "### EXTRACT STATE OUTPUT OF MULTICELL-RNNS\n",
        "def create_concat_state_h(state, num_layers, RNN_type, BiRNN=None):\n",
        "    '''\n",
        "        GOAL\t     : concatenate the tuple-type tensor (state) into a single tensor\n",
        "        state        : input state is a tuple ofo MulticellRNN (i.e. output of MulticellRNN)\n",
        "                       consist of only hidden states h for GRU and hidden states c and h for LSTM\n",
        "        num_layers   : number of layers in MulticellRNN\n",
        "        RNN_type     : either 'LSTM' or 'GRU'\n",
        "    '''\n",
        "    for i in range(num_layers):\n",
        "        if BiRNN != None:\n",
        "            if RNN_type == 'LSTM':\n",
        "                tmp = tf.concat([state[0][i][1], state[1][i][1]], axis=1) ## i-th layer, h state for LSTM\n",
        "            elif RNN_type == 'GRU':\n",
        "                tmp = tf.concat([state[0][i], state[1][i]], axis=1) ## i-th layer, h state for GRU\n",
        "            else:\n",
        "                print('ERROR: WRONG RNN CELL TYPE')\n",
        "        else:\n",
        "            if RNN_type == 'LSTM':\n",
        "                tmp = state[i][1] ## i-th layer, h state for LSTM\n",
        "            elif RNN_type == 'GRU':\n",
        "                tmp = state[i] ## i-th layer, h state for GRU\n",
        "            else:\n",
        "                print('ERROR: WRONG RNN CELL TYPE')\n",
        "\n",
        "        if i == 0:\n",
        "            rnn_state_out = tmp\n",
        "        else:\n",
        "            rnn_state_out = tf.concat([rnn_state_out, tmp], axis = 1)\n",
        "    \n",
        "    return rnn_state_out\n",
        "\n",
        "\n",
        "def create_concat_state_c(state, num_layers, RNN_type, BiRNN=None):\n",
        "    for i in range(num_layers):\n",
        "        if BiRNN != None:\n",
        "            if RNN_type == 'LSTM':\n",
        "                tmp = tf.concat([state[0][i][0], state[1][i][0]], axis=1) ## i-th layer, c state for LSTM\n",
        "            elif RNN_type == 'GRU':\n",
        "                tmp = tf.concat([state[0][i], state[1][i]], axis=1) ## i-th layer, c=h state for GRU\n",
        "            else:\n",
        "                print('ERROR: WRONG RNN CELL TYPE')\n",
        "        else:\n",
        "            if RNN_type == 'LSTM':\n",
        "                tmp = state[i][0] ## i-th layer, c state for LSTM\n",
        "            elif RNN_type == 'GRU':\n",
        "                tmp = state[i] ## i-th layer, h state for GRU\n",
        "            else:\n",
        "                print('ERROR: WRONG RNN CELL TYPE')\n",
        "\n",
        "        if i == 0:\n",
        "            rnn_state_out = tmp\n",
        "        else:\n",
        "            rnn_state_out = tf.concat([rnn_state_out, tmp], axis = 1)\n",
        "    \n",
        "    return rnn_state_out\n",
        "\n",
        "### FEEDFORWARD NETWORK\n",
        "def create_FCNet(inputs, num_layers, h_dim, h_fn, o_dim, o_fn, w_init, w_reg=None, keep_prob=1.0):\n",
        "    '''\n",
        "        GOAL             : Create FC network with different specifications \n",
        "        inputs (tensor)  : input tensor\n",
        "        num_layers       : number of layers in FCNet\n",
        "        h_dim  (int)     : number of hidden units\n",
        "        h_fn             : activation function for hidden layers (default: tf.nn.relu)\n",
        "        o_dim  (int)     : number of output units\n",
        "        o_fn             : activation function for output layers (defalut: None)\n",
        "        w_init           : initialization for weight matrix (defalut: Xavier)\n",
        "        keep_prob        : keep probabilty [0, 1]  (if None, dropout is not employed)\n",
        "    '''\n",
        "    # default active functions (hidden: relu, out: None)\n",
        "    if h_fn is None:\n",
        "        h_fn = tf.nn.relu\n",
        "    if o_fn is None:\n",
        "        o_fn = None\n",
        "\n",
        "    # default initialization functions (weight: Xavier, bias: None)\n",
        "    if w_init is None:\n",
        "        w_init = tf.contrib.layers.xavier_initializer() # Xavier initialization\n",
        "\n",
        "    for layer in range(num_layers):\n",
        "        if num_layers == 1:\n",
        "            out = FC_Net(inputs, o_dim, activation_fn=o_fn, weights_initializer=w_init, weights_regularizer=w_reg)\n",
        "        else:\n",
        "            if layer == 0:\n",
        "                h = FC_Net(inputs, h_dim, activation_fn=h_fn, weights_initializer=w_init, weights_regularizer=w_reg)\n",
        "                if not keep_prob is None:\n",
        "                    h = tf.nn.dropout(h, keep_prob=keep_prob)\n",
        "\n",
        "            elif layer > 0 and layer != (num_layers-1): # layer > 0:\n",
        "                h = FC_Net(h, h_dim, activation_fn=h_fn, weights_initializer=w_init, weights_regularizer=w_reg)\n",
        "                if not keep_prob is None:\n",
        "                    h = tf.nn.dropout(h, keep_prob=keep_prob)\n",
        "\n",
        "            else: # layer == num_layers-1 (the last layer)\n",
        "                out = FC_Net(h, o_dim, activation_fn=o_fn, weights_initializer=w_init, weights_regularizer=w_reg)\n",
        "\n",
        "    return out\n"
      ],
      "metadata": {
        "id": "X-tsbM4gdGzy"
      },
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class AC_TPC:\n",
        "    def __init__(self, sess, name, input_dims, network_settings):\n",
        "        self.sess               = sess\n",
        "        self.name               = name\n",
        "        \n",
        "        # INPUT/OUTPUT DIMENSIONS\n",
        "        self.x_dim           = input_dims['x_dim'] #features + delta\n",
        "        self.y_dim           = input_dims['y_dim']\n",
        "        self.y_type          = input_dims['y_type']\n",
        "        self.K               = input_dims['max_cluster']\n",
        "        self.max_length      = input_dims['max_length']\n",
        "\n",
        "        # Encoder\n",
        "        self.h_dim_f         = network_settings['h_dim_encoder'] #encoder nodes\n",
        "        self.num_layers_f    = network_settings['num_layers_encoder'] #encoder layers\n",
        "        self.rnn_type        = network_settings['rnn_type']\n",
        "        self.rnn_activate_fn   = network_settings['rnn_activate_fn']\n",
        "\n",
        "        # Selector\n",
        "        self.h_dim_h         = network_settings['h_dim_selector'] #selector nodes\n",
        "        self.num_layers_h    = network_settings['num_layers_selector'] #selector layers\n",
        "        \n",
        "        # Predictor\n",
        "        self.h_dim_g         = network_settings['h_dim_predictor'] #predictor nodes\n",
        "        self.num_layers_g    = network_settings['num_layers_predictor'] #predictor layers\n",
        "        \n",
        "        self.fc_activate_fn  = network_settings['fc_activate_fn'] #selector & predictor\n",
        "        \n",
        "        # Latent Space\n",
        "        self.z_dim           = self.h_dim_f * self.num_layers_f\n",
        "\n",
        "        self._build_net()\n",
        "\n",
        "    def _build_net(self):\n",
        "        with tf.compat.v1.variable_scope(self.name):            \n",
        "            self.mb_size     = tf.placeholder(tf.int32, [], name='batch_size')\n",
        "            self.lr_rate1    = tf.placeholder(tf.float32, name='learning_rate1')\n",
        "            self.lr_rate2    = tf.placeholder(tf.float32, name='learning_rate2')\n",
        "            self.keep_prob   = tf.placeholder(tf.float32, name='keep_probability')\n",
        "\n",
        "            # Input and Output\n",
        "            self.x          = tf.placeholder(tf.float32, [None, self.max_length, self.x_dim], name='inputs')\n",
        "            self.y          = tf.placeholder(tf.float32, [None, self.max_length, self.y_dim], name='labels_onehot')\n",
        "            \n",
        "            # Embedding\n",
        "            self.E          = tf.placeholder(tf.float32, [self.K, self.z_dim], name='embeddings_input')\n",
        "            self.EE         = tf.Variable(self.E, name='embeddings_var')\n",
        "            self.embeddings = tf.nn.tanh(self.EE)\n",
        "\n",
        "            # self.embde         = tf.nn.tanh(self.EE)\n",
        "            # self.EE         = tf.Variable(self.E, name='embeddings_var')\n",
        "            \n",
        "            self.s          = tf.placeholder(tf.int32, [None], name='cluster_label')\n",
        "            self.s_onehot   = tf.one_hot(self.s, self.K)\n",
        "\n",
        "            # LOSS PARAMETERS\n",
        "            self.alpha      = tf.placeholder(tf.float32, name = 'alpha') #For sample-wise entropy\n",
        "            self.beta       = tf.placeholder(tf.float32, name = 'beta')  #For prediction loss (i.e., mle)\n",
        "            self.gamma      = tf.placeholder(tf.float32, name = 'gamma') #For batch-wise entropy\n",
        "            self.delta      = tf.placeholder(tf.float32, name = 'delta') #For embedding\n",
        "\n",
        "            '''\n",
        "                ### CREATE RNN MASK\n",
        "                    - This is to flexibly handle sequences with different length\n",
        "                    - rnn_mask1: last observation; [mb_size, max_length]\n",
        "                    - rnn_mask2: all available observations; [mb_size, max_length]\n",
        "            '''\n",
        "            # CREATE RNN MASK:            \n",
        "            seq_length     = get_seq_length(self.x)\n",
        "            tmp_range      = tf.expand_dims(tf.range(0, self.max_length, 1), axis=0)\n",
        "            self.rnn_mask1 = tf.cast(tf.equal(tmp_range, tf.expand_dims(seq_length-1, axis=1)), tf.float32) #last observation\n",
        "            self.rnn_mask2 = tf.cast(tf.less_equal(tmp_range, tf.expand_dims(seq_length-1, axis=1)), tf.float32) #all available observation\n",
        "            \n",
        "            \n",
        "            ### DEFINE SELECTOR\n",
        "            def selector(x_, o_dim_=self.K, num_layers_=2, h_dim_=self.h_dim_h, activation_fn=self.fc_activate_fn, reuse=tf.AUTO_REUSE):\n",
        "                out_fn = tf.nn.softmax\n",
        "                with tf.compat.v1.variable_scope('selector', reuse=reuse):\n",
        "                    if num_layers_ == 1:\n",
        "                        out =  tf_slim.layers.fully_connected(inputs=x_, num_outputs=o_dim_, activation_fn=out_fn, scope='selector_out')\n",
        "                    else: #num_layers > 1\n",
        "                        for tmp_layer in range(num_layers_-1):\n",
        "                            if tmp_layer == 0:\n",
        "                                net = x_\n",
        "                            net = tf_slim.layers.fully_connected(inputs=net, num_outputs=h_dim_, activation_fn=activation_fn, scope='selector_'+str(tmp_layer))\n",
        "                            net = tf.nn.dropout(net, keep_prob=self.keep_prob)\n",
        "                        out =  tf_slim.layers.fully_connected(inputs=net, num_outputs=o_dim_, activation_fn=out_fn, scope='selector_out')  \n",
        "                return out\n",
        "            \n",
        "            \n",
        "            ### DEFINE PREDICTOR\n",
        "            def predictor(x_, o_dim_=self.y_dim, o_type_=self.y_type, num_layers_=1, h_dim_=self.h_dim_g, activation_fn=self.fc_activate_fn, reuse=tf.AUTO_REUSE):\n",
        "                if o_type_ == 'continuous':\n",
        "                    out_fn = None\n",
        "                elif o_type_ == 'categorical':\n",
        "                    out_fn = tf.nn.softmax #for classification task\n",
        "                elif o_type_ == 'binary':\n",
        "                    out_fn = tf.nn.sigmoid\n",
        "                else:\n",
        "                    raise Exception('Wrong output type. The value {}!!'.format(o_type_))\n",
        "                    \n",
        "                with tf.compat.v1.variable_scope('predictor', reuse=reuse):\n",
        "                    if num_layers_ == 1:\n",
        "                        out =  tf.contrib.layers.fully_connected(inputs=x_, num_outputs=o_dim_, activation_fn=out_fn, scope='predictor_out')\n",
        "                    else: #num_layers > 1\n",
        "                        for tmp_layer in range(num_layers_-1):\n",
        "                            if tmp_layer == 0:\n",
        "                                net = x_\n",
        "                            net = tf_slim.layers.fully_connected(inputs=net, num_outputs=h_dim_, activation_fn=activation_fn, scope='predictor_'+str(tmp_layer))\n",
        "                            net = tf.nn.dropout(net, keep_prob=self.keep_prob)\n",
        "                        out =  tf_slim.layers.fully_connected(inputs=net, num_outputs=o_dim_, activation_fn=out_fn, scope='predictor_out')  \n",
        "                return out\n",
        "\n",
        "            \n",
        "            ### DEFINE LOOP FUNCTION FOR ENCODRER (f-g, f-h relations are created here)\n",
        "            def loop_fn(time, cell_output, cell_state, loop_state):\n",
        "                \n",
        "                emit_output = cell_output \n",
        "\n",
        "                if cell_output is None:  # time == 0\n",
        "                    next_cell_state = cell.zero_state(self.mb_size, tf.float32)\n",
        "                    next_loop_state = loop_state_ta\n",
        "                else:\n",
        "                    next_cell_state = cell_state\n",
        "                    tmp_z  = create_concat_state_h(next_cell_state, self.num_layers_f, self.rnn_type)      \n",
        "                    tmp_y  = predictor(tmp_z, self.y_dim, self.y_type, self.num_layers_g, self.h_dim_g, self.fc_activate_fn)        \n",
        "                    tmp_pi = selector(tmp_z, self.K, self.num_layers_h, self.h_dim_h, self.fc_activate_fn)\n",
        "\n",
        "                    next_loop_state = (loop_state[0].write(time-1, tmp_z),  # save all the hidden states\n",
        "                                       loop_state[1].write(time-1, tmp_y),  # save all the output\n",
        "                                       loop_state[2].write(time-1, tmp_pi)) # save all the selector_net output (i.e., pi)\n",
        "\n",
        "                elements_finished = (time >= self.max_length)\n",
        "\n",
        "                #this gives the break-point (no more recurrence after the max_length)\n",
        "                finished = tf.reduce_all(elements_finished)    \n",
        "                next_input = tf.cond(finished, \n",
        "                                     lambda: tf.zeros([self.mb_size, self.x_dim], dtype=tf.float32),  \n",
        "                                     lambda: inputs_ta.read(time))\n",
        "                return (elements_finished, next_input, next_cell_state, emit_output, next_loop_state)\n",
        "\n",
        "            \n",
        "            '''\n",
        "                ##### CREATE RNN NETWORK\n",
        "                    - (INPUT)  inputs_ta: TensorArray with [max_length, mb_size, x_dim] #x_dim included delta\n",
        "                    - (OUTPUT) \n",
        "                        . zs     = rnn states (h) in LSTM/GRU             ; [mb_size, max_length z_dim]\n",
        "                        . y_hats = output of predictor taking zs as inputs; [mb_size, max_length, y_dim]\n",
        "                        . pis    = output of selector                     ; [mb_size, max_length, K]\n",
        "\n",
        "            '''\n",
        "            inputs    = self.x\n",
        "            inputs_ta = tf.TensorArray(\n",
        "                dtype=tf.float32, size=self.max_length\n",
        "            ).unstack(_transpose_batch_time(inputs), name = 'rnn_input')\n",
        "\n",
        "\n",
        "            cell = create_rnn_cell(\n",
        "                self.h_dim_f, self.num_layers_f, \n",
        "                self.keep_prob, self.rnn_type, self.rnn_activate_fn\n",
        "            )\n",
        "\n",
        "            #define the loop_state TensorArray for information from rnn time steps\n",
        "            loop_state_ta = (\n",
        "                tf.TensorArray(size=self.max_length, dtype=tf.float32, clear_after_read=False),  #zs (j=1,...,J)\n",
        "                tf.TensorArray(size=self.max_length, dtype=tf.float32, clear_after_read=False),  #y_hats (j=1,...,J)\n",
        "                tf.TensorArray(size=self.max_length, dtype=tf.float32, clear_after_read=False)   #pis (j=1,...,J)\n",
        "            )  \n",
        "\n",
        "            _, _, loop_state_ta = tf.nn.raw_rnn(cell, loop_fn) #, parallel_iterations=1)\n",
        "\n",
        "\n",
        "            self.zs         = _transpose_batch_time(loop_state_ta[0].stack())\n",
        "            self.y_hats     = _transpose_batch_time(loop_state_ta[1].stack())\n",
        "            self.pis        = _transpose_batch_time(loop_state_ta[2].stack())\n",
        "\n",
        "            ### SAMPLING PROCESS\n",
        "            s_dist          = tf.distributions.Categorical(probs=tf.reshape(self.pis, [-1, self.K])) #define the categorical dist.\n",
        "            s_sample        = s_dist.sample()\n",
        "\n",
        "            mask_e          = tf.cast(tf.equal(tf.expand_dims(tf.range(0, self.K, 1), axis=0), tf.expand_dims(s_sample, axis=1)), tf.float32)\n",
        "            z_bars          = tf.matmul(mask_e, self.embeddings)\n",
        "            pi_sample       = tf.reduce_sum(mask_e * tf.reshape(log(self.pis), [-1, self.K]), axis=1)\n",
        "\n",
        "            with tf.compat.v1.variable_scope('rnn', reuse=True):\n",
        "                y_bars   = predictor(z_bars, self.y_dim, self.y_type, self.num_layers_g, self.h_dim_g, self.fc_activate_fn)\n",
        "\n",
        "            self.z_bars    = tf.reshape(z_bars, [-1, self.max_length, self.z_dim])\n",
        "            self.y_bars    = tf.reshape(y_bars, [-1, self.max_length, self.y_dim])\n",
        "            self.pi_sample = tf.reshape(pi_sample, [-1, self.max_length])\n",
        "            self.s_sample  = tf.reshape(s_sample, [-1, self.max_length])\n",
        "\n",
        "            \n",
        "            ### DEFINE LOSS FUNCTIONS\n",
        "            #\\ell_{1}: KL divergence loss for regression and binary/categorical-classification task\n",
        "            def loss_1(y_true_, y_pred_, y_type_ = self.y_type):                \n",
        "                if y_type_ == 'continuous':\n",
        "                    tmp_loss = tf.reduce_sum((y_true_ - y_pred_)**2, axis=-1)\n",
        "                elif y_type_ == 'categorical':\n",
        "                    tmp_loss = - tf.reduce_sum(y_true_ * log(y_pred_), axis=-1)\n",
        "                elif y_type_ == 'binary':\n",
        "                    tmp_loss = - tf.reduce_sum(y_true_ * log(y_pred_) + (1.-y_true_) * log(1.-y_pred_), axis=-1)\n",
        "                else:\n",
        "                    raise Exception('Wrong output type. The value {}!!'.format(y_type_))                    \n",
        "                return tmp_loss\n",
        "\n",
        "            #batch-wise entropy\n",
        "            tmp_pis   = tf.tile(tf.expand_dims(self.rnn_mask2, axis=2), [1,1,self.K]) * self.pis\n",
        "            mean_pis  = tf.reduce_sum(tf.reduce_sum(tmp_pis, axis=1), axis=0) / tf.reduce_sum(tf.reduce_sum(self.rnn_mask2, axis=1), axis=0, keepdims=True)\n",
        "\n",
        "            ## LOSS_MLE: MLE prediction loss (for initalization)\n",
        "            self.LOSS_MLE   = tf.reduce_mean(tf.reduce_sum(self.rnn_mask2 * loss_1(self.y, self.y_hats, self.y_type), axis=1))\n",
        "            \n",
        "            \n",
        "            ## LOSS1: predictive clustering loss\n",
        "            self.LOSS_1     = tf.reduce_mean(tf.reduce_sum(self.rnn_mask2 * loss_1(self.y, self.y_bars, self.y_type), axis=1))\n",
        "            self.LOSS_1_AC  = tf.reduce_mean(tf.reduce_sum(self.rnn_mask2 * self.pi_sample * loss_1(self.y, self.y_bars, self.y_type), axis=1))\n",
        "\n",
        "            ## LOSS2: sample-wise entropy loss\n",
        "            self.LOSS_2     = tf.reduce_mean(-tf.reduce_sum(self.rnn_mask2 * tf.reduce_sum(self.pis * log(self.pis), axis=2), axis=1))\n",
        "            \n",
        "            \n",
        "            predictor_vars  = tf.get_collection(tf.GraphKeys.GLOBAL_VARIABLES, scope= self.name+'/rnn/predictor')\n",
        "            selecter_vars   = tf.get_collection(tf.GraphKeys.GLOBAL_VARIABLES, scope= self.name+'/rnn/selector')\n",
        "            embedding_vars  = tf.get_collection(tf.GraphKeys.GLOBAL_VARIABLES, scope= self.name+'/embeddings_var')\n",
        "            encoder_vars    = [vars_ for vars_ in tf.get_collection(tf.GraphKeys.GLOBAL_VARIABLES) \n",
        "                               if vars_ not in predictor_vars+selecter_vars+embedding_vars]\n",
        "            \n",
        "            ### EMBEDDING TRAINING\n",
        "            with tf.compat.v1.variable_scope('rnn', reuse=True):\n",
        "                Ey   = predictor(self.embeddings, self.y_dim, self.y_type, self.num_layers_g, self.h_dim_g, self.fc_activate_fn)\n",
        "                # Ey   = predictor(self.EE, self.y_dim, self.y_type, self.num_layers_g, self.h_dim_g, self.fc_activate_fn)\n",
        "\n",
        "            ## LOSS3: embedding separation loss (prevents embedding from collapsing)\n",
        "            self.LOSS_3 = 0\n",
        "            for i in range(self.K):\n",
        "                for j in range(i+1, self.K):\n",
        "                    self.LOSS_3 += - loss_1(Ey[i, :], Ey[j, :], y_type_ = self.y_type) / ((self.K-1)*(self.K-2)) # negative because we want to increase this;\n",
        "                    \n",
        "            \n",
        "            ### DEFINE OPTIMIZATION SOLVERS\n",
        "            self.solver_MLE           = tf.train.AdamOptimizer(self.lr_rate1).minimize(\n",
        "                self.LOSS_MLE, var_list=encoder_vars+predictor_vars\n",
        "            )\n",
        "            self.solver_L1_critic     = tf.train.AdamOptimizer(self.lr_rate1).minimize(\n",
        "                self.LOSS_1,\n",
        "                var_list=encoder_vars+predictor_vars\n",
        "            )\n",
        "            self.solver_L1_actor      = tf.train.AdamOptimizer(self.lr_rate2).minimize(\n",
        "                self.LOSS_1_AC + self.alpha*self.LOSS_2, \n",
        "                var_list=encoder_vars + selecter_vars\n",
        "            )            \n",
        "            self.solver_E             = tf.train.AdamOptimizer(self.lr_rate1).minimize(\n",
        "                self.LOSS_1 + self.beta*self.LOSS_3, \n",
        "                var_list=embedding_vars\n",
        "            )\n",
        "\n",
        "            ### INITIALIZE SELECTOR\n",
        "            self.zz     = tf.placeholder(tf.float32, [None, self.z_dim])\n",
        "            with tf.compat.v1.variable_scope('rnn', reuse=True):\n",
        "                self.yy    = predictor(self.zz, self.y_dim, self.y_type, self.num_layers_g, self.h_dim_g, self.fc_activate_fn) #to check the predictor output given z\n",
        "                self.s_out = selector(self.zz, self.K, self.num_layers_h, self.h_dim_h, self.fc_activate_fn)\n",
        "            \n",
        "            ## LOSS_S: selector initialization (cross-entropy wrt initialized class)\n",
        "            self.LOSS_S   = tf.reduce_mean(- tf.reduce_sum(self.s_onehot*log(self.s_out), axis=1))\n",
        "            self.solver_S = tf.train.AdamOptimizer(self.lr_rate1).minimize(\n",
        "                self.LOSS_S, var_list=selecter_vars\n",
        "            )\n",
        "            \n",
        "    ### TRAINING FUNCTIONS        \n",
        "    def train_mle(self, x_, y_, lr_train, k_prob):\n",
        "        return self.sess.run([self.solver_MLE, self.LOSS_MLE],\n",
        "                             feed_dict={self.x: x_, self.y: y_,\n",
        "                                        self.mb_size:np.shape(x_)[0], self.lr_rate1: lr_train, self.keep_prob: k_prob})\n",
        "    \n",
        "    def train_critic(self, x_, y_, lr_train, k_prob):\n",
        "        return self.sess.run([self.solver_L1_critic, self.LOSS_1],\n",
        "                             feed_dict={self.x: x_, self.y: y_, \n",
        "                                        self.mb_size:np.shape(x_)[0], self.lr_rate1: lr_train, self.keep_prob: k_prob})\n",
        "    \n",
        "    def train_actor(self, x_, y_, alpha_, lr_train, k_prob):\n",
        "        return self.sess.run([self.solver_L1_actor, self.LOSS_1, self.LOSS_2],\n",
        "                             feed_dict={self.x: x_, self.y: y_,\n",
        "                                        self.alpha: alpha_,\n",
        "                                        self.mb_size:np.shape(x_)[0], self.lr_rate2: lr_train, self.keep_prob: k_prob})\n",
        "    \n",
        "    def train_selector(self, z_, s_, lr_train, k_prob):\n",
        "        return self.sess.run([self.solver_S, self.LOSS_S],\n",
        "                             feed_dict={self.zz: z_, self.s: s_, \n",
        "                                        self.lr_rate1: lr_train, self.keep_prob: k_prob})\n",
        "    \n",
        "    def train_embedding(self, x_, y_, beta_, lr_train, k_prob):   \n",
        "        return self.sess.run([self.solver_E, self.LOSS_1, self.LOSS_3], \n",
        "                             feed_dict={self.x:x_, self.y:y_,\n",
        "                                        self.beta:beta_,\n",
        "                                        self.mb_size:np.shape(x_)[0], \n",
        "                                        self.lr_rate1:lr_train, self.keep_prob:k_prob})\n",
        "\n",
        "    def get_losses(self, x_, y_):   \n",
        "        return self.sess.run([self.LOSS_1, self.LOSS_2, self.LOSS_3], \n",
        "                             feed_dict={self.x:x_, self.y:y_,\n",
        "                                        self.mb_size:np.shape(x_)[0], \n",
        "                                        self.keep_prob:1.0})\n",
        "        \n",
        "    ### PREDICTION FUNCTIONS\n",
        "    def predict_y_hats(self, x_):\n",
        "        return self.sess.run([self.y_hats, self.rnn_mask2], \n",
        "                             feed_dict={self.x:x_, self.mb_size:np.shape(x_)[0], self.keep_prob:1.0})\n",
        "    \n",
        "    def predict_y_bars(self, x_):\n",
        "        return self.sess.run([self.y_bars, self.rnn_mask2], \n",
        "                             feed_dict={self.x:x_, self.mb_size:np.shape(x_)[0], self.keep_prob:1.0})\n",
        "    \n",
        "    def predict_yy(self, z_):\n",
        "        return self.sess.run(self.yy,\n",
        "                             feed_dict={self.zz:z_, self.mb_size:np.shape(z_)[0], self.keep_prob:1.0})\n",
        "        \n",
        "    def predict_zs_and_pis_m2(self, x_):\n",
        "        return self.sess.run([self.zs, self.pis, self.rnn_mask2], \n",
        "                             feed_dict={self.x:x_, self.mb_size:np.shape(x_)[0], self.keep_prob:1.0})\n",
        "    \n",
        "    def predict_s_sample(self, x_):\n",
        "        return self.sess.run([self.s_sample, self.rnn_mask2], \n",
        "                             feed_dict={self.x:x_, self.mb_size:np.shape(x_)[0], self.keep_prob:1.0})\n",
        "    \n",
        "    def predict_zbars_and_pis_m1(self, x_):\n",
        "        return self.sess.run([self.z_bars, self.pis, self.rnn_mask1], \n",
        "                             feed_dict={self.x:x_, self.mb_size:np.shape(x_)[0], self.keep_prob:1.0})\n",
        "    \n",
        "    def predict_zs_and_pis_m1(self, x_):\n",
        "        return self.sess.run([self.zs, self.pis, self.rnn_mask1], \n",
        "                             feed_dict={self.x:x_, self.mb_size:np.shape(x_)[0], self.keep_prob:1.0})\n",
        "    \n",
        "    def predict_zbars_and_pis_m2(self, x_):\n",
        "        return self.sess.run([self.z_bars, self.pis, self.rnn_mask2], \n",
        "                             feed_dict={self.x:x_, self.mb_size:np.shape(x_)[0], self.keep_prob:1.0})\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "KXb9nCyvb-lH"
      },
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from tensorflow.core.framework.full_type_pb2 import TFT_INT32\n",
        "print('Initialize Network...')\n",
        "\n",
        "from tensorflow.python.framework import ops\n",
        "ops.reset_default_graph()\n",
        "\n",
        "# Turn on xla optimization\n",
        "config = tf.compat.v1.ConfigProto()\n",
        "config.gpu_options.allow_growth = True\n",
        "sess = tf.compat.v1.Session(config=config)\n",
        "\n",
        "model = AC_TPC(sess, \"AC_TPC\", input_dims, network_settings)\n",
        "\n",
        "\n",
        "saver = tf.train.Saver()\n",
        "sess.run(tf.global_variables_initializer(), feed_dict={model.E:np.zeros([K, z_dim]).astype(float)})\n",
        "\n",
        "avg_loss  = 0\n",
        "for itr in range(ITERATION):\n",
        "    x_mb, y_mb  = f_get_minibatch(mb_size, tr_data_x, tr_data_y)\n",
        "\n",
        "    _, tmp_loss = model.train_mle(x_mb, y_mb, lr_rate, keep_prob)\n",
        "    avg_loss   += tmp_loss/check_step\n",
        "\n",
        "    if (itr+1)%check_step == 0:                \n",
        "        tmp_y, tmp_m = model.predict_y_hats(va_data_x)\n",
        "\n",
        "        y_pred = tmp_y.reshape([-1, y_dim])[tmp_m.reshape([-1]) == 1]\n",
        "        y_true = va_data_y.reshape([-1, y_dim])[tmp_m.reshape([-1]) == 1]\n",
        "\n",
        "        AUROC = np.zeros([y_dim])\n",
        "        AUPRC = np.zeros([y_dim])\n",
        "        for y_idx in range(y_dim):\n",
        "            auroc, auprc = f_get_prediction_scores(y_true[:, y_idx], y_pred[:, y_idx])\n",
        "            AUROC[y_idx] = auroc\n",
        "            AUPRC[y_idx] = auprc\n",
        "\n",
        "        print (\"ITR {:05d}: loss_2={:.3f} | va_auroc:{:.3f}, va_auprc:{:.3f}\".format(\n",
        "                itr+1, avg_loss, np.mean(AUROC), np.mean(AUPRC))\n",
        "              )        \n",
        "        avg_loss = 0\n",
        "\n",
        "saver.save(sess, save_path + 'models/model_K{}'.format(K))\n",
        "save_logging(network_settings, save_path + 'models/network_settings_K{}.txt'.format(K))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HDhFfxx9TYx1",
        "outputId": "cccf9ad4-3f7b-4f9f-db96-0d18e41410be"
      },
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Initialize Network...\n",
            "WARNING:tensorflow:`tf.nn.rnn_cell.MultiRNNCell` is deprecated. This class is equivalent as `tf.keras.layers.StackedRNNCells`, and will be replaced by that in Tensorflow 2.0.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:21: UserWarning: `tf.nn.rnn_cell.LSTMCell` is deprecated and will be removed in a future version. This class is equivalent as `tf.keras.layers.LSTMCell`, and will be replaced by that in Tensorflow 2.0.\n",
            "/usr/local/lib/python3.7/dist-packages/keras/layers/legacy_rnn/rnn_cell_impl.py:988: UserWarning: `layer.add_variable` is deprecated and will be removed in a future version. Please use `layer.add_weight` method instead.\n",
            "  partitioner=maybe_partitioner)\n",
            "/usr/local/lib/python3.7/dist-packages/keras/layers/legacy_rnn/rnn_cell_impl.py:996: UserWarning: `layer.add_variable` is deprecated and will be removed in a future version. Please use `layer.add_weight` method instead.\n",
            "  initializer=initializer)\n",
            "/usr/local/lib/python3.7/dist-packages/tensorflow/python/keras/engine/base_layer_v1.py:1694: UserWarning: `layer.apply` is deprecated and will be removed in a future version. Please use `layer.__call__` method instead.\n",
            "  warnings.warn('`layer.apply` is deprecated and '\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "WARNING:tensorflow:From /usr/local/lib/python3.7/dist-packages/tensorflow/python/util/dispatch.py:1082: calling dropout (from tensorflow.python.ops.nn_ops) with keep_prob is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Please use `rate` instead of `keep_prob`. Rate should be set to `rate = 1 - keep_prob`.\n",
            "WARNING:tensorflow:From <ipython-input-16-50e5c00fee46>:180: Categorical.__init__ (from tensorflow.python.ops.distributions.categorical) is deprecated and will be removed after 2019-01-01.\n",
            "Instructions for updating:\n",
            "The TensorFlow Distributions library has moved to TensorFlow Probability (https://github.com/tensorflow/probability). You should update all references to use `tfp.distributions` instead of `tf.distributions`.\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.7/dist-packages/tensorflow/python/ops/distributions/categorical.py:238: Distribution.__init__ (from tensorflow.python.ops.distributions.distribution) is deprecated and will be removed after 2019-01-01.\n",
            "Instructions for updating:\n",
            "The TensorFlow Distributions library has moved to TensorFlow Probability (https://github.com/tensorflow/probability). You should update all references to use `tfp.distributions` instead of `tf.distributions`.\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.7/dist-packages/tensorflow/python/util/dispatch.py:1082: multinomial (from tensorflow.python.ops.random_ops) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Use `tf.random.categorical` instead.\n",
            "ITR 01000: loss_2=2.322 | va_auroc:0.916, va_auprc:0.665\n",
            "ITR 02000: loss_2=1.321 | va_auroc:0.901, va_auprc:0.640\n",
            "ITR 03000: loss_2=1.104 | va_auroc:0.897, va_auprc:0.626\n",
            "ITR 04000: loss_2=0.956 | va_auroc:0.896, va_auprc:0.620\n",
            "ITR 05000: loss_2=0.869 | va_auroc:0.888, va_auprc:0.617\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "alpha  = 1.0\n",
        "beta   = 0.01\n",
        "\n",
        "mb_size    = 128\n",
        "M          = int(tr_data_x.shape[0]/mb_size) #for main algorithm\n",
        "keep_prob  = 0.7\n",
        "lr_rate1   = 1e-3\n",
        "lr_rate2   = 1e-3\n",
        "\n",
        "save_path = './{}/proposed/trained/'.format(data_name)\n",
        "\n",
        "if not os.path.exists(save_path + '/models/'):\n",
        "    os.makedirs(save_path + '/models/')\n",
        "\n",
        "if not os.path.exists(save_path + '/results/'):\n",
        "    os.makedirs(save_path + '/results/')"
      ],
      "metadata": {
        "id": "U_WiviHCXuMj"
      },
      "execution_count": 19,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "### LOAD INITIALIZED NETWORK\n",
        "\n",
        "load_path = './{}/proposed/init/'.format(data_name)\n",
        "\n",
        "tf.reset_default_graph()\n",
        "\n",
        "# Turn on xla optimization\n",
        "config = tf.ConfigProto()\n",
        "config.gpu_options.allow_growth = True\n",
        "sess = tf.Session(config=config)\n",
        "\n",
        "network_settings = load_logging(load_path + 'models/network_settings_K{}.txt'.format(K))\n",
        "z_dim = network_settings['num_layers_encoder'] * network_settings['h_dim_encoder']\n",
        "\n",
        "model = AC_TPC(sess, \"AC_TPC\", input_dims, network_settings)\n",
        "\n",
        "saver = tf.train.Saver()\n",
        "saver.restore(sess, load_path + 'models/model_K{}'.format(K))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GVI4scEmcTsA",
        "outputId": "10159bcd-3d8f-440a-dd42-78b429bb0cc4"
      },
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "WARNING:tensorflow:`tf.nn.rnn_cell.MultiRNNCell` is deprecated. This class is equivalent as `tf.keras.layers.StackedRNNCells`, and will be replaced by that in Tensorflow 2.0.\n",
            "INFO:tensorflow:Restoring parameters from ./sample/proposed/init/models/model_K6\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print('=============================================')\n",
        "print('===== INITIALIZING EMBEDDING & SELECTOR =====')\n",
        "# K-means over the latent encodings\n",
        "e, s_init, tmp_z = initialize_embedding(model, tr_data_x, K)\n",
        "e = np.arctanh(e)\n",
        "sess.run(model.EE.initializer, feed_dict={model.E:e}) #model.EE = tf.nn.tanh(model.E)\n",
        "\n",
        "# update selector wrt initial classes\n",
        "ITERATION  = 5000\n",
        "check_step = 1000\n",
        "\n",
        "avg_loss_s = 0\n",
        "for itr in range(ITERATION):\n",
        "    z_mb, s_mb = f_get_minibatch(mb_size, tmp_z, s_init)\n",
        "    _, tmp_loss_s = model.train_selector(z_mb, s_mb, lr_rate1, k_prob=keep_prob)\n",
        "\n",
        "    avg_loss_s += tmp_loss_s/check_step\n",
        "    if (itr+1)%check_step == 0:\n",
        "        print(\"ITR:{:04d} | Loss_s:{:.4f}\".format(itr+1, avg_loss_s) )\n",
        "        avg_loss_s = 0\n",
        "\n",
        "tmp_ybars = model.predict_yy(np.tanh(e))\n",
        "new_e     = np.copy(e)\n",
        "print('=============================================')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VIYnGVz-lcXi",
        "outputId": "0ceb532f-8f7f-4ff3-a296-516c1578591f"
      },
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "=============================================\n",
            "===== INITIALIZING EMBEDDING & SELECTOR =====\n",
            "ITR:1000 | Loss_s:0.2760\n",
            "ITR:2000 | Loss_s:0.1137\n",
            "ITR:3000 | Loss_s:0.0848\n",
            "ITR:4000 | Loss_s:0.0690\n",
            "ITR:5000 | Loss_s:0.0578\n",
            "=============================================\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print('=============================================')\n",
        "print('========== TRAINING MAIN ALGORITHM ==========')\n",
        "'''\n",
        "    L1: predictive clustering loss\n",
        "    L2: sample-wise entropy loss\n",
        "    L3: embedding separation loss\n",
        "'''\n",
        "\n",
        "ITERATION     = 3000\n",
        "check_step    = 10\n",
        "\n",
        "avg_loss_c_L1 = 0\n",
        "avg_loss_a_L1 = 0\n",
        "avg_loss_a_L2 = 0\n",
        "avg_loss_e_L1 = 0 \n",
        "avg_loss_e_L3 = 0\n",
        "\n",
        "va_avg_loss_L1 = 0\n",
        "va_avg_loss_L2 = 0\n",
        "va_avg_loss_L3 = 0\n",
        "\n",
        "for itr in range(ITERATION):        \n",
        "    e = np.copy(new_e)\n",
        "\n",
        "    for _ in range(M):\n",
        "        x_mb, y_mb = f_get_minibatch(mb_size, tr_data_x, tr_data_y)\n",
        "\n",
        "        _, tmp_loss_c_L1  = model.train_critic(x_mb, y_mb, lr_rate1, keep_prob)\n",
        "        avg_loss_c_L1    += tmp_loss_c_L1/(M*check_step)\n",
        "\n",
        "        x_mb, y_mb = f_get_minibatch(mb_size, tr_data_x, tr_data_y)\n",
        "\n",
        "        _, tmp_loss_a_L1, tmp_loss_a_L2 = model.train_actor(x_mb, y_mb, alpha, lr_rate2, keep_prob)\n",
        "        avg_loss_a_L1 += tmp_loss_a_L1/(M*check_step)\n",
        "        avg_loss_a_L2 += tmp_loss_a_L2/(M*check_step)\n",
        "        \n",
        "    for _ in range(M):\n",
        "        x_mb, y_mb = f_get_minibatch(mb_size, tr_data_x, tr_data_y)\n",
        "\n",
        "        _, tmp_loss_e_L1, tmp_loss_e_L3 = model.train_embedding(x_mb, y_mb, beta, lr_rate1, keep_prob)\n",
        "        avg_loss_e_L1  += tmp_loss_e_L1/(M*check_step)\n",
        "        avg_loss_e_L3  += tmp_loss_e_L3/(M*check_step)\n",
        "\n",
        "        \n",
        "    x_mb, y_mb = f_get_minibatch(mb_size, va_data_x, va_data_y)\n",
        "    tmp_loss_L1, tmp_loss_L2, tmp_loss_L3 = model.get_losses(x_mb, y_mb)\n",
        "    \n",
        "    va_avg_loss_L1  += tmp_loss_L1/check_step\n",
        "    va_avg_loss_L2  += tmp_loss_L2/check_step\n",
        "    va_avg_loss_L3  += tmp_loss_L3/check_step\n",
        "\n",
        "    new_e = sess.run(model.embeddings)\n",
        "\n",
        "    if (itr+1)%check_step == 0:\n",
        "        tmp_ybars = model.predict_yy(new_e)\n",
        "        print (\"ITR {:04d}: L1_c={:.3f}  L1_a={:.3f}  L1_e={:.3f}  L2={:.3f}  L3={:.3f} || va_L1={:.3f}  va_L2={:.3f}  va_L3={:.3f}\".format(\n",
        "            itr+1, avg_loss_c_L1, avg_loss_a_L1, avg_loss_e_L1, avg_loss_a_L2, avg_loss_e_L3,\n",
        "            va_avg_loss_L1, va_avg_loss_L2, va_avg_loss_L3\n",
        "        ))\n",
        "        avg_loss_c_L1 = 0\n",
        "        avg_loss_a_L1 = 0\n",
        "        avg_loss_a_L2 = 0\n",
        "        avg_loss_e_L1 = 0\n",
        "        avg_loss_e_L3 = 0\n",
        "        va_avg_loss_L1 = 0\n",
        "        va_avg_loss_L2 = 0\n",
        "        va_avg_loss_L3 = 0\n",
        "print('=============================================')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OXhw-u2fu3el",
        "outputId": "f6886281-faf9-4347-9312-c28207ac285e"
      },
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "=============================================\n",
            "========== TRAINING MAIN ALGORITHM ==========\n",
            "ITR 0010: L1_c=1.471  L1_a=1.444  L1_e=1.542  L2=0.762  L3=-4.368 || va_L1=6.903  va_L2=1.253  va_L3=-4.553\n",
            "ITR 0020: L1_c=1.585  L1_a=1.535  L1_e=1.512  L2=0.581  L3=-3.907 || va_L1=6.263  va_L2=0.892  va_L3=-4.091\n",
            "ITR 0030: L1_c=1.407  L1_a=1.447  L1_e=1.492  L2=0.426  L3=-3.648 || va_L1=6.398  va_L2=0.712  va_L3=-3.818\n",
            "ITR 0040: L1_c=1.347  L1_a=1.356  L1_e=1.408  L2=0.402  L3=-3.639 || va_L1=6.011  va_L2=0.690  va_L3=-3.695\n",
            "ITR 0050: L1_c=1.445  L1_a=1.430  L1_e=1.467  L2=0.374  L3=-3.741 || va_L1=6.259  va_L2=0.606  va_L3=-3.765\n",
            "ITR 0060: L1_c=1.369  L1_a=1.366  L1_e=1.382  L2=0.359  L3=-3.533 || va_L1=6.421  va_L2=0.579  va_L3=-3.684\n",
            "ITR 0070: L1_c=1.356  L1_a=1.325  L1_e=1.338  L2=0.351  L3=-3.657 || va_L1=6.589  va_L2=0.522  va_L3=-3.698\n",
            "ITR 0080: L1_c=1.388  L1_a=1.381  L1_e=1.343  L2=0.344  L3=-3.877 || va_L1=6.664  va_L2=0.511  va_L3=-3.855\n",
            "ITR 0090: L1_c=1.351  L1_a=1.341  L1_e=1.355  L2=0.325  L3=-3.865 || va_L1=6.246  va_L2=0.519  va_L3=-3.966\n",
            "ITR 0100: L1_c=1.291  L1_a=1.354  L1_e=1.345  L2=0.327  L3=-3.840 || va_L1=6.682  va_L2=0.513  va_L3=-3.964\n",
            "ITR 0110: L1_c=1.306  L1_a=1.299  L1_e=1.339  L2=0.307  L3=-4.091 || va_L1=5.931  va_L2=0.487  va_L3=-4.208\n",
            "ITR 0120: L1_c=1.347  L1_a=1.371  L1_e=1.355  L2=0.308  L3=-4.140 || va_L1=6.443  va_L2=0.486  va_L3=-4.262\n",
            "ITR 0130: L1_c=1.329  L1_a=1.375  L1_e=1.320  L2=0.311  L3=-4.036 || va_L1=6.570  va_L2=0.440  va_L3=-4.167\n",
            "ITR 0140: L1_c=1.273  L1_a=1.323  L1_e=1.329  L2=0.310  L3=-4.194 || va_L1=5.891  va_L2=0.474  va_L3=-4.257\n",
            "ITR 0150: L1_c=1.308  L1_a=1.360  L1_e=1.392  L2=0.288  L3=-4.113 || va_L1=6.091  va_L2=0.455  va_L3=-4.179\n",
            "ITR 0160: L1_c=1.338  L1_a=1.327  L1_e=1.318  L2=0.277  L3=-4.100 || va_L1=5.533  va_L2=0.444  va_L3=-4.235\n",
            "ITR 0170: L1_c=1.295  L1_a=1.288  L1_e=1.309  L2=0.247  L3=-4.325 || va_L1=5.688  va_L2=0.422  va_L3=-4.494\n",
            "ITR 0180: L1_c=1.292  L1_a=1.319  L1_e=1.354  L2=0.268  L3=-4.580 || va_L1=6.105  va_L2=0.419  va_L3=-4.707\n",
            "ITR 0190: L1_c=1.337  L1_a=1.327  L1_e=1.293  L2=0.280  L3=-4.680 || va_L1=6.036  va_L2=0.413  va_L3=-4.773\n",
            "ITR 0200: L1_c=1.324  L1_a=1.339  L1_e=1.365  L2=0.252  L3=-4.955 || va_L1=6.231  va_L2=0.415  va_L3=-5.040\n",
            "ITR 0210: L1_c=1.383  L1_a=1.352  L1_e=1.306  L2=0.249  L3=-5.150 || va_L1=5.911  va_L2=0.376  va_L3=-5.250\n",
            "ITR 0220: L1_c=1.339  L1_a=1.304  L1_e=1.314  L2=0.233  L3=-5.275 || va_L1=6.605  va_L2=0.398  va_L3=-5.389\n",
            "ITR 0230: L1_c=1.335  L1_a=1.349  L1_e=1.358  L2=0.241  L3=-5.117 || va_L1=6.056  va_L2=0.361  va_L3=-5.214\n",
            "ITR 0240: L1_c=1.299  L1_a=1.289  L1_e=1.382  L2=0.226  L3=-5.201 || va_L1=5.654  va_L2=0.375  va_L3=-5.361\n",
            "ITR 0250: L1_c=1.349  L1_a=1.363  L1_e=1.325  L2=0.229  L3=-5.436 || va_L1=5.697  va_L2=0.363  va_L3=-5.567\n",
            "ITR 0260: L1_c=1.398  L1_a=1.333  L1_e=1.370  L2=0.221  L3=-5.431 || va_L1=6.637  va_L2=0.365  va_L3=-5.562\n",
            "ITR 0270: L1_c=1.326  L1_a=1.367  L1_e=1.360  L2=0.216  L3=-5.365 || va_L1=5.856  va_L2=0.342  va_L3=-5.584\n",
            "ITR 0280: L1_c=1.304  L1_a=1.359  L1_e=1.349  L2=0.222  L3=-5.434 || va_L1=6.391  va_L2=0.337  va_L3=-5.528\n",
            "ITR 0290: L1_c=1.348  L1_a=1.380  L1_e=1.352  L2=0.216  L3=-5.382 || va_L1=6.535  va_L2=0.322  va_L3=-5.526\n",
            "ITR 0300: L1_c=1.376  L1_a=1.310  L1_e=1.351  L2=0.218  L3=-5.402 || va_L1=6.060  va_L2=0.336  va_L3=-5.496\n",
            "ITR 0310: L1_c=1.414  L1_a=1.378  L1_e=1.367  L2=0.197  L3=-5.469 || va_L1=5.613  va_L2=0.335  va_L3=-5.556\n",
            "ITR 0320: L1_c=1.388  L1_a=1.345  L1_e=1.359  L2=0.207  L3=-5.513 || va_L1=5.949  va_L2=0.357  va_L3=-5.545\n",
            "ITR 0330: L1_c=1.325  L1_a=1.355  L1_e=1.378  L2=0.211  L3=-5.506 || va_L1=6.375  va_L2=0.334  va_L3=-5.564\n",
            "ITR 0340: L1_c=1.345  L1_a=1.338  L1_e=1.365  L2=0.207  L3=-5.554 || va_L1=5.588  va_L2=0.321  va_L3=-5.627\n",
            "ITR 0350: L1_c=1.318  L1_a=1.328  L1_e=1.359  L2=0.202  L3=-5.546 || va_L1=5.686  va_L2=0.342  va_L3=-5.620\n",
            "ITR 0360: L1_c=1.377  L1_a=1.388  L1_e=1.354  L2=0.217  L3=-5.518 || va_L1=5.847  va_L2=0.320  va_L3=-5.581\n",
            "ITR 0370: L1_c=1.352  L1_a=1.324  L1_e=1.359  L2=0.206  L3=-5.577 || va_L1=5.559  va_L2=0.307  va_L3=-5.653\n",
            "ITR 0380: L1_c=1.389  L1_a=1.356  L1_e=1.274  L2=0.199  L3=-5.482 || va_L1=5.588  va_L2=0.317  va_L3=-5.634\n",
            "ITR 0390: L1_c=1.302  L1_a=1.333  L1_e=1.313  L2=0.208  L3=-5.549 || va_L1=5.227  va_L2=0.314  va_L3=-5.581\n",
            "ITR 0400: L1_c=1.343  L1_a=1.331  L1_e=1.352  L2=0.205  L3=-5.622 || va_L1=5.377  va_L2=0.341  va_L3=-5.696\n",
            "ITR 0410: L1_c=1.294  L1_a=1.311  L1_e=1.293  L2=0.204  L3=-5.794 || va_L1=6.202  va_L2=0.371  va_L3=-5.817\n",
            "ITR 0420: L1_c=1.271  L1_a=1.292  L1_e=1.308  L2=0.188  L3=-5.710 || va_L1=6.113  va_L2=0.341  va_L3=-5.749\n",
            "ITR 0430: L1_c=1.350  L1_a=1.370  L1_e=1.333  L2=0.194  L3=-5.623 || va_L1=6.019  va_L2=0.351  va_L3=-5.712\n",
            "ITR 0440: L1_c=1.308  L1_a=1.274  L1_e=1.326  L2=0.199  L3=-5.607 || va_L1=6.207  va_L2=0.322  va_L3=-5.675\n",
            "ITR 0450: L1_c=1.342  L1_a=1.334  L1_e=1.314  L2=0.190  L3=-5.724 || va_L1=5.653  va_L2=0.319  va_L3=-5.791\n",
            "ITR 0460: L1_c=1.310  L1_a=1.273  L1_e=1.297  L2=0.187  L3=-5.642 || va_L1=5.938  va_L2=0.330  va_L3=-5.705\n",
            "ITR 0470: L1_c=1.321  L1_a=1.296  L1_e=1.328  L2=0.176  L3=-5.555 || va_L1=5.721  va_L2=0.290  va_L3=-5.705\n",
            "ITR 0480: L1_c=1.371  L1_a=1.291  L1_e=1.306  L2=0.176  L3=-5.851 || va_L1=6.027  va_L2=0.282  va_L3=-5.818\n",
            "ITR 0490: L1_c=1.358  L1_a=1.329  L1_e=1.361  L2=0.184  L3=-5.744 || va_L1=6.388  va_L2=0.284  va_L3=-5.766\n",
            "ITR 0500: L1_c=1.246  L1_a=1.256  L1_e=1.273  L2=0.184  L3=-5.557 || va_L1=6.202  va_L2=0.325  va_L3=-5.659\n",
            "ITR 0510: L1_c=1.304  L1_a=1.299  L1_e=1.278  L2=0.181  L3=-5.739 || va_L1=5.834  va_L2=0.335  va_L3=-5.739\n",
            "ITR 0520: L1_c=1.271  L1_a=1.280  L1_e=1.288  L2=0.179  L3=-5.596 || va_L1=5.612  va_L2=0.305  va_L3=-5.774\n",
            "ITR 0530: L1_c=1.272  L1_a=1.275  L1_e=1.257  L2=0.186  L3=-5.815 || va_L1=5.973  va_L2=0.276  va_L3=-5.861\n",
            "ITR 0540: L1_c=1.302  L1_a=1.270  L1_e=1.306  L2=0.170  L3=-5.876 || va_L1=6.227  va_L2=0.288  va_L3=-5.894\n",
            "ITR 0550: L1_c=1.431  L1_a=1.365  L1_e=1.349  L2=0.170  L3=-5.834 || va_L1=5.884  va_L2=0.296  va_L3=-5.948\n",
            "ITR 0560: L1_c=1.263  L1_a=1.292  L1_e=1.353  L2=0.160  L3=-5.974 || va_L1=6.395  va_L2=0.270  va_L3=-6.013\n",
            "ITR 0570: L1_c=1.274  L1_a=1.308  L1_e=1.271  L2=0.168  L3=-5.858 || va_L1=6.102  va_L2=0.273  va_L3=-5.912\n",
            "ITR 0580: L1_c=1.267  L1_a=1.258  L1_e=1.273  L2=0.167  L3=-5.921 || va_L1=6.574  va_L2=0.251  va_L3=-5.995\n",
            "ITR 0590: L1_c=1.288  L1_a=1.254  L1_e=1.269  L2=0.158  L3=-5.859 || va_L1=6.563  va_L2=0.238  va_L3=-6.041\n",
            "ITR 0600: L1_c=1.216  L1_a=1.265  L1_e=1.267  L2=0.163  L3=-5.986 || va_L1=6.446  va_L2=0.276  va_L3=-5.946\n",
            "ITR 0610: L1_c=1.210  L1_a=1.225  L1_e=1.302  L2=0.167  L3=-5.736 || va_L1=6.050  va_L2=0.258  va_L3=-5.887\n",
            "ITR 0620: L1_c=1.282  L1_a=1.292  L1_e=1.324  L2=0.162  L3=-5.872 || va_L1=6.147  va_L2=0.272  va_L3=-5.969\n",
            "ITR 0630: L1_c=1.362  L1_a=1.311  L1_e=1.328  L2=0.161  L3=-5.775 || va_L1=5.543  va_L2=0.266  va_L3=-5.853\n",
            "ITR 0640: L1_c=1.283  L1_a=1.297  L1_e=1.297  L2=0.167  L3=-5.773 || va_L1=6.068  va_L2=0.275  va_L3=-5.824\n",
            "ITR 0650: L1_c=1.338  L1_a=1.318  L1_e=1.330  L2=0.169  L3=-5.618 || va_L1=5.667  va_L2=0.260  va_L3=-5.720\n",
            "ITR 0660: L1_c=1.288  L1_a=1.248  L1_e=1.249  L2=0.153  L3=-5.680 || va_L1=6.022  va_L2=0.260  va_L3=-5.770\n",
            "ITR 0670: L1_c=1.222  L1_a=1.287  L1_e=1.221  L2=0.173  L3=-5.754 || va_L1=6.461  va_L2=0.262  va_L3=-5.811\n",
            "ITR 0680: L1_c=1.304  L1_a=1.351  L1_e=1.365  L2=0.159  L3=-5.632 || va_L1=5.873  va_L2=0.256  va_L3=-5.695\n",
            "ITR 0690: L1_c=1.286  L1_a=1.301  L1_e=1.344  L2=0.140  L3=-5.740 || va_L1=5.446  va_L2=0.226  va_L3=-5.800\n",
            "ITR 0700: L1_c=1.249  L1_a=1.306  L1_e=1.291  L2=0.152  L3=-5.754 || va_L1=5.532  va_L2=0.239  va_L3=-5.823\n",
            "ITR 0710: L1_c=1.312  L1_a=1.289  L1_e=1.304  L2=0.154  L3=-5.871 || va_L1=5.533  va_L2=0.275  va_L3=-5.875\n",
            "ITR 0720: L1_c=1.343  L1_a=1.268  L1_e=1.300  L2=0.147  L3=-5.926 || va_L1=5.033  va_L2=0.243  va_L3=-5.962\n",
            "ITR 0730: L1_c=1.283  L1_a=1.297  L1_e=1.244  L2=0.156  L3=-5.783 || va_L1=5.914  va_L2=0.249  va_L3=-5.885\n",
            "ITR 0740: L1_c=1.329  L1_a=1.270  L1_e=1.279  L2=0.151  L3=-5.804 || va_L1=5.622  va_L2=0.247  va_L3=-5.943\n",
            "ITR 0750: L1_c=1.345  L1_a=1.286  L1_e=1.298  L2=0.145  L3=-5.918 || va_L1=5.800  va_L2=0.214  va_L3=-5.969\n",
            "ITR 0760: L1_c=1.317  L1_a=1.287  L1_e=1.295  L2=0.141  L3=-5.728 || va_L1=6.162  va_L2=0.204  va_L3=-5.969\n",
            "ITR 0770: L1_c=1.345  L1_a=1.349  L1_e=1.378  L2=0.149  L3=-6.016 || va_L1=5.662  va_L2=0.223  va_L3=-6.003\n",
            "ITR 0780: L1_c=1.415  L1_a=1.379  L1_e=1.325  L2=0.157  L3=-5.965 || va_L1=6.442  va_L2=0.230  va_L3=-6.048\n",
            "ITR 0790: L1_c=1.322  L1_a=1.310  L1_e=1.316  L2=0.135  L3=-6.190 || va_L1=6.228  va_L2=0.243  va_L3=-6.279\n",
            "ITR 0800: L1_c=1.255  L1_a=1.262  L1_e=1.244  L2=0.136  L3=-6.132 || va_L1=6.444  va_L2=0.224  va_L3=-6.328\n",
            "ITR 0810: L1_c=1.360  L1_a=1.290  L1_e=1.364  L2=0.139  L3=-6.269 || va_L1=6.213  va_L2=0.211  va_L3=-6.373\n",
            "ITR 0820: L1_c=1.374  L1_a=1.448  L1_e=1.430  L2=0.147  L3=-6.032 || va_L1=5.800  va_L2=0.221  va_L3=-6.373\n",
            "ITR 0830: L1_c=1.334  L1_a=1.328  L1_e=1.278  L2=0.134  L3=-6.170 || va_L1=5.711  va_L2=0.244  va_L3=-6.351\n",
            "ITR 0840: L1_c=1.251  L1_a=1.280  L1_e=1.304  L2=0.153  L3=-6.138 || va_L1=6.440  va_L2=0.221  va_L3=-6.346\n",
            "ITR 0850: L1_c=1.292  L1_a=1.256  L1_e=1.294  L2=0.145  L3=-6.261 || va_L1=5.802  va_L2=0.216  va_L3=-6.433\n",
            "ITR 0860: L1_c=1.269  L1_a=1.280  L1_e=1.206  L2=0.139  L3=-6.274 || va_L1=5.362  va_L2=0.244  va_L3=-6.447\n",
            "ITR 0870: L1_c=1.272  L1_a=1.236  L1_e=1.232  L2=0.132  L3=-6.104 || va_L1=5.308  va_L2=0.238  va_L3=-6.433\n",
            "ITR 0880: L1_c=1.259  L1_a=1.303  L1_e=1.235  L2=0.135  L3=-6.124 || va_L1=5.869  va_L2=0.231  va_L3=-6.428\n",
            "ITR 0890: L1_c=1.255  L1_a=1.267  L1_e=1.288  L2=0.131  L3=-6.186 || va_L1=6.386  va_L2=0.226  va_L3=-6.409\n",
            "ITR 0900: L1_c=1.229  L1_a=1.236  L1_e=1.231  L2=0.129  L3=-6.281 || va_L1=5.667  va_L2=0.212  va_L3=-6.414\n",
            "ITR 0910: L1_c=1.204  L1_a=1.235  L1_e=1.193  L2=0.130  L3=-6.090 || va_L1=5.565  va_L2=0.231  va_L3=-6.404\n",
            "ITR 0920: L1_c=1.187  L1_a=1.229  L1_e=1.251  L2=0.139  L3=-6.049 || va_L1=5.985  va_L2=0.258  va_L3=-6.420\n",
            "ITR 0930: L1_c=1.253  L1_a=1.260  L1_e=1.269  L2=0.136  L3=-6.310 || va_L1=5.457  va_L2=0.234  va_L3=-6.451\n",
            "ITR 0940: L1_c=1.281  L1_a=1.263  L1_e=1.243  L2=0.137  L3=-6.109 || va_L1=6.017  va_L2=0.256  va_L3=-6.476\n",
            "ITR 0950: L1_c=1.321  L1_a=1.329  L1_e=1.312  L2=0.135  L3=-6.205 || va_L1=6.841  va_L2=0.237  va_L3=-6.445\n",
            "ITR 0960: L1_c=1.279  L1_a=1.259  L1_e=1.228  L2=0.136  L3=-6.098 || va_L1=5.917  va_L2=0.243  va_L3=-6.368\n",
            "ITR 0970: L1_c=1.264  L1_a=1.337  L1_e=1.299  L2=0.137  L3=-6.164 || va_L1=5.631  va_L2=0.208  va_L3=-6.320\n",
            "ITR 0980: L1_c=1.268  L1_a=1.281  L1_e=1.243  L2=0.135  L3=-6.283 || va_L1=5.714  va_L2=0.212  va_L3=-6.395\n",
            "ITR 0990: L1_c=1.170  L1_a=1.168  L1_e=1.210  L2=0.137  L3=-6.251 || va_L1=6.505  va_L2=0.236  va_L3=-6.408\n",
            "ITR 1000: L1_c=1.187  L1_a=1.229  L1_e=1.204  L2=0.129  L3=-6.225 || va_L1=6.718  va_L2=0.230  va_L3=-6.394\n",
            "ITR 1010: L1_c=1.265  L1_a=1.262  L1_e=1.259  L2=0.135  L3=-6.249 || va_L1=6.263  va_L2=0.212  va_L3=-6.416\n",
            "ITR 1020: L1_c=1.301  L1_a=1.243  L1_e=1.286  L2=0.132  L3=-6.148 || va_L1=6.055  va_L2=0.217  va_L3=-6.380\n",
            "ITR 1030: L1_c=1.316  L1_a=1.310  L1_e=1.366  L2=0.134  L3=-6.275 || va_L1=5.765  va_L2=0.203  va_L3=-6.393\n",
            "ITR 1040: L1_c=1.230  L1_a=1.286  L1_e=1.292  L2=0.123  L3=-6.291 || va_L1=5.809  va_L2=0.192  va_L3=-6.505\n",
            "ITR 1050: L1_c=1.312  L1_a=1.284  L1_e=1.269  L2=0.127  L3=-6.361 || va_L1=5.785  va_L2=0.246  va_L3=-6.517\n",
            "ITR 1060: L1_c=1.339  L1_a=1.338  L1_e=1.319  L2=0.131  L3=-6.235 || va_L1=5.864  va_L2=0.219  va_L3=-6.500\n",
            "ITR 1070: L1_c=1.278  L1_a=1.372  L1_e=1.304  L2=0.132  L3=-6.296 || va_L1=6.187  va_L2=0.240  va_L3=-6.502\n",
            "ITR 1080: L1_c=1.309  L1_a=1.324  L1_e=1.292  L2=0.126  L3=-6.317 || va_L1=6.012  va_L2=0.219  va_L3=-6.501\n",
            "ITR 1090: L1_c=1.326  L1_a=1.265  L1_e=1.252  L2=0.124  L3=-6.272 || va_L1=5.857  va_L2=0.220  va_L3=-6.541\n",
            "ITR 1100: L1_c=1.245  L1_a=1.226  L1_e=1.252  L2=0.121  L3=-6.356 || va_L1=6.299  va_L2=0.213  va_L3=-6.498\n",
            "ITR 1110: L1_c=1.236  L1_a=1.215  L1_e=1.207  L2=0.124  L3=-6.307 || va_L1=6.349  va_L2=0.207  va_L3=-6.515\n",
            "ITR 1120: L1_c=1.230  L1_a=1.247  L1_e=1.234  L2=0.123  L3=-6.245 || va_L1=5.860  va_L2=0.228  va_L3=-6.492\n",
            "ITR 1130: L1_c=1.253  L1_a=1.206  L1_e=1.241  L2=0.135  L3=-6.272 || va_L1=6.049  va_L2=0.224  va_L3=-6.534\n",
            "ITR 1140: L1_c=1.230  L1_a=1.222  L1_e=1.212  L2=0.124  L3=-6.277 || va_L1=5.615  va_L2=0.215  va_L3=-6.533\n",
            "ITR 1150: L1_c=1.258  L1_a=1.293  L1_e=1.233  L2=0.117  L3=-6.288 || va_L1=6.079  va_L2=0.196  va_L3=-6.526\n",
            "ITR 1160: L1_c=1.213  L1_a=1.303  L1_e=1.229  L2=0.125  L3=-6.276 || va_L1=5.784  va_L2=0.192  va_L3=-6.508\n",
            "ITR 1170: L1_c=1.232  L1_a=1.268  L1_e=1.236  L2=0.127  L3=-6.418 || va_L1=5.940  va_L2=0.213  va_L3=-6.561\n",
            "ITR 1180: L1_c=1.226  L1_a=1.213  L1_e=1.164  L2=0.119  L3=-6.236 || va_L1=5.466  va_L2=0.214  va_L3=-6.530\n",
            "ITR 1190: L1_c=1.252  L1_a=1.301  L1_e=1.268  L2=0.122  L3=-6.303 || va_L1=5.709  va_L2=0.202  va_L3=-6.522\n",
            "ITR 1200: L1_c=1.250  L1_a=1.244  L1_e=1.270  L2=0.129  L3=-6.311 || va_L1=5.685  va_L2=0.198  va_L3=-6.531\n",
            "ITR 1210: L1_c=1.220  L1_a=1.237  L1_e=1.227  L2=0.130  L3=-6.337 || va_L1=5.678  va_L2=0.218  va_L3=-6.570\n",
            "ITR 1220: L1_c=1.380  L1_a=1.367  L1_e=1.363  L2=0.132  L3=-6.318 || va_L1=5.756  va_L2=0.225  va_L3=-6.549\n",
            "ITR 1230: L1_c=1.338  L1_a=1.351  L1_e=1.286  L2=0.129  L3=-6.359 || va_L1=5.883  va_L2=0.218  va_L3=-6.530\n",
            "ITR 1240: L1_c=1.298  L1_a=1.285  L1_e=1.343  L2=0.130  L3=-6.355 || va_L1=5.822  va_L2=0.224  va_L3=-6.604\n",
            "ITR 1250: L1_c=1.282  L1_a=1.232  L1_e=1.248  L2=0.119  L3=-6.364 || va_L1=5.416  va_L2=0.213  va_L3=-6.597\n",
            "ITR 1260: L1_c=1.227  L1_a=1.260  L1_e=1.208  L2=0.113  L3=-6.429 || va_L1=5.476  va_L2=0.212  va_L3=-6.621\n",
            "ITR 1270: L1_c=1.254  L1_a=1.271  L1_e=1.281  L2=0.123  L3=-6.424 || va_L1=5.655  va_L2=0.209  va_L3=-6.688\n",
            "ITR 1280: L1_c=1.218  L1_a=1.224  L1_e=1.244  L2=0.106  L3=-6.384 || va_L1=5.611  va_L2=0.212  va_L3=-6.597\n",
            "ITR 1290: L1_c=1.220  L1_a=1.229  L1_e=1.231  L2=0.117  L3=-6.421 || va_L1=6.131  va_L2=0.197  va_L3=-6.570\n",
            "ITR 1300: L1_c=1.326  L1_a=1.261  L1_e=1.287  L2=0.119  L3=-6.298 || va_L1=5.766  va_L2=0.209  va_L3=-6.557\n",
            "ITR 1310: L1_c=1.268  L1_a=1.299  L1_e=1.243  L2=0.116  L3=-6.320 || va_L1=6.022  va_L2=0.183  va_L3=-6.461\n",
            "ITR 1320: L1_c=1.192  L1_a=1.241  L1_e=1.250  L2=0.113  L3=-6.255 || va_L1=5.741  va_L2=0.180  va_L3=-6.498\n",
            "ITR 1330: L1_c=1.221  L1_a=1.185  L1_e=1.227  L2=0.114  L3=-6.281 || va_L1=5.935  va_L2=0.182  va_L3=-6.499\n",
            "ITR 1340: L1_c=1.233  L1_a=1.214  L1_e=1.235  L2=0.110  L3=-6.359 || va_L1=5.747  va_L2=0.193  va_L3=-6.542\n",
            "ITR 1350: L1_c=1.156  L1_a=1.257  L1_e=1.231  L2=0.112  L3=-6.343 || va_L1=6.291  va_L2=0.192  va_L3=-6.559\n",
            "ITR 1360: L1_c=1.214  L1_a=1.195  L1_e=1.226  L2=0.118  L3=-6.319 || va_L1=6.568  va_L2=0.193  va_L3=-6.536\n",
            "ITR 1370: L1_c=1.194  L1_a=1.149  L1_e=1.202  L2=0.111  L3=-6.322 || va_L1=6.223  va_L2=0.202  va_L3=-6.554\n",
            "ITR 1380: L1_c=1.189  L1_a=1.185  L1_e=1.212  L2=0.111  L3=-6.405 || va_L1=6.357  va_L2=0.213  va_L3=-6.550\n",
            "ITR 1390: L1_c=1.235  L1_a=1.197  L1_e=1.222  L2=0.106  L3=-6.391 || va_L1=5.837  va_L2=0.195  va_L3=-6.578\n",
            "ITR 1400: L1_c=1.249  L1_a=1.296  L1_e=1.235  L2=0.110  L3=-6.248 || va_L1=5.521  va_L2=0.196  va_L3=-6.547\n",
            "ITR 1410: L1_c=1.219  L1_a=1.258  L1_e=1.236  L2=0.106  L3=-6.366 || va_L1=5.688  va_L2=0.190  va_L3=-6.587\n",
            "ITR 1420: L1_c=1.259  L1_a=1.239  L1_e=1.262  L2=0.102  L3=-6.362 || va_L1=6.156  va_L2=0.193  va_L3=-6.496\n",
            "ITR 1430: L1_c=1.175  L1_a=1.210  L1_e=1.185  L2=0.108  L3=-6.203 || va_L1=6.211  va_L2=0.193  va_L3=-6.391\n",
            "ITR 1440: L1_c=1.329  L1_a=1.255  L1_e=1.253  L2=0.112  L3=-6.122 || va_L1=5.999  va_L2=0.208  va_L3=-6.312\n",
            "ITR 1450: L1_c=1.277  L1_a=1.314  L1_e=1.231  L2=0.113  L3=-6.237 || va_L1=6.355  va_L2=0.200  va_L3=-6.272\n",
            "ITR 1460: L1_c=1.231  L1_a=1.277  L1_e=1.259  L2=0.113  L3=-6.049 || va_L1=5.878  va_L2=0.186  va_L3=-6.153\n",
            "ITR 1470: L1_c=1.275  L1_a=1.265  L1_e=1.249  L2=0.106  L3=-6.118 || va_L1=5.987  va_L2=0.199  va_L3=-6.156\n",
            "ITR 1480: L1_c=1.279  L1_a=1.231  L1_e=1.217  L2=0.100  L3=-6.164 || va_L1=5.615  va_L2=0.184  va_L3=-6.244\n",
            "ITR 1490: L1_c=1.271  L1_a=1.254  L1_e=1.212  L2=0.103  L3=-6.077 || va_L1=6.311  va_L2=0.188  va_L3=-6.171\n",
            "ITR 1500: L1_c=1.215  L1_a=1.247  L1_e=1.249  L2=0.102  L3=-5.883 || va_L1=6.426  va_L2=0.182  va_L3=-6.080\n",
            "ITR 1510: L1_c=1.248  L1_a=1.288  L1_e=1.217  L2=0.101  L3=-6.104 || va_L1=6.300  va_L2=0.182  va_L3=-6.179\n",
            "ITR 1520: L1_c=1.225  L1_a=1.240  L1_e=1.209  L2=0.096  L3=-6.002 || va_L1=6.437  va_L2=0.181  va_L3=-6.227\n",
            "ITR 1530: L1_c=1.252  L1_a=1.301  L1_e=1.259  L2=0.111  L3=-5.953 || va_L1=6.449  va_L2=0.203  va_L3=-6.222\n",
            "ITR 1540: L1_c=1.198  L1_a=1.235  L1_e=1.271  L2=0.103  L3=-5.956 || va_L1=6.181  va_L2=0.167  va_L3=-6.166\n",
            "ITR 1550: L1_c=1.193  L1_a=1.228  L1_e=1.195  L2=0.109  L3=-6.217 || va_L1=5.941  va_L2=0.200  va_L3=-6.235\n",
            "ITR 1560: L1_c=1.197  L1_a=1.224  L1_e=1.246  L2=0.101  L3=-5.969 || va_L1=6.673  va_L2=0.176  va_L3=-6.180\n",
            "ITR 1570: L1_c=1.220  L1_a=1.201  L1_e=1.230  L2=0.096  L3=-6.113 || va_L1=5.722  va_L2=0.162  va_L3=-6.168\n",
            "ITR 1580: L1_c=1.219  L1_a=1.233  L1_e=1.209  L2=0.110  L3=-6.107 || va_L1=6.159  va_L2=0.184  va_L3=-6.240\n",
            "ITR 1590: L1_c=1.197  L1_a=1.220  L1_e=1.213  L2=0.099  L3=-6.111 || va_L1=5.372  va_L2=0.181  va_L3=-6.257\n",
            "ITR 1600: L1_c=1.217  L1_a=1.252  L1_e=1.205  L2=0.107  L3=-6.029 || va_L1=5.593  va_L2=0.213  va_L3=-6.253\n",
            "ITR 1610: L1_c=1.191  L1_a=1.166  L1_e=1.157  L2=0.101  L3=-6.102 || va_L1=6.085  va_L2=0.201  va_L3=-6.220\n",
            "ITR 1620: L1_c=1.206  L1_a=1.196  L1_e=1.195  L2=0.101  L3=-5.929 || va_L1=6.102  va_L2=0.195  va_L3=-6.238\n",
            "ITR 1630: L1_c=1.197  L1_a=1.209  L1_e=1.169  L2=0.094  L3=-6.113 || va_L1=5.830  va_L2=0.174  va_L3=-6.214\n",
            "ITR 1640: L1_c=1.182  L1_a=1.168  L1_e=1.162  L2=0.095  L3=-6.093 || va_L1=6.180  va_L2=0.162  va_L3=-6.142\n",
            "ITR 1650: L1_c=1.199  L1_a=1.145  L1_e=1.177  L2=0.093  L3=-5.869 || va_L1=5.612  va_L2=0.171  va_L3=-6.138\n",
            "ITR 1660: L1_c=1.141  L1_a=1.165  L1_e=1.177  L2=0.097  L3=-5.753 || va_L1=6.033  va_L2=0.193  va_L3=-5.970\n",
            "ITR 1670: L1_c=1.259  L1_a=1.239  L1_e=1.290  L2=0.100  L3=-6.064 || va_L1=5.213  va_L2=0.172  va_L3=-6.040\n",
            "ITR 1680: L1_c=1.217  L1_a=1.200  L1_e=1.254  L2=0.108  L3=-5.868 || va_L1=5.810  va_L2=0.190  va_L3=-6.047\n",
            "ITR 1690: L1_c=1.218  L1_a=1.239  L1_e=1.253  L2=0.101  L3=-5.912 || va_L1=5.797  va_L2=0.186  va_L3=-5.997\n",
            "ITR 1700: L1_c=1.234  L1_a=1.268  L1_e=1.250  L2=0.094  L3=-6.025 || va_L1=5.756  va_L2=0.157  va_L3=-5.998\n",
            "ITR 1710: L1_c=1.195  L1_a=1.199  L1_e=1.209  L2=0.095  L3=-5.686 || va_L1=5.628  va_L2=0.174  va_L3=-5.978\n",
            "ITR 1720: L1_c=1.199  L1_a=1.174  L1_e=1.183  L2=0.100  L3=-5.871 || va_L1=5.383  va_L2=0.162  va_L3=-6.014\n",
            "ITR 1730: L1_c=1.192  L1_a=1.186  L1_e=1.200  L2=0.090  L3=-6.063 || va_L1=5.939  va_L2=0.165  va_L3=-6.068\n",
            "ITR 1740: L1_c=1.258  L1_a=1.227  L1_e=1.201  L2=0.094  L3=-5.873 || va_L1=5.991  va_L2=0.174  va_L3=-5.997\n",
            "ITR 1750: L1_c=1.198  L1_a=1.207  L1_e=1.197  L2=0.095  L3=-5.791 || va_L1=5.843  va_L2=0.149  va_L3=-5.957\n",
            "ITR 1760: L1_c=1.174  L1_a=1.139  L1_e=1.200  L2=0.102  L3=-5.920 || va_L1=5.168  va_L2=0.182  va_L3=-6.023\n",
            "ITR 1770: L1_c=1.172  L1_a=1.208  L1_e=1.217  L2=0.094  L3=-5.929 || va_L1=6.187  va_L2=0.182  va_L3=-6.040\n",
            "ITR 1780: L1_c=1.274  L1_a=1.233  L1_e=1.279  L2=0.100  L3=-5.852 || va_L1=5.594  va_L2=0.189  va_L3=-5.962\n",
            "ITR 1790: L1_c=1.316  L1_a=1.266  L1_e=1.300  L2=0.104  L3=-5.698 || va_L1=5.019  va_L2=0.156  va_L3=-5.928\n",
            "ITR 1800: L1_c=1.240  L1_a=1.234  L1_e=1.266  L2=0.097  L3=-5.788 || va_L1=5.350  va_L2=0.174  va_L3=-5.975\n",
            "ITR 1810: L1_c=1.292  L1_a=1.213  L1_e=1.275  L2=0.091  L3=-5.740 || va_L1=5.127  va_L2=0.185  va_L3=-5.908\n",
            "ITR 1820: L1_c=1.265  L1_a=1.299  L1_e=1.282  L2=0.093  L3=-5.987 || va_L1=6.305  va_L2=0.198  va_L3=-5.978\n",
            "ITR 1830: L1_c=1.249  L1_a=1.229  L1_e=1.267  L2=0.093  L3=-5.786 || va_L1=6.029  va_L2=0.185  va_L3=-5.979\n",
            "ITR 1840: L1_c=1.225  L1_a=1.289  L1_e=1.253  L2=0.100  L3=-5.725 || va_L1=5.454  va_L2=0.162  va_L3=-5.900\n",
            "ITR 1850: L1_c=1.303  L1_a=1.283  L1_e=1.276  L2=0.093  L3=-5.800 || va_L1=4.995  va_L2=0.143  va_L3=-5.930\n",
            "ITR 1860: L1_c=1.241  L1_a=1.236  L1_e=1.325  L2=0.100  L3=-5.708 || va_L1=5.445  va_L2=0.173  va_L3=-5.920\n",
            "ITR 1870: L1_c=1.184  L1_a=1.192  L1_e=1.141  L2=0.091  L3=-5.837 || va_L1=5.311  va_L2=0.167  va_L3=-5.979\n",
            "ITR 1880: L1_c=1.180  L1_a=1.222  L1_e=1.226  L2=0.095  L3=-5.965 || va_L1=5.821  va_L2=0.198  va_L3=-6.028\n",
            "ITR 1890: L1_c=1.265  L1_a=1.258  L1_e=1.244  L2=0.098  L3=-5.857 || va_L1=5.478  va_L2=0.170  va_L3=-6.000\n",
            "ITR 1900: L1_c=1.233  L1_a=1.209  L1_e=1.184  L2=0.090  L3=-5.967 || va_L1=5.612  va_L2=0.150  va_L3=-6.008\n",
            "ITR 1910: L1_c=1.276  L1_a=1.306  L1_e=1.300  L2=0.095  L3=-5.864 || va_L1=5.834  va_L2=0.157  va_L3=-6.017\n",
            "ITR 1920: L1_c=1.383  L1_a=1.311  L1_e=1.421  L2=0.094  L3=-5.863 || va_L1=5.389  va_L2=0.161  va_L3=-5.985\n",
            "ITR 1930: L1_c=1.366  L1_a=1.387  L1_e=1.331  L2=0.096  L3=-5.836 || va_L1=5.907  va_L2=0.179  va_L3=-5.900\n",
            "ITR 1940: L1_c=1.328  L1_a=1.244  L1_e=1.278  L2=0.092  L3=-5.750 || va_L1=5.547  va_L2=0.153  va_L3=-5.853\n",
            "ITR 1950: L1_c=1.233  L1_a=1.246  L1_e=1.220  L2=0.088  L3=-5.755 || va_L1=5.897  va_L2=0.150  va_L3=-5.928\n",
            "ITR 1960: L1_c=1.212  L1_a=1.218  L1_e=1.239  L2=0.082  L3=-5.855 || va_L1=5.726  va_L2=0.157  va_L3=-5.957\n",
            "ITR 1970: L1_c=1.245  L1_a=1.206  L1_e=1.201  L2=0.094  L3=-5.791 || va_L1=6.104  va_L2=0.171  va_L3=-5.859\n",
            "ITR 1980: L1_c=1.165  L1_a=1.210  L1_e=1.240  L2=0.080  L3=-5.905 || va_L1=6.582  va_L2=0.170  va_L3=-5.975\n",
            "ITR 1990: L1_c=1.245  L1_a=1.205  L1_e=1.250  L2=0.089  L3=-5.836 || va_L1=6.106  va_L2=0.141  va_L3=-5.887\n",
            "ITR 2000: L1_c=1.170  L1_a=1.170  L1_e=1.190  L2=0.081  L3=-5.738 || va_L1=5.992  va_L2=0.146  va_L3=-5.841\n",
            "ITR 2010: L1_c=1.276  L1_a=1.219  L1_e=1.218  L2=0.084  L3=-5.706 || va_L1=5.889  va_L2=0.191  va_L3=-5.815\n",
            "ITR 2020: L1_c=1.214  L1_a=1.230  L1_e=1.310  L2=0.082  L3=-5.822 || va_L1=5.464  va_L2=0.168  va_L3=-5.820\n",
            "ITR 2030: L1_c=1.308  L1_a=1.248  L1_e=1.258  L2=0.081  L3=-5.609 || va_L1=5.707  va_L2=0.149  va_L3=-5.750\n",
            "ITR 2040: L1_c=1.286  L1_a=1.255  L1_e=1.283  L2=0.084  L3=-5.553 || va_L1=5.681  va_L2=0.171  va_L3=-5.752\n",
            "ITR 2050: L1_c=1.266  L1_a=1.321  L1_e=1.289  L2=0.091  L3=-5.342 || va_L1=5.334  va_L2=0.160  va_L3=-5.607\n",
            "ITR 2060: L1_c=1.243  L1_a=1.290  L1_e=1.287  L2=0.094  L3=-5.506 || va_L1=5.519  va_L2=0.158  va_L3=-5.598\n",
            "ITR 2070: L1_c=1.212  L1_a=1.259  L1_e=1.209  L2=0.090  L3=-5.671 || va_L1=5.313  va_L2=0.142  va_L3=-5.712\n",
            "ITR 2080: L1_c=1.229  L1_a=1.243  L1_e=1.231  L2=0.082  L3=-5.707 || va_L1=6.058  va_L2=0.174  va_L3=-5.742\n",
            "ITR 2090: L1_c=1.244  L1_a=1.245  L1_e=1.256  L2=0.088  L3=-5.649 || va_L1=6.031  va_L2=0.144  va_L3=-5.772\n",
            "ITR 2100: L1_c=1.250  L1_a=1.178  L1_e=1.186  L2=0.083  L3=-5.716 || va_L1=5.424  va_L2=0.154  va_L3=-5.808\n",
            "ITR 2110: L1_c=1.187  L1_a=1.158  L1_e=1.199  L2=0.082  L3=-5.770 || va_L1=5.924  va_L2=0.181  va_L3=-5.832\n",
            "ITR 2120: L1_c=1.212  L1_a=1.218  L1_e=1.193  L2=0.080  L3=-5.782 || va_L1=5.618  va_L2=0.155  va_L3=-5.866\n",
            "ITR 2130: L1_c=1.212  L1_a=1.195  L1_e=1.192  L2=0.088  L3=-5.636 || va_L1=5.434  va_L2=0.168  va_L3=-5.797\n",
            "ITR 2140: L1_c=1.211  L1_a=1.150  L1_e=1.203  L2=0.083  L3=-5.587 || va_L1=6.160  va_L2=0.144  va_L3=-5.840\n",
            "ITR 2150: L1_c=1.191  L1_a=1.224  L1_e=1.245  L2=0.084  L3=-5.688 || va_L1=5.795  va_L2=0.171  va_L3=-5.795\n",
            "ITR 2160: L1_c=1.224  L1_a=1.245  L1_e=1.242  L2=0.085  L3=-5.500 || va_L1=5.713  va_L2=0.178  va_L3=-5.759\n",
            "ITR 2170: L1_c=1.225  L1_a=1.208  L1_e=1.214  L2=0.085  L3=-5.505 || va_L1=6.233  va_L2=0.161  va_L3=-5.716\n",
            "ITR 2180: L1_c=1.207  L1_a=1.220  L1_e=1.201  L2=0.085  L3=-5.512 || va_L1=5.465  va_L2=0.163  va_L3=-5.676\n",
            "ITR 2190: L1_c=1.133  L1_a=1.172  L1_e=1.140  L2=0.081  L3=-5.491 || va_L1=6.008  va_L2=0.155  va_L3=-5.726\n",
            "ITR 2200: L1_c=1.167  L1_a=1.167  L1_e=1.215  L2=0.084  L3=-5.640 || va_L1=5.285  va_L2=0.175  va_L3=-5.756\n",
            "ITR 2210: L1_c=1.289  L1_a=1.246  L1_e=1.243  L2=0.088  L3=-5.635 || va_L1=5.804  va_L2=0.166  va_L3=-5.790\n",
            "ITR 2220: L1_c=1.196  L1_a=1.212  L1_e=1.224  L2=0.084  L3=-5.765 || va_L1=5.821  va_L2=0.162  va_L3=-5.824\n",
            "ITR 2230: L1_c=1.173  L1_a=1.116  L1_e=1.217  L2=0.084  L3=-5.568 || va_L1=6.412  va_L2=0.149  va_L3=-5.764\n",
            "ITR 2240: L1_c=1.201  L1_a=1.233  L1_e=1.215  L2=0.081  L3=-5.553 || va_L1=5.773  va_L2=0.156  va_L3=-5.707\n",
            "ITR 2250: L1_c=1.237  L1_a=1.284  L1_e=1.238  L2=0.094  L3=-5.730 || va_L1=5.865  va_L2=0.155  va_L3=-5.785\n",
            "ITR 2260: L1_c=1.279  L1_a=1.261  L1_e=1.303  L2=0.095  L3=-5.597 || va_L1=6.133  va_L2=0.142  va_L3=-5.767\n",
            "ITR 2270: L1_c=1.200  L1_a=1.207  L1_e=1.196  L2=0.088  L3=-5.526 || va_L1=5.417  va_L2=0.154  va_L3=-5.690\n",
            "ITR 2280: L1_c=1.165  L1_a=1.167  L1_e=1.162  L2=0.081  L3=-5.640 || va_L1=6.131  va_L2=0.167  va_L3=-5.721\n",
            "ITR 2290: L1_c=1.162  L1_a=1.186  L1_e=1.180  L2=0.080  L3=-5.714 || va_L1=6.224  va_L2=0.167  va_L3=-5.752\n",
            "ITR 2300: L1_c=1.207  L1_a=1.184  L1_e=1.156  L2=0.081  L3=-5.648 || va_L1=5.880  va_L2=0.168  va_L3=-5.706\n",
            "ITR 2310: L1_c=1.175  L1_a=1.167  L1_e=1.216  L2=0.081  L3=-5.495 || va_L1=5.544  va_L2=0.152  va_L3=-5.651\n",
            "ITR 2320: L1_c=1.160  L1_a=1.194  L1_e=1.222  L2=0.080  L3=-5.580 || va_L1=6.008  va_L2=0.151  va_L3=-5.674\n",
            "ITR 2330: L1_c=1.229  L1_a=1.169  L1_e=1.198  L2=0.084  L3=-5.521 || va_L1=6.103  va_L2=0.131  va_L3=-5.636\n",
            "ITR 2340: L1_c=1.164  L1_a=1.161  L1_e=1.167  L2=0.076  L3=-5.593 || va_L1=6.112  va_L2=0.147  va_L3=-5.650\n",
            "ITR 2350: L1_c=1.130  L1_a=1.137  L1_e=1.172  L2=0.077  L3=-5.567 || va_L1=5.947  va_L2=0.149  va_L3=-5.682\n",
            "ITR 2360: L1_c=1.176  L1_a=1.191  L1_e=1.158  L2=0.078  L3=-5.727 || va_L1=6.141  va_L2=0.144  va_L3=-5.700\n",
            "ITR 2370: L1_c=1.155  L1_a=1.162  L1_e=1.173  L2=0.081  L3=-5.729 || va_L1=5.819  va_L2=0.150  va_L3=-5.706\n",
            "ITR 2380: L1_c=1.180  L1_a=1.208  L1_e=1.176  L2=0.086  L3=-5.565 || va_L1=5.459  va_L2=0.166  va_L3=-5.647\n",
            "ITR 2390: L1_c=1.147  L1_a=1.150  L1_e=1.182  L2=0.080  L3=-5.569 || va_L1=5.856  va_L2=0.148  va_L3=-5.689\n",
            "ITR 2400: L1_c=1.184  L1_a=1.200  L1_e=1.181  L2=0.078  L3=-5.611 || va_L1=5.993  va_L2=0.151  va_L3=-5.699\n",
            "ITR 2410: L1_c=1.157  L1_a=1.112  L1_e=1.134  L2=0.080  L3=-5.540 || va_L1=6.183  va_L2=0.144  va_L3=-5.614\n",
            "ITR 2420: L1_c=1.145  L1_a=1.136  L1_e=1.186  L2=0.080  L3=-5.543 || va_L1=7.354  va_L2=0.149  va_L3=-5.628\n",
            "ITR 2430: L1_c=1.112  L1_a=1.161  L1_e=1.137  L2=0.079  L3=-5.660 || va_L1=6.110  va_L2=0.147  va_L3=-5.719\n",
            "ITR 2440: L1_c=1.173  L1_a=1.113  L1_e=1.129  L2=0.076  L3=-5.685 || va_L1=6.266  va_L2=0.160  va_L3=-5.768\n",
            "ITR 2450: L1_c=1.191  L1_a=1.214  L1_e=1.171  L2=0.081  L3=-5.676 || va_L1=5.853  va_L2=0.162  va_L3=-5.696\n",
            "ITR 2460: L1_c=1.174  L1_a=1.142  L1_e=1.206  L2=0.076  L3=-5.648 || va_L1=5.264  va_L2=0.155  va_L3=-5.760\n",
            "ITR 2470: L1_c=1.164  L1_a=1.143  L1_e=1.172  L2=0.073  L3=-5.588 || va_L1=5.682  va_L2=0.145  va_L3=-5.728\n",
            "ITR 2480: L1_c=1.204  L1_a=1.193  L1_e=1.227  L2=0.076  L3=-5.744 || va_L1=5.622  va_L2=0.150  va_L3=-5.797\n",
            "ITR 2490: L1_c=1.224  L1_a=1.227  L1_e=1.226  L2=0.077  L3=-5.687 || va_L1=5.149  va_L2=0.150  va_L3=-5.715\n",
            "ITR 2500: L1_c=1.198  L1_a=1.149  L1_e=1.189  L2=0.083  L3=-5.515 || va_L1=5.425  va_L2=0.157  va_L3=-5.685\n",
            "ITR 2510: L1_c=1.186  L1_a=1.148  L1_e=1.155  L2=0.080  L3=-5.667 || va_L1=5.293  va_L2=0.142  va_L3=-5.684\n",
            "ITR 2520: L1_c=1.126  L1_a=1.146  L1_e=1.140  L2=0.077  L3=-5.686 || va_L1=5.253  va_L2=0.156  va_L3=-5.663\n",
            "ITR 2530: L1_c=1.159  L1_a=1.148  L1_e=1.136  L2=0.080  L3=-5.692 || va_L1=5.862  va_L2=0.148  va_L3=-5.694\n",
            "ITR 2540: L1_c=1.205  L1_a=1.224  L1_e=1.245  L2=0.082  L3=-5.670 || va_L1=5.061  va_L2=0.141  va_L3=-5.714\n",
            "ITR 2550: L1_c=1.219  L1_a=1.259  L1_e=1.259  L2=0.082  L3=-5.153 || va_L1=6.082  va_L2=0.170  va_L3=-5.434\n",
            "ITR 2560: L1_c=1.238  L1_a=1.241  L1_e=1.216  L2=0.077  L3=-5.426 || va_L1=5.832  va_L2=0.174  va_L3=-5.470\n",
            "ITR 2570: L1_c=1.227  L1_a=1.195  L1_e=1.203  L2=0.076  L3=-5.306 || va_L1=5.542  va_L2=0.149  va_L3=-5.493\n",
            "ITR 2580: L1_c=1.264  L1_a=1.244  L1_e=1.269  L2=0.074  L3=-5.339 || va_L1=5.814  va_L2=0.154  va_L3=-5.532\n",
            "ITR 2590: L1_c=1.150  L1_a=1.171  L1_e=1.165  L2=0.076  L3=-5.500 || va_L1=5.478  va_L2=0.151  va_L3=-5.500\n",
            "ITR 2600: L1_c=1.135  L1_a=1.157  L1_e=1.175  L2=0.071  L3=-5.487 || va_L1=5.510  va_L2=0.112  va_L3=-5.569\n",
            "ITR 2610: L1_c=1.147  L1_a=1.164  L1_e=1.113  L2=0.076  L3=-5.533 || va_L1=5.850  va_L2=0.161  va_L3=-5.600\n",
            "ITR 2620: L1_c=1.218  L1_a=1.180  L1_e=1.216  L2=0.075  L3=-5.497 || va_L1=5.502  va_L2=0.151  va_L3=-5.604\n",
            "ITR 2630: L1_c=1.196  L1_a=1.224  L1_e=1.215  L2=0.080  L3=-5.480 || va_L1=6.379  va_L2=0.136  va_L3=-5.538\n",
            "ITR 2640: L1_c=1.126  L1_a=1.170  L1_e=1.167  L2=0.079  L3=-5.471 || va_L1=6.220  va_L2=0.161  va_L3=-5.519\n",
            "ITR 2650: L1_c=1.127  L1_a=1.148  L1_e=1.109  L2=0.077  L3=-5.411 || va_L1=6.151  va_L2=0.129  va_L3=-5.494\n",
            "ITR 2660: L1_c=1.151  L1_a=1.141  L1_e=1.160  L2=0.073  L3=-5.393 || va_L1=6.074  va_L2=0.131  va_L3=-5.533\n",
            "ITR 2670: L1_c=1.129  L1_a=1.125  L1_e=1.170  L2=0.067  L3=-5.484 || va_L1=6.357  va_L2=0.144  va_L3=-5.575\n",
            "ITR 2680: L1_c=1.142  L1_a=1.160  L1_e=1.168  L2=0.072  L3=-5.624 || va_L1=6.036  va_L2=0.139  va_L3=-5.673\n",
            "ITR 2690: L1_c=1.180  L1_a=1.152  L1_e=1.191  L2=0.070  L3=-5.627 || va_L1=5.942  va_L2=0.130  va_L3=-5.685\n",
            "ITR 2700: L1_c=1.166  L1_a=1.123  L1_e=1.174  L2=0.076  L3=-5.563 || va_L1=6.408  va_L2=0.153  va_L3=-5.639\n",
            "ITR 2710: L1_c=1.172  L1_a=1.162  L1_e=1.189  L2=0.072  L3=-5.495 || va_L1=5.498  va_L2=0.122  va_L3=-5.596\n",
            "ITR 2720: L1_c=1.183  L1_a=1.161  L1_e=1.163  L2=0.071  L3=-5.594 || va_L1=6.091  va_L2=0.137  va_L3=-5.604\n",
            "ITR 2730: L1_c=1.190  L1_a=1.192  L1_e=1.204  L2=0.069  L3=-5.522 || va_L1=5.909  va_L2=0.129  va_L3=-5.607\n",
            "ITR 2740: L1_c=1.213  L1_a=1.159  L1_e=1.191  L2=0.072  L3=-5.359 || va_L1=5.914  va_L2=0.141  va_L3=-5.561\n",
            "ITR 2750: L1_c=1.146  L1_a=1.225  L1_e=1.167  L2=0.073  L3=-5.626 || va_L1=5.761  va_L2=0.116  va_L3=-5.615\n",
            "ITR 2760: L1_c=1.188  L1_a=1.184  L1_e=1.173  L2=0.069  L3=-5.420 || va_L1=5.491  va_L2=0.130  va_L3=-5.628\n",
            "ITR 2770: L1_c=1.159  L1_a=1.158  L1_e=1.115  L2=0.073  L3=-5.507 || va_L1=6.143  va_L2=0.136  va_L3=-5.594\n",
            "ITR 2780: L1_c=1.165  L1_a=1.165  L1_e=1.147  L2=0.072  L3=-5.411 || va_L1=6.305  va_L2=0.159  va_L3=-5.556\n",
            "ITR 2790: L1_c=1.146  L1_a=1.188  L1_e=1.152  L2=0.073  L3=-5.408 || va_L1=6.360  va_L2=0.157  va_L3=-5.516\n",
            "ITR 2800: L1_c=1.141  L1_a=1.150  L1_e=1.180  L2=0.073  L3=-5.249 || va_L1=6.159  va_L2=0.150  va_L3=-5.446\n",
            "ITR 2810: L1_c=1.138  L1_a=1.171  L1_e=1.134  L2=0.077  L3=-5.397 || va_L1=6.308  va_L2=0.136  va_L3=-5.520\n",
            "ITR 2820: L1_c=1.145  L1_a=1.190  L1_e=1.133  L2=0.072  L3=-5.448 || va_L1=5.788  va_L2=0.150  va_L3=-5.584\n",
            "ITR 2830: L1_c=1.211  L1_a=1.189  L1_e=1.208  L2=0.074  L3=-5.497 || va_L1=6.608  va_L2=0.160  va_L3=-5.512\n",
            "ITR 2840: L1_c=1.222  L1_a=1.207  L1_e=1.154  L2=0.073  L3=-5.401 || va_L1=5.875  va_L2=0.147  va_L3=-5.559\n",
            "ITR 2850: L1_c=1.251  L1_a=1.230  L1_e=1.248  L2=0.072  L3=-5.504 || va_L1=5.795  va_L2=0.177  va_L3=-5.603\n",
            "ITR 2860: L1_c=1.207  L1_a=1.207  L1_e=1.194  L2=0.067  L3=-5.489 || va_L1=5.234  va_L2=0.155  va_L3=-5.600\n",
            "ITR 2870: L1_c=1.167  L1_a=1.185  L1_e=1.116  L2=0.072  L3=-5.365 || va_L1=5.265  va_L2=0.144  va_L3=-5.494\n",
            "ITR 2880: L1_c=1.189  L1_a=1.194  L1_e=1.177  L2=0.068  L3=-5.433 || va_L1=5.460  va_L2=0.127  va_L3=-5.553\n",
            "ITR 2890: L1_c=1.140  L1_a=1.199  L1_e=1.140  L2=0.069  L3=-5.533 || va_L1=5.684  va_L2=0.138  va_L3=-5.608\n",
            "ITR 2900: L1_c=1.083  L1_a=1.140  L1_e=1.134  L2=0.071  L3=-5.469 || va_L1=5.960  va_L2=0.114  va_L3=-5.586\n",
            "ITR 2910: L1_c=1.168  L1_a=1.149  L1_e=1.155  L2=0.069  L3=-5.435 || va_L1=6.489  va_L2=0.126  va_L3=-5.554\n",
            "ITR 2920: L1_c=1.181  L1_a=1.172  L1_e=1.179  L2=0.076  L3=-5.465 || va_L1=6.542  va_L2=0.128  va_L3=-5.549\n",
            "ITR 2930: L1_c=1.156  L1_a=1.183  L1_e=1.135  L2=0.070  L3=-5.562 || va_L1=5.981  va_L2=0.120  va_L3=-5.611\n",
            "ITR 2940: L1_c=1.148  L1_a=1.138  L1_e=1.164  L2=0.068  L3=-5.399 || va_L1=5.964  va_L2=0.135  va_L3=-5.599\n",
            "ITR 2950: L1_c=1.154  L1_a=1.204  L1_e=1.149  L2=0.072  L3=-5.467 || va_L1=5.864  va_L2=0.127  va_L3=-5.584\n",
            "ITR 2960: L1_c=1.140  L1_a=1.123  L1_e=1.173  L2=0.068  L3=-5.413 || va_L1=5.258  va_L2=0.135  va_L3=-5.525\n",
            "ITR 2970: L1_c=1.156  L1_a=1.236  L1_e=1.167  L2=0.075  L3=-5.202 || va_L1=5.545  va_L2=0.152  va_L3=-5.448\n",
            "ITR 2980: L1_c=1.133  L1_a=1.153  L1_e=1.147  L2=0.065  L3=-5.516 || va_L1=5.980  va_L2=0.138  va_L3=-5.574\n",
            "ITR 2990: L1_c=1.196  L1_a=1.244  L1_e=1.207  L2=0.070  L3=-5.302 || va_L1=6.559  va_L2=0.142  va_L3=-5.627\n",
            "ITR 3000: L1_c=1.140  L1_a=1.192  L1_e=1.142  L2=0.069  L3=-5.254 || va_L1=6.115  va_L2=0.146  va_L3=-5.499\n",
            "=============================================\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "va_avg_loss_L1 + alpha*va_avg_loss_L2 + beta*va_avg_loss_L3"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YmEN1FvSxyu2",
        "outputId": "3a98d1b5-70e6-4e53-8651-33c5db424648"
      },
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.0"
            ]
          },
          "metadata": {},
          "execution_count": 23
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "saver.save(sess, save_path + 'models/model_K{}'.format(K))\n",
        "\n",
        "save_logging(network_settings, save_path + 'models/network_settings_K{}.txt'.format(K))\n",
        "np.savez(save_path + 'models/embeddings.npz', e=e)"
      ],
      "metadata": {
        "id": "IT8mx8qaCB_c"
      },
      "execution_count": 24,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "saver.restore(sess, save_path + 'models/model_K{}'.format(K))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MOnxvygHCCFH",
        "outputId": "0b20cedb-7091-4fa3-a382-577258c5a053"
      },
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "INFO:tensorflow:Restoring parameters from ./sample/proposed/trained/models/model_K6\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "_, tmp_pi, tmp_m = model.predict_zbars_and_pis_m2(te_data_x)\n",
        "\n",
        "tmp_pi = tmp_pi.reshape([-1, K])[tmp_m.reshape([-1]) == 1]\n",
        "\n",
        "ncol = nrow = int(np.ceil(np.sqrt(K)))\n",
        "plt.figure(figsize=[4*ncol, 2*nrow])\n",
        "for k in range(K):\n",
        "    plt.subplot(ncol, nrow, k+1)\n",
        "    plt.hist(tmp_pi[:, k])\n",
        "plt.suptitle(\"Clustering assignment probabilities\")\n",
        "plt.show()\n",
        "# plt.savefig(save_path + 'results/figure_clustering_assignments.png')\n",
        "plt.close()\n",
        "\n",
        "\n",
        "# In[147]:\n",
        "\n",
        "\n",
        "# check selector outputs and intialized classes\n",
        "pred_y, tmp_m = model.predict_s_sample(tr_data_x)\n",
        "\n",
        "pred_y = pred_y.reshape([-1, 1])[tmp_m.reshape([-1]) == 1]\n",
        "print(np.unique(pred_y))\n",
        "\n",
        "plt.hist(pred_y[:, 0], bins=15, color='C1', alpha=1.0)\n",
        "plt.show()\n",
        "# plt.savefig(save_path + 'results/figure_clustering_hist.png')\n",
        "plt.close()\n",
        "\n",
        "\n",
        "tmp_y, tmp_m = model.predict_y_bars(te_data_x)\n",
        "\n",
        "\n",
        "y_pred = tmp_y.reshape([-1, y_dim])[tmp_m.reshape([-1]) == 1]\n",
        "y_true = te_data_y.reshape([-1, y_dim])[tmp_m.reshape([-1]) == 1]\n",
        "\n",
        "\n",
        "AUROC = np.zeros([y_dim])\n",
        "AUPRC = np.zeros([y_dim])\n",
        "for y_idx in range(y_dim):\n",
        "    auroc, auprc = f_get_prediction_scores(y_true[:, y_idx], y_pred[:, y_idx])\n",
        "    AUROC[y_idx] = auroc\n",
        "    AUPRC[y_idx] = auprc\n",
        "\n",
        "print('AUROC: {}'.format(AUROC))\n",
        "print('AUPRC: {}'.format(AUPRC))\n",
        "\n",
        "pred_y, tmp_m = model.predict_s_sample(te_data_x)\n",
        "\n",
        "pred_y = (pred_y * tmp_m).reshape([-1, 1])\n",
        "pred_y = pred_y[(tmp_m.reshape([-1, 1]) == 1)[:, 0], 0]\n",
        "\n",
        "true_y = (te_data_y * np.tile(np.expand_dims(tmp_m, axis=2), [1,1,y_dim])).reshape([-1, y_dim])\n",
        "true_y = true_y[(tmp_m.reshape([-1]) == 1)]\n",
        "true_y = np.argmax(true_y, axis=1)\n",
        "\n",
        "tmp_nmi    = normalized_mutual_info_score(true_y, pred_y)\n",
        "tmp_ri     = adjusted_rand_score(true_y, pred_y)\n",
        "tmp_purity = purity_score(true_y, pred_y)\n",
        "\n",
        "print('NMI:{:.4f}, RI:{:.4f}, PURITY:{:.4f}'.format(tmp_nmi, tmp_ri, tmp_purity))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 632
        },
        "id": "q6462IHDCCH3",
        "outputId": "819be908-34f6-428c-c7ae-485c94f24110"
      },
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 864x432 with 6 Axes>"
            ]
          },
          "metadata": {
            "needs_background": "light"
          }
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[0 1 2 5]\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "needs_background": "light"
          }
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "AUROC: [0.88063352 0.85026733 0.87599105]\n",
            "AUPRC: [0.13928127 0.85085517 0.75710277]\n",
            "NMI:0.2905, RI:0.3345, PURITY:0.8145\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "AUPRC.mean()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uv004O8WCH1w",
        "outputId": "4fbe058c-caea-476f-db7e-9638e58ad6c8"
      },
      "execution_count": 27,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.5824130691662095"
            ]
          },
          "metadata": {},
          "execution_count": 27
        }
      ]
    }
  ]
}