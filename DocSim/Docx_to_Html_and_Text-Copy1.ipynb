{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import\n",
    "import glob\n",
    "import os\n",
    "import docx\n",
    "from docx.document import Document\n",
    "from docx import Document as Dcmnt\n",
    "from docx.oxml.table import CT_Tbl\n",
    "from docx.oxml.text.paragraph import CT_P\n",
    "from docx.table import _Cell, Table\n",
    "from docx.text.paragraph import Paragraph\n",
    "import pandas as pd, numpy as np\n",
    "import re\n",
    "from data_preparation import int_to_roman\n",
    "import mammoth\n",
    "from bs4 import BeautifulSoup\n",
    "from thefuzz import fuzz"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <u>Paths</u>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# PDF folder\n",
    "#path_pdf = '/rwi/users/schettiath/SOP TO/01 Raw/01 PDFs/'\n",
    "\n",
    "# docx files with headers and footers\n",
    "#path_docx = '/rwi/users/schettiath/SOP TO/01 Raw/02 docx with Header Footer/Completed (Summary Created 2022-10-12)/' #Originally used for pdfs to docxs\n",
    "#path_docx = '/rwi/users/schettiath/SOP TO/01 Raw/00 Raw docx/' #Used for raw docxs\n",
    "path_docx = '/rwi/users/schettiath/SOP TO/01 Raw/02 docx with Header Footer/Completed (Summary Created 2022-12-07)/' # Used for POL 162, 168 & 229\n",
    "\n",
    "# docx files without headers and footers\n",
    "#path_docx_no_head_foot = '/rwi/users/schettiath/SOP TO/01 Raw/03 docx No Header Footer/Completed (Summary Created 2022-10-12)/' #Originally used for pdfs to docxs\n",
    "#path_docx_no_head_foot = '/rwi/users/schettiath/SOP TO/01 Raw/03 docx No Header Footer/' #Used for raw docxs\n",
    "path_docx_no_head_foot = '/rwi/users/schettiath/SOP TO/01 Raw/03 docx No Header Footer/Completed (Summary Created 2022-12-07)/' # Used for POL 162, 168 & 229\n",
    "\n",
    "# Unclean txts\n",
    "#path_unclean_txt = '/rwi/users/schettiath/SOP TO/01 Raw/04 Unclean txt/Completed (Summary Created 2022-10-12)/' #Originally used for pdfs to docxs\n",
    "#path_unclean_txt = '/rwi/users/schettiath/SOP TO/01 Raw/04 Unclean txt/' #Used for raw docxs\n",
    "path_unclean_txt = '/rwi/users/schettiath/SOP TO/01 Raw/04 Unclean txt/Completed (Summary Created 2022-12-07)/' # Used for POL 162, 168 & 229\n",
    "\n",
    "# Clean txts\n",
    "#path_clean_txt = '/rwi/users/schettiath/SOP TO/01 Raw/05 Clean txt/Completed (Summary Created 2022-10-12)/' #Originally used for pdfs to docxs\n",
    "#path_clean_txt = '/rwi/users/schettiath/SOP TO/01 Raw/05 Clean txt/' #Used for raw docxs\n",
    "path_clean_txt = '/rwi/users/schettiath/SOP TO/01 Raw/05 Clean txt/Completed (Summary Created 2022-12-07)/' # Used for POL 162, 168 & 229"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <u>Remove Headers and Footers from docx</u>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pull names of all documents ending with '.docx' inside 'path_docx'\n",
    "names = [os.path.basename(x) for x in glob.glob(path_docx + '*.docx')]\n",
    "\n",
    "# Remove headers & footers\n",
    "for name in names:\n",
    "    doc = docx.Document(path_docx + name)\n",
    "    for section in doc.sections:\n",
    "        section.different_first_page_header_footer = False\n",
    "        section.header.is_linked_to_previous = True # False if want to keep the header\n",
    "        section.footer.is_linked_to_previous = True # False if want to keep the footer\n",
    "    doc.save(path_docx_no_head_foot + name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <u>docx to txt</u>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================================================== Initial length of MDL: 228\n",
      "POL-162\n",
      "================================================== all_elem_with_2_cap_letters: 89\n",
      "(POL)\n",
      "POL\n",
      "(IFPMA)\n",
      "(EFPIA)\n",
      "Codes/Regulations/Laws.\n",
      "POL\n",
      "(SOP)\n",
      "(WPD)\n",
      "POL\n",
      "(HCPs)\n",
      "POL\n",
      "SOP-1921\n",
      "CP-GL-SO-POL-0034\n",
      "POL\n",
      "STL-141\n",
      "Pan-European\n",
      "(PECT)\n",
      "(LC)\n",
      "UK)\n",
      "POL\n",
      "MUST\n",
      "VISION\n",
      "Pan-European\n",
      "(PECT)\n",
      "PECT\n",
      "PECT\n",
      "(CSR)\n",
      "(CSR)\n",
      "RandD\n",
      "CSR\n",
      "(API)\n",
      "API\n",
      "(LC)\n",
      "Publication/Outlet\n",
      "POL-168\n",
      "Codes/Regulations/Laws\n",
      "Pan-European\n",
      "POL\n",
      "VI,\n",
      "Meetings/Events\n",
      "POL-229\n",
      "(POs)\n",
      "POs\n",
      "SOP-1255\n",
      "EMEA\n",
      "POs.\n",
      "(KEEs)\n",
      "(KEEs)\n",
      "KEEs\n",
      "POL-229\n",
      "KEEs\n",
      "Online/Digital\n",
      "SOP-1771\n",
      "EMEA\n",
      "SOP\n",
      "SOP-1772\n",
      "EMEA\n",
      "SOP\n",
      "POL-293\n",
      "EMEA\n",
      "LinkedIn,\n",
      "YouTube,\n",
      "SOP-1921\n",
      "CP-GL-SO-POL-0034\n",
      "EMEA\n",
      "SOP-1921\n",
      "SOPs\n",
      "(SOP-1129\n",
      "SOP-256)\n",
      "POL\n",
      "CP-GL-SO-POL-0034\n",
      "CP-GL-SO-STL-0003-EN\n",
      "POL-293\n",
      "EMEA\n",
      "SOP-2154\n",
      "EMEA\n",
      "POL-229\n",
      "POL-168\n",
      "SOP-1255\n",
      "EMEA\n",
      "SOP-1771\n",
      "EMEA\n",
      "SOP\n",
      "SOP-1772\n",
      "EMEA\n",
      "SOP\n",
      "SOP-1921\n",
      "SOP-2046\n",
      "EMEA\n",
      "################################################## DONE\n",
      "================================================== initial_abbrevs: 67\n",
      "(POL)\n",
      "POL\n",
      "(IFPMA)\n",
      "(EFPIA)\n",
      "POL\n",
      "(SOP)\n",
      "(WPD)\n",
      "POL\n",
      "(HCPs)\n",
      "POL\n",
      "CP-GL-SO-POL-0034\n",
      "POL\n",
      "STL-141\n",
      "(PECT)\n",
      "(LC)\n",
      "UK)\n",
      "POL\n",
      "MUST\n",
      "VISION\n",
      "(PECT)\n",
      "PECT\n",
      "PECT\n",
      "(CSR)\n",
      "(CSR)\n",
      "RandD\n",
      "CSR\n",
      "(API)\n",
      "API\n",
      "(LC)\n",
      "POL-168\n",
      "POL\n",
      "VI,\n",
      "POL-229\n",
      "(POs)\n",
      "POs\n",
      "EMEA\n",
      "POs.\n",
      "(KEEs)\n",
      "(KEEs)\n",
      "KEEs\n",
      "POL-229\n",
      "KEEs\n",
      "EMEA\n",
      "SOP\n",
      "EMEA\n",
      "SOP\n",
      "POL-293\n",
      "EMEA\n",
      "LinkedIn,\n",
      "YouTube,\n",
      "CP-GL-SO-POL-0034\n",
      "EMEA\n",
      "SOPs\n",
      "POL\n",
      "CP-GL-SO-POL-0034\n",
      "CP-GL-SO-STL-0003-EN\n",
      "POL-293\n",
      "EMEA\n",
      "EMEA\n",
      "POL-229\n",
      "POL-168\n",
      "EMEA\n",
      "EMEA\n",
      "SOP\n",
      "EMEA\n",
      "SOP\n",
      "EMEA\n",
      "################################################## DONE\n",
      "================================================== abbrevs_minus_nums_minus_at: 67\n",
      "(POL)\n",
      "POL\n",
      "(IFPMA)\n",
      "(EFPIA)\n",
      "POL\n",
      "(SOP)\n",
      "(WPD)\n",
      "POL\n",
      "(HCPs)\n",
      "POL\n",
      "CP-GL-SO-POL-0034\n",
      "POL\n",
      "STL-141\n",
      "(PECT)\n",
      "(LC)\n",
      "UK)\n",
      "POL\n",
      "MUST\n",
      "VISION\n",
      "(PECT)\n",
      "PECT\n",
      "PECT\n",
      "(CSR)\n",
      "(CSR)\n",
      "RandD\n",
      "CSR\n",
      "(API)\n",
      "API\n",
      "(LC)\n",
      "POL-168\n",
      "POL\n",
      "VI,\n",
      "POL-229\n",
      "(POs)\n",
      "POs\n",
      "EMEA\n",
      "POs.\n",
      "(KEEs)\n",
      "(KEEs)\n",
      "KEEs\n",
      "POL-229\n",
      "KEEs\n",
      "EMEA\n",
      "SOP\n",
      "EMEA\n",
      "SOP\n",
      "POL-293\n",
      "EMEA\n",
      "LinkedIn,\n",
      "YouTube,\n",
      "CP-GL-SO-POL-0034\n",
      "EMEA\n",
      "SOPs\n",
      "POL\n",
      "CP-GL-SO-POL-0034\n",
      "CP-GL-SO-STL-0003-EN\n",
      "POL-293\n",
      "EMEA\n",
      "EMEA\n",
      "POL-229\n",
      "POL-168\n",
      "EMEA\n",
      "EMEA\n",
      "SOP\n",
      "EMEA\n",
      "SOP\n",
      "EMEA\n",
      "################################################## DONE\n",
      "================================================== acronym_and_replacement\n",
      "IFPMA ----- International Federation Pharmaceutical Manufacturers Associations\n",
      "EFPIA ----- European Federation Pharmaceutical Industries Associations\n",
      "SOP ----- Standard Operating Procedure\n",
      "WPD ----- Working Practice Document\n",
      "LC ----- Leadership Committee\n",
      "CSR ----- Corporate Social Responsibility\n",
      "CSR ----- Corporate Social Responsibility\n",
      "API ----- Astellas Pharma Inc.\n",
      "LC ----- Leadership Committee\n",
      "advertorials ----- \n",
      "affiliate ----- \n",
      "POs ----- patient organisations\n",
      "KEEs ----- Key External Experts\n",
      "KEEs ----- Key External Experts\n",
      "================================================== Duplicate Acronym\n",
      "SOP\n",
      "================================================== Duplicate Acronym\n",
      "CSR\n",
      "================================================== Duplicate Acronym\n",
      "CSR\n",
      "================================================== Duplicate Acronym\n",
      "API\n",
      "================================================== New length of MDL: 242\n",
      "================================================== abbrevs_minus_nums_minus_at_minus_in_doc_acronyms: 44\n",
      "(POL)\n",
      "POL\n",
      "POL\n",
      "POL\n",
      "(HCPs)\n",
      "POL\n",
      "CP-GL-SO-POL-0034\n",
      "POL\n",
      "STL-141\n",
      "(PECT)\n",
      "UK)\n",
      "POL\n",
      "MUST\n",
      "VISION\n",
      "(PECT)\n",
      "PECT\n",
      "PECT\n",
      "RandD\n",
      "POL-168\n",
      "POL\n",
      "VI,\n",
      "POL-229\n",
      "EMEA\n",
      "POL-229\n",
      "EMEAStandard\n",
      "EMEAStandard\n",
      "POL-293\n",
      "EMEA\n",
      "LinkedIn,\n",
      "YouTube,\n",
      "CP-GL-SO-POL-0034\n",
      "EMEA\n",
      "POL\n",
      "CP-GL-SO-POL-0034\n",
      "CP-GL-SO-STL-0003-EN\n",
      "POL-293\n",
      "EMEA\n",
      "EMEA\n",
      "POL-229\n",
      "POL-168\n",
      "EMEA\n",
      "EMEAStandard\n",
      "EMEAStandard\n",
      "EMEA\n",
      "################################################## DONE\n",
      "================================================== abbrevs_minus_nums_minus_at_minus_in_doc_acronyms_minus_divs: 43\n",
      "(POL)\n",
      "POL\n",
      "POL\n",
      "POL\n",
      "POL\n",
      "CP-GL-SO-POL-0034\n",
      "POL\n",
      "STL-141\n",
      "(PECT)\n",
      "UK)\n",
      "POL\n",
      "MUST\n",
      "VISION\n",
      "(PECT)\n",
      "PECT\n",
      "PECT\n",
      "RandD\n",
      "POL-168\n",
      "POL\n",
      "VI,\n",
      "POL-229\n",
      "EMEA\n",
      "POL-229\n",
      "EMEAStandard\n",
      "EMEAStandard\n",
      "POL-293\n",
      "EMEA\n",
      "LinkedIn,\n",
      "YouTube,\n",
      "CP-GL-SO-POL-0034\n",
      "EMEA\n",
      "POL\n",
      "CP-GL-SO-POL-0034\n",
      "CP-GL-SO-STL-0003-EN\n",
      "POL-293\n",
      "EMEA\n",
      "EMEA\n",
      "POL-229\n",
      "POL-168\n",
      "EMEA\n",
      "EMEAStandard\n",
      "EMEAStandard\n",
      "EMEA\n",
      "################################################## DONE\n",
      "================================================== abbrevs_minus_nums_minus_at_minus_in_doc_acronyms_minus_divs_minus_mdl: 27\n",
      "CP-GL-sample\n",
      "(PECT)\n",
      "UK)\n",
      "MUST\n",
      "VISION\n",
      "(PECT)\n",
      "PECT\n",
      "PECT\n",
      "RandD\n",
      "VI,\n",
      "EMEA\n",
      "EMEAStandard\n",
      "EMEAStandard\n",
      "EMEA\n",
      "LinkedIn,\n",
      "YouTube,\n",
      "CP-GL-sample\n",
      "EMEA\n",
      "CP-GL-sample\n",
      "CP-GL-sample\n",
      "tool-0003-EN\n",
      "EMEA\n",
      "EMEA\n",
      "EMEA\n",
      "EMEAStandard\n",
      "EMEAStandard\n",
      "EMEA\n",
      "################################################## DONE\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "################################################## remaining terms w/at least 2 capital letters: 33\n",
      "PURPOSE###\n",
      "Codes/Regulations/Laws.\n",
      "###II.\n",
      "SCOPE###\n",
      "CP-GL-sample\n",
      "###III.\n",
      "INTRODUCTION###\n",
      "###IV.\n",
      "BACKGROUND###\n",
      "PRINCIPLES###\n",
      "MUST\n",
      "VISION\n",
      "Pan-European\n",
      "(PECT)\n",
      "PECT\n",
      "PECT\n",
      "(CSR)###\n",
      "RandD\n",
      "Publication/Outlet\n",
      "Codes/Regulations/Laws\n",
      "Pan-European\n",
      "VI,\n",
      "PRINCIPLES)\n",
      "Meetings/Events\n",
      "EMEA\n",
      "Online/Digital\n",
      "EMEAStandard\n",
      "Management,Standard\n",
      "EMEAStandard\n",
      "EMEA\n",
      "LinkedIn,\n",
      "YouTube,\n",
      "CP-GL-sample\n",
      "================================================== Final length of MDL: 242\n"
     ]
    }
   ],
   "source": [
    "# Read in stop words\n",
    "stop_words = pd.read_csv('/rwi/users/schettiath/MA Insights/custom_stopword_list_nltk_spacy_scikit_wordcloud_gensim.csv')\n",
    "stop_words = list(stop_words['Stop_Words'])\n",
    "\n",
    "# Read in Division names with acronyms\n",
    "div_names_acronyms = pd.read_csv('/rwi/users/schettiath/Master Definition List/Astellas_Division_Names_Acronyms.csv')\n",
    "\n",
    "# Read in MDL abbreviations & drop NaNs\n",
    "mdl = pd.read_csv('/rwi/users/schettiath/Master Definition List/Master Definition List.csv')\n",
    "mdl = mdl.dropna()\n",
    "print('='*50, 'Initial length of MDL:', len(mdl))\n",
    "\n",
    "# Pull names of all documents ending with '.docx' inside 'path_docx_no_head_foot'\n",
    "names = [os.path.basename(x) for x in glob.glob(path_docx_no_head_foot + '*.docx')]\n",
    "names = [x.replace('.docx', '') for x in names]\n",
    "\n",
    "# Loop through files\n",
    "for name in names:\n",
    "    print(name)\n",
    "    doc = docx.Document(path_docx_no_head_foot + name + '.docx')\n",
    "    \n",
    "    # Convert each paragraph to text\n",
    "    para_text_list = [para.text.strip() for para in doc.paragraphs]\n",
    "    \n",
    "    # Extract text from tables\n",
    "    extracted_table_strings = []\n",
    "    for idx in range(len(doc.tables)):\n",
    "        table = doc.tables[idx]\n",
    "\n",
    "        temp_list = [['' for i in range(len(table.columns))] for j in range(len(table.rows))]\n",
    "        for i, row in enumerate(table.rows):\n",
    "            for j, cell in enumerate(row.cells):\n",
    "                if cell.text:\n",
    "                    temp_list[i][j] = cell.text.replace('\\n', ' ')\n",
    "        df = pd.DataFrame(temp_list)\n",
    "                    \n",
    "        try:\n",
    "            table_strings = [df.iloc[x].str.extract('(.+)').squeeze().str.cat(sep=' ') for x in range(len(df))]\n",
    "        except:\n",
    "            table_strings = [df.iloc[x].str.extract('(.+)').squeeze() for x in range(len(df))]\n",
    "        table_str = ' '.join(table_strings)\n",
    "        extracted_table_strings.append(table_str.strip())\n",
    "    \n",
    "    # Extract the block types of the entire document\n",
    "    def iter_block_items(parent):\n",
    "        \"\"\"\n",
    "        Yield each paragraph and table child within *parent*, in document order.\n",
    "        Each returned value is an instance of either Table or Paragraph. *parent*\n",
    "        would most commonly be a reference to a main Document object, but\n",
    "        also works for a _Cell object, which itself can contain paragraphs and tables.\n",
    "        \"\"\"\n",
    "        if isinstance(parent, Document):\n",
    "            parent_elm = parent.element.body\n",
    "        elif isinstance(parent, _Cell):\n",
    "            parent_elm = parent._tc\n",
    "        else:\n",
    "            raise ValueError(\"something's not right\")\n",
    "\n",
    "        for child in parent_elm.iterchildren():\n",
    "            if isinstance(child, CT_P):\n",
    "                yield Paragraph(child, parent)\n",
    "            elif isinstance(child, CT_Tbl):\n",
    "                yield Table(child, parent)\n",
    "\n",
    "    block_types = [str(type(block)) for block in iter_block_items(doc)]\n",
    "    \n",
    "    # Extract the indices of the text & table block types\n",
    "    text_indices = [i for i, x in enumerate(block_types) if x == \"<class 'docx.text.paragraph.Paragraph'>\"]\n",
    "    table_indices = [i for i, x in enumerate(block_types) if x == \"<class 'docx.table.Table'>\"]\n",
    "    \n",
    "    # Create a combined list of the paragraphs & tables in the order they appear in the doc\n",
    "    para_text_table_list = block_types.copy()\n",
    "    for x, y in zip(text_indices, para_text_list):\n",
    "        para_text_table_list[x] = y\n",
    "    for x, y in zip(table_indices, extracted_table_strings):\n",
    "        para_text_table_list[x] = y\n",
    "\n",
    "    # Make some substitutions & strip\n",
    "    para_text_table_list = [re.sub('\\.+[0-9]{1,2}|NOTE:|\\\\n|\\\\t|â€¢|TABLE\\sOF\\sCONTENTS|SIGNATURE\\sPAGE', ' ', para_text_tab) for para_text_tab in para_text_table_list]\n",
    "    para_text_table_list = [re.sub('AND', 'and', para_text_tab) for para_text_tab in para_text_table_list]\n",
    "    para_text_table_list = [re.sub('\\s+', ' ', para_text_tab).strip() for para_text_tab in para_text_table_list]\n",
    "    para_text_table_list = [para_text_tab.strip() for para_text_tab in para_text_table_list]\n",
    "    \n",
    "    para_text_table_list = [re.sub('^i\\.e\\.', 'ie.', para_text_tab) for para_text_tab in para_text_table_list]\n",
    "    para_text_table_list = [re.sub('\\si\\.e\\.', ' ie.', para_text_tab) for para_text_tab in para_text_table_list]\n",
    "    para_text_table_list = [re.sub('\\(i\\.e\\.', '(ie.', para_text_tab) for para_text_tab in para_text_table_list]\n",
    "    para_text_table_list = [re.sub('^e\\.g\\.', 'eg.', para_text_tab) for para_text_tab in para_text_table_list]\n",
    "    para_text_table_list = [re.sub('\\se\\.g\\.', ' eg.', para_text_tab) for para_text_tab in para_text_table_list]\n",
    "    para_text_table_list = [re.sub('\\(e\\.g\\.', '(eg.', para_text_tab) for para_text_tab in para_text_table_list]\n",
    "\n",
    "    para_text_table_list = [re.sub('^[A-Za-z]\\.|^[IVXL]+\\.|^[0-9]{1,2}\\.[0-9]{1,2}\\.[0-9]{1,2}|^[0-9]{1,2}\\.[0-9]{1,2}|^[0-9]{1,2}\\.|\\s[A-Za-z]\\.|\\s[IVXL]+\\.|\\s[0-9]{1,2}\\.|[IVXL]+:|N/A|NA', ' ', para_text_tab) for para_text_tab in para_text_table_list]\n",
    "\n",
    "    para_text_table_list = [re.sub('^ie\\.', 'i.e.', para_text_tab) for para_text_tab in para_text_table_list]\n",
    "    para_text_table_list = [re.sub('\\sie\\.', ' i.e.', para_text_tab) for para_text_tab in para_text_table_list]\n",
    "    para_text_table_list = [re.sub('\\(ie\\.', '(i.e.', para_text_tab) for para_text_tab in para_text_table_list]\n",
    "    para_text_table_list = [re.sub('^eg\\.', 'e.g.', para_text_tab) for para_text_tab in para_text_table_list]\n",
    "    para_text_table_list = [re.sub('\\seg\\.', ' e.g.', para_text_tab) for para_text_tab in para_text_table_list]\n",
    "    para_text_table_list = [re.sub('\\(eg\\.', '(e.g.', para_text_tab) for para_text_tab in para_text_table_list]\n",
    "    \n",
    "    para_text_table_list = [re.sub('i\\.e\\.\\s', 'i.e., ', para_text_tab) for para_text_tab in para_text_table_list]\n",
    "    para_text_table_list = [re.sub('e\\.g\\.\\s', 'e.g., ', para_text_tab) for para_text_tab in para_text_table_list]    \n",
    "    \n",
    "    para_text_table_list = [re.sub('I[CMJ]{3}E', 'ICMJE', para_text_tab) for para_text_tab in para_text_table_list]\n",
    "    \n",
    "    para_text_table_list = [re.sub('\\s+', ' ', para_text_tab) for para_text_tab in para_text_table_list]\n",
    "    para_text_table_list = [para_text_tab.strip() for para_text_tab in para_text_table_list if len(para_text_tab) > 0]\n",
    "    \n",
    "    # Remove spaces inside the parenthesis but on the edges of the string\n",
    "    match_replacements_tups = []\n",
    "    for para_text_tab in para_text_table_list:\n",
    "        if re.search('\\(.+?\\)', para_text_tab):\n",
    "            matches = re.findall('\\(.+?\\)', para_text_tab)\n",
    "            for match in matches:\n",
    "                split_match = re.split('\\(|\\)', match)\n",
    "                match_2 = split_match[1].strip()\n",
    "                match_2 = '(' + match_2 + ')'\n",
    "                tup = (match, match_2)\n",
    "                match_replacements_tups.append(tup)\n",
    "\n",
    "    # Replace\n",
    "    para_text_table_list_2 = []\n",
    "    for para_text_tab in para_text_table_list:\n",
    "        if re.search('\\(.+?\\)', para_text_tab):\n",
    "            for tup in match_replacements_tups:\n",
    "                para_text_tab = re.sub(tup[0], tup[1][1:-1], para_text_tab)\n",
    "        para_text_table_list_2.append(para_text_tab)\n",
    "\n",
    "    # Ensure there is whitespace before an open parentheses & after a closed parentheses\n",
    "    para_text_table_list_3 = []\n",
    "    for para_text_tab in para_text_table_list_2:\n",
    "        if re.search('\\(.+?\\)', para_text_tab):\n",
    "            temp_split_string = para_text_tab.split('(')\n",
    "            para_text_tab = ' ('.join(temp_split_string)\n",
    "            temp_split_string = para_text_tab.split(')')\n",
    "            para_text_tab = ') '.join(temp_split_string)\n",
    "        para_text_table_list_3.append(para_text_tab)\n",
    "\n",
    "    # Replace long whitespaces with a single whitespace\n",
    "    para_text_table_list_3 = [re.sub('\\s+', ' ', para_text_tab).strip() for para_text_tab in para_text_table_list_3]\n",
    "    \n",
    "    # Expand '(ies)' to three words\n",
    "    para_text_table_list_4 = []\n",
    "    for para_text_tab in para_text_table_list_3:\n",
    "        if re.search('y\\s\\(ies\\)', para_text_tab):\n",
    "            para_text_tab = re.sub('y\\s\\(ies\\)', 'y(ies)', para_text_tab)\n",
    "            temp_split_string = para_text_tab.split()\n",
    "            temp_idx = []\n",
    "            replacement_text = []\n",
    "            for idx, elem in enumerate(temp_split_string):\n",
    "                if re.search('y\\(ies\\)', elem):\n",
    "                    base_word = re.findall('([A-Za-z]+)y\\(ies\\)', elem)[0]\n",
    "                    elem = base_word + 'y or ' + base_word + 'ies'\n",
    "                    temp_idx.append(idx)\n",
    "                    replacement_text.append(elem)\n",
    "            for idx, new_text in zip(temp_idx, replacement_text):\n",
    "                temp_split_string[idx] = new_text\n",
    "            para_text_tab = ' '.join(temp_split_string)\n",
    "        para_text_table_list_4.append(para_text_tab)\n",
    "    \n",
    "    # Expand '(s)' to three words\n",
    "    para_text_table_list_5 = []\n",
    "    for para_text_tab in para_text_table_list_4:\n",
    "        if re.search('[A-Za-z]\\s\\(s\\)', para_text_tab):\n",
    "            para_text_tab = re.sub('\\s\\(s\\)', '(s)', para_text_tab)\n",
    "            temp_split_string = para_text_tab.split()\n",
    "            temp_idx = []\n",
    "            replacement_text = []\n",
    "            for idx, elem in enumerate(temp_split_string):\n",
    "                if re.search('[A-Za-z]\\(s\\)', elem):\n",
    "                    base_word = re.findall('([A-Za-z]+)\\(s\\)', elem)[0]\n",
    "                    elem = base_word + ' or ' + base_word + 's'\n",
    "                    temp_idx.append(idx)\n",
    "                    replacement_text.append(elem)\n",
    "            for idx, new_text in zip(temp_idx, replacement_text):\n",
    "                temp_split_string[idx] = new_text\n",
    "            para_text_tab = ' '.join(temp_split_string)\n",
    "        para_text_table_list_5.append(para_text_tab)\n",
    "    \n",
    "    # Compute summary statistics\n",
    "    all_elem_with_2_cap_letters = []\n",
    "    initial_abbrevs = []\n",
    "    for para_text_tab in para_text_table_list_5:\n",
    "        temp_split_string = para_text_tab.split()\n",
    "        for elem in temp_split_string:\n",
    "#             if re.search('DDR', elem):\n",
    "#                 print('='*50, 'DDR', 'index #', para_text_table_list_5.index(para_text_tab))\n",
    "#                 print(para_text_tab)\n",
    "            if not re.search('PURPOSE|SCOPE|DEFINITION|INTRODUCTION|OVERVIEW|BACKGROUND|GENERAL|PRINC[A-Z]+|ROLES|PROCEDURE|DOCUMENT|HISTORY|REFERENCES|ATTACHMENTS', elem):\n",
    "                cap_matches = re.findall('[A-Z]', elem)\n",
    "                len_cap_matches = len(cap_matches)\n",
    "                lower_matches = re.findall('[a-z]', elem)\n",
    "                len_lower_matches = len(lower_matches)\n",
    "                len_elem = len(elem)\n",
    "                if len_cap_matches >= 2:\n",
    "                    all_elem_with_2_cap_letters.append(elem)\n",
    "                    if len_cap_matches/len_elem < 1:\n",
    "                        if len_lower_matches > 0:\n",
    "                            if len_cap_matches/len_lower_matches > 0.25:\n",
    "                                initial_abbrevs = initial_abbrevs + [elem]\n",
    "                        elif len_cap_matches/len_elem > 0.4:\n",
    "                            initial_abbrevs = initial_abbrevs + [elem]\n",
    "                    else:\n",
    "                        initial_abbrevs = initial_abbrevs + [elem]\n",
    "    print('='*50, 'all_elem_with_2_cap_letters:', len(all_elem_with_2_cap_letters))\n",
    "    for x in all_elem_with_2_cap_letters:\n",
    "        print(x)\n",
    "    print('#'*50, 'DONE')\n",
    "    print('='*50, 'initial_abbrevs:', len(initial_abbrevs))\n",
    "    for x in initial_abbrevs:\n",
    "        print(x)\n",
    "    print('#'*50, 'DONE')\n",
    "    \n",
    "    # 2022-12-05: Evaluate if this step is needed or if it can be deleted\n",
    "    # Remove elements that are all numbers or contain an @\n",
    "    para_text_table_list_6 = []\n",
    "    for para_text_tab in para_text_table_list_5:\n",
    "        if re.search('\\([^\\s]+\\)', para_text_tab):\n",
    "            temp_split_string = para_text_tab.split()\n",
    "            to_delete_idx_list = []\n",
    "            for idx, elem in enumerate(temp_split_string):\n",
    "                if re.search('\\([^\\s]+\\)', elem):\n",
    "                    acronym = re.findall('\\(([^\\s]+)\\)', elem)[0]\n",
    "                    if acronym.isnumeric() or re.search('@', acronym):\n",
    "                        to_delete_idx_list.append(idx)\n",
    "            temp_split_string = list(np.delete(temp_split_string, to_delete_idx_list))\n",
    "            para_text_tab = ' '.join(temp_split_string)\n",
    "        para_text_table_list_6.append(para_text_tab)  \n",
    "\n",
    "    # Compute summary statistics\n",
    "    abbrevs_list = []\n",
    "    for para_text_tab in para_text_table_list_6:\n",
    "        temp_split_string = para_text_tab.split()\n",
    "        for elem in temp_split_string:\n",
    "#             if re.search('DDR', elem):\n",
    "#                 print('='*50, 'DDR', 'index #', para_text_table_list_6.index(para_text_tab))\n",
    "#                 print(para_text_tab)\n",
    "            if not re.search('PURPOSE|SCOPE|DEFINITION|INTRODUCTION|OVERVIEW|BACKGROUND|GENERAL|PRINC[A-Z]+|ROLES|PROCEDURE|DOCUMENT|HISTORY|REFERENCES|ATTACHMENTS', elem):\n",
    "                cap_matches = re.findall('[A-Z]', elem)\n",
    "                len_cap_matches = len(cap_matches)\n",
    "                lower_matches = re.findall('[a-z]', elem)\n",
    "                len_lower_matches = len(lower_matches)\n",
    "                len_elem = len(elem)\n",
    "                if len_cap_matches >= 2:\n",
    "                    if len_cap_matches/len_elem < 1:\n",
    "                        if len_lower_matches > 0:\n",
    "                            if len_cap_matches/len_lower_matches > 0.25:\n",
    "                                abbrevs_list = abbrevs_list + [elem]\n",
    "                        elif len_cap_matches/len_elem > 0.4:\n",
    "                            abbrevs_list = abbrevs_list + [elem]\n",
    "                    else:\n",
    "                        abbrevs_list = abbrevs_list + [elem]\n",
    "    print('='*50, 'abbrevs_minus_nums_minus_at:', len(abbrevs_list))\n",
    "    for x in abbrevs_list:\n",
    "        print(x)\n",
    "    print('#'*50, 'DONE')\n",
    "        \n",
    "    # Replace matching acronyms within the active doc with their long forms\n",
    "    complete_acronyms_list = []\n",
    "    complete_terms_list = []\n",
    "    para_text_table_list_7 = []\n",
    "    for para_text_tab in para_text_table_list_6:  # Test to confirm that all parentheses containing anything but spaces only house abbreviations\n",
    "        if re.search('\\([^\\s]+\\)', para_text_tab):\n",
    "            temp_split_string = para_text_tab.split()\n",
    "            acronym_idx_and_replacement = []\n",
    "            to_delete_idx_list = []\n",
    "            for idx, elem in enumerate(temp_split_string):\n",
    "                if re.search('\\([^\\s]+\\)', elem):\n",
    "                    acronym = re.findall('\\(([^\\s]+)\\)', elem)[0]\n",
    "                    cap_letters_nums_list = re.findall('[A-Z0-9]', acronym)\n",
    "                    len_cap_letters_nums_list = len(cap_letters_nums_list)\n",
    "                    acronym_words_idx = []\n",
    "                    acronym_words = []\n",
    "                    for num in range(len_cap_letters_nums_list, 0, -1):\n",
    "                        idx_2 = idx - num\n",
    "                        acronym_words_idx.append(idx_2)\n",
    "                        acronym_words.append(temp_split_string[idx_2])\n",
    "                    check_list = []\n",
    "                    for letter_or_num, term in zip(cap_letters_nums_list, acronym_words):\n",
    "                        if not re.search('[0-9]', letter_or_num):\n",
    "                            letter_or_num = letter_or_num.lower()\n",
    "                            lower_term = term.lower()\n",
    "                            reg_exp = '^' + letter_or_num\n",
    "                            if re.search(reg_exp, lower_term):\n",
    "                                check_list.append(True)\n",
    "                        else:\n",
    "                            reg_exp = '^' + letter_or_num\n",
    "                            if re.search(reg_exp, term):\n",
    "                                check_list.append(True)\n",
    "                    if len(acronym_words) == len(check_list):\n",
    "                        acronym_idx_and_replacement.append((idx, ' '.join(acronym_words)))\n",
    "                        to_delete_idx_list = to_delete_idx_list + acronym_words_idx\n",
    "                        complete_acronyms_list.append(acronym)\n",
    "                    else:\n",
    "                        temp_split_string_subset = temp_split_string[:idx]\n",
    "                        temp_split_string_subset_2 = [string_elem for string_elem in temp_split_string_subset if string_elem not in stop_words]\n",
    "                        acronym_words = temp_split_string_subset_2[-len_cap_letters_nums_list:]\n",
    "                        check_list = []\n",
    "                        for letter_or_num, term in zip(cap_letters_nums_list, acronym_words):\n",
    "                            if not re.search('[0-9]', letter_or_num):\n",
    "                                letter_or_num = letter_or_num.lower()\n",
    "                                lower_term = term.lower()\n",
    "                                reg_exp = '^' + letter_or_num\n",
    "                                if re.search(reg_exp, lower_term):\n",
    "                                    check_list.append(True)\n",
    "                            else:\n",
    "                                reg_exp = '^' + letter_or_num\n",
    "                                if re.search(reg_exp, term):\n",
    "                                    check_list.append(True)\n",
    "                        if len(acronym_words) == len(check_list):\n",
    "                            acronym_idx_and_replacement.append((idx, ' '.join(acronym_words)))                                \n",
    "                            temp_split_string_subset.reverse()\n",
    "                            acronym_words_idx = [temp_split_string_subset.index(term) for term in acronym_words]\n",
    "                            acronym_words_idx = [ac_w_idx + 1 for ac_w_idx in acronym_words_idx]\n",
    "                            acronym_words_idx = [ac_w_idx*-1 for ac_w_idx in acronym_words_idx]\n",
    "                            acronym_words_idx = [ac_w_idx + len(temp_split_string_subset) for ac_w_idx in acronym_words_idx]\n",
    "                            acronym_words_idx = [val for val in range(acronym_words_idx[0], acronym_words_idx[-1] + 1)]\n",
    "                            to_delete_idx_list = to_delete_idx_list + acronym_words_idx\n",
    "                            complete_acronyms_list.append(acronym)\n",
    "            if len(acronym_idx_and_replacement) > 0:\n",
    "                for tup in acronym_idx_and_replacement:\n",
    "                    complete_terms_list.append(tup[1])\n",
    "                    temp_split_string[tup[0]] = tup[1]\n",
    "                temp_split_string = list(np.delete(temp_split_string, to_delete_idx_list))\n",
    "            para_text_tab = ' '.join(temp_split_string)\n",
    "        para_text_table_list_7.append(para_text_tab)  \n",
    "    para_text_table_list_8 = []\n",
    "    for para_text_tab in para_text_table_list_7:\n",
    "        for acronym, term in zip(complete_acronyms_list, complete_terms_list):\n",
    "            regex = '^' + acronym + '[^A-Za-rt-z]|[^A-Za-z]' + acronym + '$|[^A-Za-z]' + acronym + '[^A-Za-rt-z]'\n",
    "            para_text_tab = re.sub(regex, term, para_text_tab)\n",
    "        para_text_table_list_8.append(para_text_tab)\n",
    "    print('='*50, 'acronym_and_replacement')\n",
    "    for x, y in zip(complete_acronyms_list, complete_terms_list):\n",
    "        print(x, '-----', y)\n",
    "    \n",
    "    # Add in document acronyms to MDL\n",
    "    mdl_acronyms = list(mdl['Acronym/Abbreviation'])\n",
    "    for x in complete_acronyms_list:\n",
    "        if x in mdl_acronyms:\n",
    "            print('='*50, 'Duplicate Acronym')\n",
    "            print(x)\n",
    "            #raise Exception('Acronym already exists in the MDL')\n",
    "    for x, y in zip(complete_acronyms_list, complete_terms_list):\n",
    "        temp_dict = {'Name': y, 'Acronym/Abbreviation': x}\n",
    "        mdl = mdl.append(temp_dict, ignore_index=True)\n",
    "    print('='*50, 'New length of MDL:', len(mdl))\n",
    "    \n",
    "    # Compute summary statistics\n",
    "    abbrevs_list = []\n",
    "    for para_text_tab in para_text_table_list_8:\n",
    "        temp_split_string = para_text_tab.split()\n",
    "        for elem in temp_split_string:\n",
    "#             if re.search('DDR', elem):\n",
    "#                 print('='*50, 'DDR', 'index #', para_text_table_list_8.index(para_text_tab))\n",
    "#                 print(para_text_tab)\n",
    "            if not re.search('PURPOSE|SCOPE|DEFINITION|INTRODUCTION|OVERVIEW|BACKGROUND|GENERAL|PRINC[A-Z]+|ROLES|PROCEDURE|DOCUMENT|HISTORY|REFERENCES|ATTACHMENTS', elem):\n",
    "                cap_matches = re.findall('[A-Z]', elem)\n",
    "                len_cap_matches = len(cap_matches)\n",
    "                lower_matches = re.findall('[a-z]', elem)\n",
    "                len_lower_matches = len(lower_matches)\n",
    "                len_elem = len(elem)\n",
    "                if len_cap_matches >= 2:\n",
    "                    if len_cap_matches/len_elem < 1:\n",
    "                        if len_lower_matches > 0:\n",
    "                            if len_cap_matches/len_lower_matches > 0.25:\n",
    "                                abbrevs_list = abbrevs_list + [elem]\n",
    "                        elif len_cap_matches/len_elem > 0.4:\n",
    "                            abbrevs_list = abbrevs_list + [elem]\n",
    "                    else:\n",
    "                        abbrevs_list = abbrevs_list + [elem]\n",
    "    print('='*50, 'abbrevs_minus_nums_minus_at_minus_in_doc_acronyms:', len(abbrevs_list))\n",
    "    for x in abbrevs_list:\n",
    "        print(x)\n",
    "    print('#'*50, 'DONE')\n",
    "    \n",
    "    # Replace Division acronyms with their long form\n",
    "    div_acronyms = list(div_names_acronyms['Acronym'])\n",
    "    div_names = list(div_names_acronyms['Division'])\n",
    "    para_text_table_list_9 = []\n",
    "    for para_text_tab in para_text_table_list_8:\n",
    "        temp_split_string = para_text_tab.split()\n",
    "        temp_idx_list = []\n",
    "        temp_elem_list = []\n",
    "        for idx, elem in enumerate(temp_split_string):\n",
    "            for acronym, term in zip(div_acronyms, div_names):\n",
    "                if re.search(acronym, elem) and acronym not in complete_acronyms_list:\n",
    "                    regex = '^' + acronym + '[^A-Za-rt-z]|[^A-Za-z]' + acronym + '$|[^A-Za-z]' + acronym + '[^A-Za-rt-z]'\n",
    "                    if acronym == elem or re.search(regex, elem):\n",
    "                        elem = re.sub(acronym, term, elem)\n",
    "                        temp_idx_list.append(idx)\n",
    "                        temp_elem_list.append(elem)\n",
    "        if len(temp_idx_list) > 0:\n",
    "            for idx, elem in zip(temp_idx_list, temp_elem_list):\n",
    "                temp_split_string[idx] = elem\n",
    "        para_text_tab = ' '.join(temp_split_string)\n",
    "        para_text_table_list_9.append(para_text_tab)\n",
    "    \n",
    "    # Compute summary statistics\n",
    "    abbrevs_list = []\n",
    "    for para_text_tab in para_text_table_list_9:\n",
    "        temp_split_string = para_text_tab.split()\n",
    "        for elem in temp_split_string:\n",
    "            if re.search('DDR', elem):\n",
    "                print('='*50, 'DDR', 'index #', para_text_table_list_9.index(para_text_tab))\n",
    "                print(para_text_tab)\n",
    "            if not re.search('PURPOSE|SCOPE|DEFINITION|INTRODUCTION|OVERVIEW|BACKGROUND|GENERAL|PRINC[A-Z]+|ROLES|PROCEDURE|DOCUMENT|HISTORY|REFERENCES|ATTACHMENTS', elem):\n",
    "                cap_matches = re.findall('[A-Z]', elem)\n",
    "                len_cap_matches = len(cap_matches)\n",
    "                lower_matches = re.findall('[a-z]', elem)\n",
    "                len_lower_matches = len(lower_matches)\n",
    "                len_elem = len(elem)\n",
    "                if len_cap_matches >= 2:\n",
    "                    if len_cap_matches/len_elem < 1:\n",
    "                        if len_lower_matches > 0:\n",
    "                            if len_cap_matches/len_lower_matches > 0.25:\n",
    "                                abbrevs_list = abbrevs_list + [elem]\n",
    "                        elif len_cap_matches/len_elem > 0.4:\n",
    "                            abbrevs_list = abbrevs_list + [elem]\n",
    "                    else:\n",
    "                        abbrevs_list = abbrevs_list + [elem]\n",
    "    print('='*50, 'abbrevs_minus_nums_minus_at_minus_in_doc_acronyms_minus_divs:', len(abbrevs_list))\n",
    "    for x in abbrevs_list:\n",
    "        print(x)\n",
    "    print('#'*50, 'DONE')\n",
    "    \n",
    "    # Replace acronyms in the MDL with their full length forms\n",
    "    #mdl.sort_values(by=['Acronym/Abbreviation'], ascending=False, key=lambda x: len(x))  # Confirm this works once pandas==1.1.0\n",
    "    mdl_terms_list = list(mdl['Name'])\n",
    "    mdl_acronyms_list = list(mdl['Acronym/Abbreviation'])\n",
    "    acronyms_to_exclude = ['CDA', 'EC', 'ESI', 'GMS'] + complete_acronyms_list + div_acronyms\n",
    "    para_text_table_list_10 = []\n",
    "    for para_text_tab in para_text_table_list_9:\n",
    "        temp_split_string = para_text_tab.split()\n",
    "        temp_idx_list = []\n",
    "        temp_elem_list = []\n",
    "        for idx, elem in enumerate(temp_split_string):\n",
    "            for acronym, term in zip(mdl_acronyms_list, mdl_terms_list):\n",
    "                if acronym not in acronyms_to_exclude:    \n",
    "                    regex = '^' + acronym + '[^A-Za-rt-z]|[^A-Za-z]' + acronym + '$|[^A-Za-z]' + acronym + '[^A-Za-rt-z]'\n",
    "                    if acronym == elem or re.search(regex, elem):\n",
    "                        elem = re.sub(acronym, term, elem)\n",
    "                        temp_idx_list.append(idx)\n",
    "                        temp_elem_list.append(elem)\n",
    "        for idx, elem in zip(temp_idx_list, temp_elem_list):\n",
    "            temp_split_string[idx] = elem\n",
    "        para_text_tab = ' '.join(temp_split_string)\n",
    "        para_text_table_list_10.append(para_text_tab)\n",
    "        \n",
    "    # Compute summary statistics\n",
    "    abbrevs_list = []\n",
    "    for para_text_tab in para_text_table_list_10:\n",
    "        temp_split_string = para_text_tab.split()\n",
    "        for elem in temp_split_string:\n",
    "#             if re.search('DDR', elem):\n",
    "#                 print('='*50, 'DDR', 'index #', para_text_table_list_10.index(para_text_tab))\n",
    "#                 print(para_text_tab)\n",
    "            if not re.search('PURPOSE|SCOPE|DEFINITION|INTRODUCTION|OVERVIEW|BACKGROUND|GENERAL|PRINC[A-Z]+|ROLES|PROCEDURE|DOCUMENT|HISTORY|REFERENCES|ATTACHMENTS', elem):\n",
    "                cap_matches = re.findall('[A-Z]', elem)\n",
    "                len_cap_matches = len(cap_matches)\n",
    "                lower_matches = re.findall('[a-z]', elem)\n",
    "                len_lower_matches = len(lower_matches)\n",
    "                len_elem = len(elem)\n",
    "                if len_cap_matches >= 2:\n",
    "                    if len_cap_matches/len_elem < 1:\n",
    "                        if len_lower_matches > 0:\n",
    "                            if len_cap_matches/len_lower_matches > 0.25:\n",
    "                                abbrevs_list = abbrevs_list + [elem]\n",
    "                        elif len_cap_matches/len_elem > 0.4:\n",
    "                            abbrevs_list = abbrevs_list + [elem]\n",
    "                    else:\n",
    "                        abbrevs_list = abbrevs_list + [elem]\n",
    "    print('='*50, 'abbrevs_minus_nums_minus_at_minus_in_doc_acronyms_minus_divs_minus_mdl:', len(abbrevs_list))\n",
    "    for x in abbrevs_list:\n",
    "        print(x)\n",
    "    print('#'*50, 'DONE')\n",
    "\n",
    "    # Remove the 'Doc. Title' repeated underscore pattern\n",
    "    to_delete_idx_list = []\n",
    "    for idx, para_text_tab in enumerate(para_text_table_list_10):\n",
    "        if re.search('Doc\\.\\sTitle:|______', para_text_tab):\n",
    "            to_delete_idx_list.append(idx)\n",
    "            if len(to_delete_idx_list) > 1:\n",
    "                idx_2 = idx - 2\n",
    "                if not re.search('Doc\\.\\sTitle:', para_text_tab) and to_delete_idx_list[-2] == idx_2:\n",
    "                    to_delete_idx_list.insert(-1, idx - 1)\n",
    "    try:\n",
    "        para_text_table_list_10 = list(np.delete(para_text_table_list_10, to_delete_idx_list))        \n",
    "    except:\n",
    "        pass\n",
    "    \n",
    "    # Extract section headers that are all capital letters\n",
    "    caps_sec_heads_list = [para_text_tab for para_text_tab in para_text_table_list_10 if not re.search('[\\.\\?\\!]$|[a-z]', para_text_tab) or re.search('^ATTACHMENTS', para_text_tab)]\n",
    "    caps_sec_heads_list = [caps_sec_head for caps_sec_head in caps_sec_heads_list if re.search('^[A-Z]{2}', caps_sec_head) and re.search('[A-Z]{2}$', caps_sec_head) or re.search('^ATTACHMENTS', caps_sec_head)]\n",
    "    rslt = []\n",
    "    [rslt.append(head) for head in caps_sec_heads_list if head not in rslt]\n",
    "    caps_sec_heads_list = rslt\n",
    "\n",
    "    # Clean up all caps section\n",
    "    caps_sec_heads_list_2 = []\n",
    "    for head in caps_sec_heads_list:\n",
    "        idx = para_text_table_list_10.index(head)\n",
    "        if re.search('ATTACHMENTS', head):\n",
    "            head = 'ATTACHMENTS'\n",
    "        if re.search('PROCEDURE', head):\n",
    "            head = 'PROCEDURES'\n",
    "        if re.search('ROLES', head):\n",
    "            head = 'ROLES AND RESPONSIBILITIES'\n",
    "        head = re.sub(\"\\'|\\.\", '', head)\n",
    "        caps_sec_heads_list_2.append(head)\n",
    "        para_text_table_list_10[idx] = head\n",
    "        \n",
    "    # Join & write the unclean string to file; will be used to measure token reduction percentage\n",
    "    unclean_text_string = ' '.join(para_text_table_list_10)\n",
    "#     with open(path_unclean_txt + name + '_uncl_ms' + '.txt', 'w', encoding='utf8') as f:\n",
    "#         f.write(unclean_text_string)\n",
    "#         f.close()\n",
    "    \n",
    "    # Remove selected sections if they exist\n",
    "    if 'DEFINITIONS' in caps_sec_heads_list_2:\n",
    "        idx_1 = caps_sec_heads_list_2.index('DEFINITIONS')\n",
    "        idx_2 = idx_1 + 1\n",
    "        next_sec_head = caps_sec_heads_list_2[idx_2]\n",
    "        def_idx_start = para_text_table_list_10.index('DEFINITIONS') - 1\n",
    "        def_idx_end = para_text_table_list_10.index(next_sec_head) - 1\n",
    "        for x in range(def_idx_end, def_idx_start, -1):\n",
    "            del para_text_table_list_10[x]\n",
    "\n",
    "    if 'ATTACHMENTS' in caps_sec_heads_list_2:\n",
    "        idx_start = para_text_table_list_10.index('ATTACHMENTS')\n",
    "        del para_text_table_list_10[idx_start:]\n",
    "    \n",
    "    if 'REFERENCES' in caps_sec_heads_list_2:\n",
    "        idx_start = para_text_table_list_10.index('REFERENCES')\n",
    "        del para_text_table_list_10[idx_start:]\n",
    "    \n",
    "    if 'DOCUMENT HISTORY' in caps_sec_heads_list_2:\n",
    "        idx_start = para_text_table_list_10.index('DOCUMENT HISTORY')\n",
    "        del para_text_table_list_10[idx_start:]\n",
    "    \n",
    "    # Update index & headers list\n",
    "    idx_caps_sec_heads = []\n",
    "    caps_sec_heads_list_3 = []\n",
    "    for head in caps_sec_heads_list_2:\n",
    "        try:\n",
    "            idx = para_text_table_list_10.index(head)\n",
    "            idx_caps_sec_heads.append(idx)\n",
    "            caps_sec_heads_list_3.append(head)\n",
    "        except:\n",
    "            pass\n",
    "    \n",
    "    # Add roman numerals & tag all caps headers with hashtags\n",
    "    for idx, head in enumerate(caps_sec_heads_list_3):\n",
    "        idx_2 = idx + 1\n",
    "        rom_num = int_to_roman(idx_2)\n",
    "        new_head = '###' + rom_num + '. ' + head + '###'\n",
    "        para_text_table_list_10[idx_caps_sec_heads[idx]] = new_head\n",
    "    #print()    \n",
    "\n",
    "\n",
    "    ##################################\n",
    "    # Start of docx to html conversion\n",
    "    ##################################\n",
    "\n",
    "\n",
    "    # Read in docx again for html conversion\n",
    "    doc = open(path_docx_no_head_foot + name + '.docx', 'rb')\n",
    "    \n",
    "    # Convert to html\n",
    "    html = mammoth.convert_to_html(doc)\n",
    "    soup = BeautifulSoup(html.value)\n",
    "    \n",
    "    # Extract headers that begin with a capital letter or number\n",
    "    if len(soup.find_all('h2')) > 0:\n",
    "        soup_text = soup.prettify()\n",
    "        soup_text = re.sub('\\\\n +', '', soup_text)\n",
    "        soup_text = soup_text.split('<h1>')\n",
    "        soup_text = soup_text[1:]\n",
    "\n",
    "        num_period_headers_tup_list_final = []\n",
    "        cap_letter_period_headers_tup_list_final = []\n",
    "        for sec in soup_text:\n",
    "            if re.search('<h2>', sec):\n",
    "                sub_sects = sec.split('<h2>')[1:]\n",
    "                cap_letter_period_headers_list = []\n",
    "                for sub_sec in sub_sects:\n",
    "                    cap_letter_period_header = re.findall('([^<>]+)</h2>', sub_sec)[0]\n",
    "                    if re.search('^[A-Z]\\.', cap_letter_period_header):\n",
    "                        cap_letter_period_header = cap_letter_period_header[2:].strip()\n",
    "                    cap_letter_period_headers_list.append(cap_letter_period_header)\n",
    "                    cap_letter_period_headers_list = [re.sub('&amp;', '&', head) for head in cap_letter_period_headers_list]\n",
    "                    matches = re.findall('<ol><li>.{0,70}<strong>([^<>]+)</strong></li>|([^<>]+)</h3>', sub_sec) + [('', '')]\n",
    "                    matches = [''.join(match) for match in matches if len(match[0]) == 0 or len(match[1]) == 0]\n",
    "                    if re.search('^[0-9]{1,2}\\.', matches[0]) and not re.search('^1\\.', matches[0]):\n",
    "                        matches = ['']\n",
    "                    match_lens = [len(match) for match in matches]\n",
    "                    if sum(match_lens) > 0:\n",
    "                        if re.search('POL\\-162', name) and 'Working with the Media' in matches:\n",
    "                            idx = matches.index('Working with the Media')\n",
    "                            matches.insert(idx + 1, 'Payment for Space in a Media Publication/Outlet and Journal Feature Articles')   \n",
    "                        matches = [re.sub('^[0-9]{1,2}\\.', '', match).strip() if re.search('^[0-9]{1,2}\\.', match) else match for match in matches]\n",
    "                        matches = [match for match in matches if len(match) > 0]\n",
    "                        matches = [re.sub('&amp;', '&', match) for match in matches]\n",
    "                        num_period_headers_tup_list = [(match, str(idx + 1) + '. ' + match) for idx, match in enumerate(matches)]\n",
    "                        num_period_headers_tup_list_final = num_period_headers_tup_list_final + num_period_headers_tup_list\n",
    "                cap_letter_period_headers_tup_list = [(head, chr(ord('@') + (idx + 1)) + '. ' + head) for idx, head in enumerate(cap_letter_period_headers_list)]\n",
    "                cap_letter_period_headers_tup_list_final = cap_letter_period_headers_tup_list_final + cap_letter_period_headers_tup_list\n",
    "        #print('#' * 50, 'cap_letter_period_headers_tup_list_final')\n",
    "        if len(cap_letter_period_headers_tup_list_final) > 0:\n",
    "            for tup in cap_letter_period_headers_tup_list_final:\n",
    "                #print(tup[1])\n",
    "                print()\n",
    "        else:\n",
    "            #print('X'*50, None)\n",
    "            print()\n",
    "        #print('#' * 50, 'num_period_headers_tup_list_final')\n",
    "        if len(num_period_headers_tup_list_final) > 0:\n",
    "            for tup in num_period_headers_tup_list_final:\n",
    "                #print(tup[1])\n",
    "                print()\n",
    "        else:\n",
    "            #print('X'*50, None)\n",
    "            print()\n",
    "        cap_num_period_headers_tup_list = cap_letter_period_headers_tup_list_final + num_period_headers_tup_list_final\n",
    "    else:\n",
    "        # Extract headers\n",
    "        ol_list = soup.find_all('ol')\n",
    "        \n",
    "        li_list = []\n",
    "        for elem in ol_list:\n",
    "            li_list = li_list + elem.find_all('li')\n",
    "        \n",
    "        strong_list = []\n",
    "        for elem in li_list:\n",
    "            strong_list = strong_list + elem.find_all('strong')\n",
    "        \n",
    "        pure_text_headers = [elem.get_text().strip() for elem in strong_list]\n",
    "        \n",
    "        # Clean headers\n",
    "        clean_headers = []\n",
    "        for elem in pure_text_headers:\n",
    "            if re.search('^[A-Za-z]\\.', elem):\n",
    "                res = re.findall('^[A-Za-z]\\.', elem)\n",
    "                res = elem.split(res[0])[1].strip()\n",
    "                elem = res\n",
    "            elif re.search('^[0-9]{1,2}\\.', elem):\n",
    "                res = re.findall('^[0-9]{1,2}\\.', elem)\n",
    "                res = elem.split(res[0])[1].strip()\n",
    "                elem = res\n",
    "            clean_headers.append(elem)\n",
    "        \n",
    "        # Remove duplicates\n",
    "        rslt = []\n",
    "        [rslt.append(head) for head in clean_headers if head not in rslt]\n",
    "        clean_headers = rslt\n",
    "        \n",
    "        # Remove all caps level 1 headers & add capital letter period pattern to remaining headers\n",
    "        i = 0\n",
    "        cap_letter_period_headers_tup_list = []\n",
    "        for elem in clean_headers:\n",
    "            if not re.search('^[A-Z]{5}', elem):\n",
    "                i += 1\n",
    "                head = chr(ord('@') + i) + '. ' + elem\n",
    "                tup = (elem, head)\n",
    "                cap_letter_period_headers_tup_list.append(tup)\n",
    "            else:\n",
    "                i = 0\n",
    "        \n",
    "        # Concatenate\n",
    "        num_period_headers_tup_list = []\n",
    "        cap_num_period_headers_tup_list = cap_letter_period_headers_tup_list + num_period_headers_tup_list\n",
    "\n",
    "\n",
    "    ##################################\n",
    "    # End of docx to html conversion\n",
    "    ##################################\n",
    "\n",
    "    \n",
    "    # Update index & tag level 2 & 3 section headers\n",
    "    if len(cap_num_period_headers_tup_list) > 0:\n",
    "        idx_cap_num_periods_headers_list = []\n",
    "        cap_num_period_headers_tup_list_2 = []\n",
    "        for tup in cap_num_period_headers_tup_list:\n",
    "            for idx, para_text_tab in enumerate(para_text_table_list_10):\n",
    "                #print(idx, 'YYY', para_text_tab)\n",
    "                fuzz_ratio = fuzz.ratio(tup[0], para_text_tab)\n",
    "                if fuzz_ratio > 90:\n",
    "#                     print('#' * 50, 'Fuzz Ratio')\n",
    "#                     print(tup[0], para_text_tab, fuzz_ratio)\n",
    "                    idx_cap_num_periods_headers_list.append(idx)\n",
    "                    cap_num_period_headers_tup_list_2.append('###' + tup[1] + '###')\n",
    "\n",
    "        # Throw an error if headers counts don't match up\n",
    "        if len(cap_num_period_headers_tup_list) != len(cap_num_period_headers_tup_list_2):\n",
    "            raise Exception('Unequal number of level 3 section headers')\n",
    "\n",
    "        # Replace level 2 & level 3 headers\n",
    "        for idx, head in zip(idx_cap_num_periods_headers_list, cap_num_period_headers_tup_list_2):\n",
    "            para_text_table_list_10[idx] = head\n",
    "    \n",
    "    # Join list into string, remove abbreviations\n",
    "    clean_text_string = ' '.join(para_text_table_list_10)\n",
    "    \n",
    "    # Test to find patterns of any abbreviations that were missed\n",
    "    test_split_string = clean_text_string.split()\n",
    "    remaining_terms_with_2_cap_letters = []\n",
    "    for elem in test_split_string:\n",
    "        matches = re.findall('[A-Z]', elem)\n",
    "        if len(matches) > 1:\n",
    "            remaining_terms_with_2_cap_letters.append(elem)\n",
    "    print('#'*50, 'remaining terms w/at least 2 capital letters:', len(remaining_terms_with_2_cap_letters))\n",
    "    for elem in remaining_terms_with_2_cap_letters:\n",
    "        print(elem)\n",
    "\n",
    "    # Write the clean string to file\n",
    "#     with open(path_clean_txt + name + '_cl_ms' + '.txt', 'w', encoding='utf8') as f:\n",
    "#         f.write(clean_text_string)\n",
    "#         f.close()\n",
    "\n",
    "# Write new MDL to file\n",
    "print('='*50, 'Final length of MDL:', len(mdl))\n",
    "mdl.to_csv('/rwi/users/schettiath/SOP TO/02 Resources/Master Definition List with POL SOP In-Document Acronyms.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==================================================\n",
      "###I. PURPOSE###\n",
      "==================================================\n",
      "The purpose of this Policy (policy) is to describe the principles and minimum standards required when communicating with internal and external stakeholders within the Established Markets region. This policy should be used in conjunction with relevant policies and procedures outlined in the References section at the end of this document.\n",
      "==================================================\n",
      "This policy has been prepared in order to meet the relevant requirements of the International Federation of Pharmaceutical Manufacturers & Associations (IFPMA) and the European Federation Pharmaceutical Industries Associations . All steps must only occur in accordance with Astellas Policies and Procedures and applicable Codes/Regulations/Laws. Where stricter local requirements apply, these always take precedence, and must be adhered to in all circumstances.\n",
      "==================================================\n",
      "The principles within this policy may be used to help develop a country-specific communications Standard Operating Procedure or Working Practice Document .\n",
      "==================================================\n",
      "###II. SCOPE###\n",
      "==================================================\n",
      "This policy applies to all employees and non-employees (e.g., contract staff or agencies) within Established Markets who undertake any activity that requires communication with internal or external stakeholders, including but not limited to: the public, media, professional organisations, patients, patient organisations, healthcare professionals (Healthcare Policys) , finance and business community, politicians and government officials, industry groups, customers, and the broader pharmaceutical industry.\n",
      "==================================================\n",
      "The principles and requirements in this policy apply to all forms of communication (verbal, written, print, digital and online) .\n",
      "==================================================\n",
      "Please refer toStandard Operating Procedure1921 Social Media Management for Established Markets and CP-GL-sample operations-policy-0034 Global Policy on Social Media for guidance on professional and personal use of social media.\n",
      "==================================================\n",
      "Appropriate communications which maintain high professional standards are expected at all times.\n",
      "==================================================\n",
      "This policy should not inhibit appropriate discussions pertinent to our day-to-day business activities, nor inhibit the appropriate exchange of information when acting as a nominated company representative.\n",
      "==================================================\n",
      "###III. INTRODUCTION###\n",
      "==================================================\n",
      "Communications activities and materials communicate newsworthy or educational information to colleagues and key internal and external stakeholders, and build and protect Astellasâ€™ reputation with these audiences. All covered by this policy must be aware of, and adhere to the companyâ€™s high standards of ethical and transparent communication with all stakeholders, and comply with applicable industry Codes of Practice, Regulations and Laws.\n",
      "==================================================\n",
      "###IV. BACKGROUND###\n",
      "==================================================\n",
      "All communications activities and materials undertaken or developed must comply with applicable industry Codes of Practice, Regulations and Laws.\n",
      "==================================================\n",
      "###V. PRINCIPLES###\n",
      "==================================================\n",
      "Communications, whether with internal or external stakeholders, must be in line with our five Astellas Way values: Patient Focus, Ownership, Integrity, Results, and Openness.\n",
      "==================================================\n",
      "It is essential that employees and non-employees (e.g., contract staff) within Established Markets who undertake any activity that requires communication with internal or external stakeholders must inform a member of the Established Markets Communications Team before planning, developing or implementing any external communications activity.\n",
      "==================================================\n",
      "Any information provided to the general public, directly or indirectly (e.g., via the media through appropriate communications channels) , MUST be non-promotional.\n",
      "==================================================\n",
      "###A. Employee Communications###\n",
      "==================================================\n",
      "Employee Communications aim to inform, engage and, align Astellas colleagues with strategic priorities.\n",
      "==================================================\n",
      "Astellas Established Markets Communications dedicated Employee Communications team provides employees (and non-employees) with timely, up-to-date information. The team works to enable and partner with local communicators, leaders and subject matter experts to deliver communications across the Astellas Established Markets division. It is important to ensure our colleagues feel valued and listened to by facilitating a two-way conversation between colleagues and leaders.\n",
      "==================================================\n",
      "The Employee Communications Team has created the following guiding principles to help shape best practice communications at Astellas Established Markets:\n",
      "==================================================\n",
      "Aligned: to ensure we all understand our role in the delivery of the Astellas VISION\n",
      "==================================================\n",
      "Accurate & Honest: to build trust with our audiences.\n",
      "==================================================\n",
      "Timely: to ensure that employees hear our news first internally, whenever possible.\n",
      "==================================================\n",
      "Clear: using a language, and visual aids, to ensure our audience understands.\n",
      "==================================================\n",
      "Engaging: presenting our communications to inspire and engage employees.\n",
      "==================================================\n",
      "Open: to encourage feedback and real conversations between colleagues across Established Markets.\n",
      "==================================================\n",
      "Accessible: using a range of channels to enable our employees to connect.\n",
      "==================================================\n",
      "It is the responsibility of all Astellas colleagues, line managers and senior leaders to communicate employee information appropriately, responsibly, and in a timely manner. All of our colleagues should bear in mind that their employee communications can be subject to external scrutiny. Our employee communications are subject to strict governance and review/approval processes.\n",
      "==================================================\n",
      "The Established Markets Employee Communications Team can offer guidance, advice and support to help ensure compliance with communications governance. Please email: or visit our page on for more information.\n",
      "==================================================\n",
      "###B. External Communications###\n",
      "==================================================\n",
      "###1. Crisis Communications###\n",
      "==================================================\n",
      "Astellas Established Markets has a dedicated crisis team â€“ the Astellas Pan-European Crisis Team (PECT) â€“ which will convene, in the event of a potential or actual crisis, to agree on the action required to resolve the crisis and the corresponding communications required externally and internally. PECT will work in collaboration with the Established Markets Communications Team to respond to a potential or actual crisis.\n",
      "==================================================\n",
      "The Astellas PECT defines a crisis as an event that has the potential, if handled poorly, to have a severe negative impact on Astellasâ€™ normal operations affecting Astellas Established Markets colleagues, patients, distributors, customers and our brand or reputation. The event may generate adverse media coverage, leading to public and/or government scrutiny and criticism.\n",
      "==================================================\n",
      "If you observe an event that you consider to be a potential crisis or that may escalate to one, you should call the crisis team coordinator on +44 7825 104388 and confirm the call with a text and email ().\n",
      "==================================================\n",
      "###2. Disaster Response and Relief (Natural disasters)###\n",
      "==================================================\n",
      "There is a full Disaster Response Process in place across Astellas Established Markets. This defines how Astellas should respond to natural disasters. Please refer to supporting document Astellas Social Contribution Guideline.\n",
      "==================================================\n",
      "The local communication lead in eachis the Disaster Relief champion.\n",
      "==================================================\n",
      "###3. Corporate Social Responsibility (CSR)###\n",
      "==================================================\n",
      "The ethos of Changing tomorrow is at the heart of Astellas' business practice and informs Astellasâ€™ Corporate Social Responsibility programme.\n",
      "==================================================\n",
      "Astellas want to make a positive difference; from how we grow our business, to minimising our impact on the environment, and supporting people in need, both locally and across the world. Being a successful business is not enough; we want to be a valuable member of society, by improving access to health, sharing R&D expertise, supporting local communities and working sustainably.\n",
      "==================================================\n",
      "As part of Astellasâ€™ commitment to supporting local communities, volunteering on Changing tomorrow Day gives employees and contract staff the opportunity to spend time during working hours to help local charities and causes.\n",
      "==================================================\n",
      "To find out more about how to get involved in appropriate volunteering, fundraising or charitable activities, contact\n",
      "==================================================\n",
      "Further information about ourCorporate Social Responsibilitygoals and achievements can be found in the Astellas Annual Corporate Report.\n",
      "==================================================\n",
      "###4. Communicating Material Information###\n",
      "==================================================\n",
      "Astellas Pharma Inc. is a listed company (Tokyo Stock Exchange) . It is a legal requirement that â€œmaterialâ€ company information is disclosed through filing to the Stock Exchange for distribution to the media.\n",
      "==================================================\n",
      "Information about Astellas or its products may be considered material if such information has the potential to change the perceived value of a security (tradable asset) once disclosed to the public. It is essential that the Astellas Established Markets Communications Team is informed of any potential â€œmaterialâ€ or commercially sensitive information as soon as possible, to ensure thatAstellas Pharma Inc.is made aware to ensure the appropriate communications approach.\n",
      "==================================================\n",
      "###5. Government Affairs and Regulatory Affairs###\n",
      "==================================================\n",
      "Any communications that describe or comment on Astellasâ€™ public position in relation to government or public policy, or communications directed at government departments, must be planned and developed through Astellas Established Markets Government Affairs.\n",
      "==================================================\n",
      "Any communications concerning regulatory issues, or direct communication with regulatory bodies, must be conducted through Astellas Established Markets Regulatory Affairs.\n",
      "==================================================\n",
      "All content and material will require approval in accordance with Astellas process and comply with applicable industry Codes of Practice, Regulations and Laws.\n",
      "==================================================\n",
      "###6. Corporate Brand & Product Communications###\n",
      "==================================================\n",
      "Any communications that describe, promote or comment on Astellas as a company or corporate brand, including our mission and values, and any other external communication that may impact on Astellasâ€™ corporate reputation, including media outreach related to our products and / or the therapy areas we work in, must only be conducted by Astellas Established Markets Communications and local communicators unless the Established Markets Communications Team has specifically authorised another Astellas function to do so.\n",
      "==================================================\n",
      "Regardless of location across Established Markets, you are required to notify the Astellas Established Markets Communications Team of any external corporate or product-related media outreach, (e.g., press releases, and reactive press statements) to ensure alignment with corporate branding and communication strategy. If you have such activity planned please contact .\n",
      "==================================================\n",
      "All content and material will require local approval in accordance with Astellas process and with applicable industry Codes of Practice, Regulations and Laws.\n",
      "==================================================\n",
      "###7. Corporate Brand Guidelines and narrative###\n",
      "==================================================\n",
      "Corporate Brand Guidelines are available to inform your style of language and explain how to use appropriate corporate visual assets in your materials, including correct use of logos, fonts and colours. The guidelines are available\n",
      "==================================================\n",
      "The Established Markets Communications Team have developed the Astellas corporate narrative, available on the corporate narrative page on , which provides everyone across Established Markets with a consistent and engaging way to tell the Astellas story:\n",
      "==================================================\n",
      "By using the narrative as our guide it ensures we communicate with clarity and speak with â€˜one voiceâ€™\n",
      "==================================================\n",
      "The language and tone used reflect the Astellas brand and help to articulate our story across all forms of communication, no matter the audience\n",
      "==================================================\n",
      "The proof points provide tangible examples of our story, which can be used to help bring the narrative to life through materials and communications activities. Any materials produced using this narrative and its proof points will need appropriate review and approval in line with local Code, Regulations and Laws, along with applicable Astellas procedures.\n",
      "==================================================\n",
      "###8. Working with the Media###\n",
      "==================================================\n",
      "Only selected and media-trained members of the Astellas Established Markets Leadership Committee and other designated Senior Leaders are authorised to speak directly to journalists.\n",
      "==================================================\n",
      "###9. Payment for Space in a Media Publication/Outlet and Journal Feature Articles###\n",
      "==================================================\n",
      "If Astellas Established Markets, in any way, pays for the space in which content appears in a media outlet, or commissions and pays for a supplement (e.g., in a journal) , then this constitutes an advertisement or advertorial and must be reviewed and approved, and Astellasâ€™ involvement in the development of the content must be made clear in the final form.\n",
      "==================================================\n",
      "In contrast, content which is generated by a third party e.g., news media coverage published by a journalist, which is neither directly nor indirectly created, reviewed, paid for by Astellas Established Markets is known as â€˜earned mediaâ€™.\n",
      "==================================================\n",
      "For publications and/or journal feature articles on Astellas-sponsored product development and related research, please refer to policy-168 Global Publication Policy, other Astellas Policies and Procedures and applicable industry Codes of Practice, Regulations and Laws.\n",
      "==================================================\n",
      "Astellas is responsible for any activity undertaken with its authority and is responsible for any third-party activity which it instigates, controls or pays for either directly or indirectly. This applies whether the third party is acting on specific instructions from the Company, or on its own initiative.\n",
      "==================================================\n",
      "Astellas may be considered responsible for a statement reported by a journalist if the Company has been involved in activities such as:\n",
      "==================================================\n",
      "Briefing the journalist\n",
      "==================================================\n",
      "Facilitating the journalistâ€™s attendance at an Astellas meeting\n",
      "==================================================\n",
      "Editing and approving an article\n",
      "==================================================\n",
      "Influencing the distribution or placement of the article\n",
      "==================================================\n",
      "Paid-for for media placements\n",
      "==================================================\n",
      "Provision of company information to the journalist.\n",
      "==================================================\n",
      "Astellas is responsible for information it provides to the media. To allow for accurate reporting, Astellas must ensure the following:\n",
      "==================================================\n",
      "Briefing material is approved and accurate according to the applicable Codes/Regulations/Laws\n",
      "==================================================\n",
      "Briefing material is supplied in an appropriate way\n",
      "==================================================\n",
      "###10. Media Enquiries###\n",
      "==================================================\n",
      "Media enquiries should be handled by the relevant local or regional team. Affiliates have responsibility for their local journalists.\n",
      "==================================================\n",
      "Pan-European media enquiries should be forwarded to the Press Office, which is staffed by the Established Markets Communications Team, who will respond to media enquiries on behalf of Astellas.\n",
      "==================================================\n",
      "Established Markets Press Office details:\n",
      "==================================================\n",
      "Phone: +44 7919 302926\n",
      "==================================================\n",
      "###18. Email###\n",
      "==================================================\n",
      "###11. Media Materials###\n",
      "==================================================\n",
      "The Astellas Established Markets Communications Team and Local Communicators are responsible for the development and provision of information to the media.\n",
      "==================================================\n",
      "Always consult with Established Markets Communications before considering working with the media or creating any media materials to ensure alignment with our strategy.\n",
      "==================================================\n",
      "All media materials must comply with the general principles stated in this policy (Section VI, PRINCIPLES) .\n",
      "==================================================\n",
      "All media communications materials and activities developed and implemented by Astellas Established Markets (or by agencies/contractors working on Astellasâ€™ behalf) , must have undergone appropriate review and approval in accordance with Astellas process and with applicable industry Codes of Practice, Regulations and Laws.\n",
      "==================================================\n",
      "###12. Meetings/Events Involving the Media###\n",
      "==================================================\n",
      "All arrangements for activities must be undertaken in accordance with policy-229 Meetings and Consultancy Policy or relevant local documents.\n",
      "==================================================\n",
      "Meetings with the media will be led by the Established Markets Communications Team, or by Local Communicators. Astellas Established Markets Communications must comply with applicable industry Codes of Practice, Regulations and Laws.\n",
      "==================================================\n",
      "###13. Media Attendance at Conferences and Journalists Used as Consultants by Astellas###\n",
      "==================================================\n",
      "Consult with your Local Communicators, and Astellas Established Markets Communications to understand the procedures that must be followed for media attendance at conferences and for journalists used as consultants. Any subsequent media activities should be led by these communicators.\n",
      "==================================================\n",
      "Any arrangements with journalists should be reviewed according to the local approval process, Codes of Practice, Regulations and Laws in the country where the conference is taking place and in the countries where the journalists attending come from.\n",
      "==================================================\n",
      "###14. Working with Patient Organisations###\n",
      "==================================================\n",
      "Engagement with patient organisations may be permitted once all relevant compliance review and approvals have been obtained. This includes proactive engagement withpatient organisationsto obtain insight to inform communications or communications activities, in partnership with them. Refer toStandard Operating Procedure1255 EMEA Working with Patient Organisations or relevant local documents for details on which functions must be included for review and approvals, and the processes to follow when engaging withpatient organisations\n",
      "==================================================\n",
      "Working with Key External Experts\n",
      "==================================================\n",
      "Astellas Established Markets may engage with Key External Experts to support communications activities.\n",
      "==================================================\n",
      "All arrangements for activities involvingKey External Expertsmust be approved in accordance with policy-229 Meetings and Consultancy Policy or relevant local documents.\n",
      "==================================================\n",
      "It is recommended that basic media training, along with training on applicable industry Codes of Practice, Regulations and Laws and product information, if appropriate, is offered toKey External Expertswho have been engaged by Astellas Established Markets to provide services for Astellas, which may include media activities.\n",
      "==================================================\n",
      "###16. Speaker Opportunities###\n",
      "==================================================\n",
      "Astellas Established Markets employees, when speaking in a personal capacity, may be perceived to be acting on behalf of Astellas.\n",
      "==================================================\n",
      "Any Astellas employee invited to engage in external speaker opportunities as a representative of Astellas (or in a personal capacity) must seek approval from their line manager, prior to acceptance of the invitation. Any subsequent materials used (such as presentation slides or script) should be reviewed and approved prior to use in accordance with Astellas processes, applicable industry Codes of Practice, Regulations and Laws.\n",
      "==================================================\n",
      "An Astellas Established Markets employee (or non-employee) who is thinking of engaging as an external speaker should consider using the corporate presentation if appropriate.\n",
      "==================================================\n",
      "###17. Online/Digital Communications###\n",
      "==================================================\n",
      "Online or electronic communications are subject to the same rules that apply to all our other company communications. All online media activity must adhere to Astellas processes, applicable Codes of Practice, Regulations and Laws. This applies to company/corporate websites, and content created by Astellas appearing on third-party websites.\n",
      "==================================================\n",
      "Refer toStandard Operating Procedure1771 EMEAStandard Operating Procedurefor Mobile App Lifecycle Management,Standard Operating Procedure1772 EMEAStandard Operating Procedurefor Website Lifecycle Management, and policy-293 EMEA Digital Governance Policy for our Digital Governance Policy.\n",
      "==================================================\n",
      "Any online/digital media activities or content planned should be communicated to Astellas Established Markets Communications ahead of initiation.\n",
      "==================================================\n",
      "###18. Email###\n",
      "==================================================\n",
      "Astellas Established Markets email accounts are for communicating for professional purposes only and should not be used for personal communications.\n",
      "==================================================\n",
      "Ensure that you are using the correct Astellas email signature to sign off your emails. This includes links to the corporate website and social media channels in your/ region. Please refer to the Corporate Brand Site on for email signature guidance.\n",
      "==================================================\n",
      "###19. Social Media â€“ professional and personal use###\n",
      "==================================================\n",
      "Social media includes, but is not limited to, external and internal social networks (Facebook, Twitter, LinkedIn, YouTube, Instagram, etc.) , blogs, wikis, and team collaboration sites, and any other information sharing services or websites for both personal and professional use.\n",
      "==================================================\n",
      "Given its widespread acceptance, responsible personal and corporate social media use is expected from all our Astellas Established Markets employees and non-employees to maintain the highest professional standards and avoid potentially significant legal, regulatory and reputational risk.\n",
      "==================================================\n",
      "Please refer toStandard Operating Procedure1921 Social Media Management for Established Markets and CP-GL-sample operations-policy-0034 Global Policy on Social Media for guidance on professional and personal use of social media.\n"
     ]
    }
   ],
   "source": [
    "for x in para_text_table_list_10:\n",
    "    print('='*50)\n",
    "    print(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "createdOn": 1663730858677,
  "creator": "schettiath",
  "customFields": {},
  "hide_input": false,
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  },
  "modifiedBy": "schettiath",
  "tags": []
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
