{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Markov Chain Monte Carlo (MCMC) simulation in __INPUT_DATASET_SMART_NAME__\n",
    "\n",
    "In the Bayesian methodology, the observations from a dataset are considered to be generated by a random process, which itself is parametrized by random variables. The goal of *Bayesian inference* is then to produce an estimate of the *posterior probability distribution* of those parameters.\n",
    "\n",
    "In this notebook, we are going to select univariate observations from a dataset in the Flow, and leverage **Markov Chain Monte Carlo (MCMC)** in order to apply a Bayesian inference analysis on them. We leverage the capabilities of the pymc3 Python package, that allows to cleanly define the inference process and provides multiple sampling options.\n",
    "\n",
    "Even though our use of Monte-Carlo methods (and Bayesian inference) here is mostly targeting a \"generic\" analysis, it can also be used within a broad range of specialized/targeted applications, such as survival analysis for churn prevention or unsupervised clustering for segmentation under uncertainty."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'numpy.distutils'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-4-c5b981746d88>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mnumpy\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mmath\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 5\u001b[1;33m \u001b[1;32mimport\u001b[0m \u001b[0mpymc3\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mpm\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      6\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mmatplotlib\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpyplot\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mplt\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      7\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mscipy\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstats\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Roaming\\Python\\Python38\\site-packages\\pymc3\\__init__.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     21\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mwarnings\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     22\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 23\u001b[1;33m \u001b[1;32mimport\u001b[0m \u001b[0mnumpy\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdistutils\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     24\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0msemver\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     25\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'numpy.distutils'"
     ]
    }
   ],
   "source": [
    "%matplotlib inline\n",
    "\n",
    "import numpy as np\n",
    "import math\n",
    "import pymc3 as pm\n",
    "import matplotlib.pyplot as plt\n",
    "import scipy.stats\n",
    "import warnings\n",
    "\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Selecting the observation vector\n",
    "\n",
    "The first task is to extract the observations of interest from our input dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Choose the column containing the observations:\n",
    "obs_column = \"Customers\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "my_dataset = dataiku.Dataset(\"__INPUT_DATASET_SMART_NAME__\")\n",
    "dataset_columns = [x[\"name\"] for x in my_dataset.read_schema()]\n",
    "if obs_column not in dataset_columns:\n",
    "    raise ValueError(\"Column {} does not exist\".format(obs_column))\n",
    "else:\n",
    "    obs = my_dataset.get_dataframe(columns=[obs_column]).values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we have our observation vector, we can take a look at its histogram to get a first overview of its distribution."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plt.figure(figsize=(8,7))\n",
    "plt.title(\"{} - observed distribution\".format(obs_column))\n",
    "plt.xlabel(obs_column)\n",
    "plt.hist(obs, bins=64);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Defining and running the probabilistic model\n",
    "\n",
    "After assessing the distribution of the observations, you can now make an assumption about the nature of the random process that generated those observations. \n",
    "\n",
    "* In practice, we need to associate this process with a statistical distribution and its associated parameters: the probability of the observations given those parameters is called the *likelihood*. \n",
    "\n",
    "* In the Bayesian context, we consider those parameters to be random variables: the end goal is to obtain their *posterior probability distribution*, namely their probability conditioned by the observations. \n",
    "\n",
    "* To do so, we need to define the *prior distributions* of the parameters, which act as first-guess provided by the user about their statistical behaviour.\n",
    "\n",
    "* We will then run a *sampler* which will iteratively:\n",
    "    * draw values from the parameter priors,\n",
    "    * compute their likelihood given the observations,\n",
    "    * apply Bayes theorem to get a probability value for their posterior distribution,\n",
    "    * optimize the sampler parameters to guide it towards areas of higher posterior probability."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###  Likelihood and prior\n",
    "\n",
    "The first item to define is the likelihood function. The Gaussian distribution (`normal`) often fits well to describe observations, but there are also some cases where specific distributions are more appropriated, e.g. a Poisson distribution (`poisson`) for counts.\n",
    "\n",
    "* You will have to define the likelihood parameters `lk_params` directly within the context manager (inside the `with` statement) before launching the sampler.\n",
    "\n",
    "* If the type of likelihood you are looking for is not in the `lk_options` dictionary from the next cell, you can add a new entry by choosing from [all the different distributions available from pymc3](https://docs.pymc.io/api/distributions.html)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Select the desired type of likelihood (must be part of lk_options dictionary!) \n",
    "lk_type = \"normal\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Append additional likelihood shortcuts here:\n",
    "lk_options = {\"poisson\": pm.Poisson,\n",
    "              \"normal\": pm.Normal}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following item to define is a prior distribution for each of the likelihood's parameters. The choice of priors is a vast topic that we simplify here by choosing a uniform distribution, which boundaries you can specify as being the most coherent with your use-case.\n",
    "\n",
    "* If you are working with a Gaussian likelihood, you should also set up the `prior_sigma` parameter, which is the asserted value of the standard deviation. The simplest case is to assume a fixed value that reflects the potential spread of your observations around the inferred mean."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Put prior hyperparameters here:\n",
    "unif_lower = 0\n",
    "unif_upper = 2000\n",
    "\n",
    "# For a Gaussian likelihood:\n",
    "prior_sigma = 1000"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###  Simulation parameters\n",
    "\n",
    "The next step is to define the simulation parameters used by the sampler, as well as the sampler type.\n",
    "\n",
    "A sampler is a given methodology for exploring the space of parameters and optimize that search for maximizing the posterior probability. In the case of MCMC, simply put, you progressively build a Markov chain that has the posterior distribution of the parameters as its equilibrium distribution.\n",
    "\n",
    "There are several important parameters to set up for the sampler, the main ones are :\n",
    "* the number of Markov chains to run simultaneously (`n_chains`): it is important to build at least 2 different chains that will validate the robustness of the sampling outcome by displaying similar results\n",
    "* the number of draws per chain to be performed (`n_draws`): the sampler should do enough iterations for the Markov chains to be able to properly converge towards their stationary distribution. However, too many iterations may also lead to a slow computation, especially if the likelihood is costly to evaluate at each iteration.\n",
    "* the algorithm used for sampling (`sampler_type`): there are numerous methods available to perform MCMC, the most common and generic ones in the pymc3 package are the *Metropolis-Hastings* (`metropolis`) and the *No-U-Turn sampler* (`nuts`).\n",
    "\n",
    "If the type of sampler you are looking for is not in the `sampler_options` dictionary from the next cell, you can add a new entry by choosing from [all the different samplers available from pymc3](https://docs.pymc.io/api/inference.html#step-methods)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sampler_options = {\"metropolis\": pm.Metropolis,\n",
    "                   \"nuts\": pm.NUTS}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_chains = 3 # nb of independent Markov chains to be run in total\n",
    "n_draws = 100000 # nb of samples to draw per Markov chain\n",
    "\n",
    "sampler_type = \"metropolis\" "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The final step is then to create a *model* instanciated in the form of a context manager where you will define all the elements of your probabilistic model.\n",
    "\n",
    "**Make sure to properly define the prior and `lk_params` according to your desired probabilistic model.**\n",
    "\n",
    "Once you execute the next cell, the simuation will start running, and depending on the data and the simulation parameters, it can take a decent amount of time."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with pm.Model() as my_model:\n",
    "    # Define the parameter(s) to perform inference on:\n",
    "    mu_lk = pm.Uniform('mu_lk', lower=unif_lower, upper=unif_upper)\n",
    "    \n",
    "    # Define the likelihood parameters:\n",
    "    lk_params = {\"mu\": mu_lk, \"sd\": prior_sigma}\n",
    "    # Additional keys to reference variable type and observations (do not change!)\n",
    "    lk_params[\"name\"] = \"likelihood\"\n",
    "    lk_params[\"observed\"] = obs\n",
    "    # Instanciate actual likelihood distribution:\n",
    "    likelihood = lk_options[lk_type](**lk_params)\n",
    "    print(\"Chosen likelihood is {}\".format(lk_type))\n",
    "    \n",
    "    # Initialize the sampler at the maximum a posteriori (MAP) for faster convergence:\n",
    "    start = pm.find_MAP()\n",
    "    \n",
    "    # Enforce sampler type and options:\n",
    "    step = sampler_options[sampler_type]()\n",
    "    \n",
    "    # Launch sampling and store output results in the trace variable:\n",
    "    trace = pm.sample(draws=n_draws,\n",
    "                      chains=n_chains,\n",
    "                      start=start,\n",
    "                      step=step)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Displaying and analyzing sampling results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Summary statistics\n",
    "\n",
    "A quick way of looking at the results of the sampling operation is to produce a small DataFrame that contains a set of summary statistics, some of them being especially useful to assess convergence:\n",
    "\n",
    "- `Rhat` shoud be <1.05\n",
    "- `n_eff` is the *effective sample size* and reflects the number of effective number of simulation draws that are truly independent: the higher, the better."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pm.summary(trace, varnames=['mu_lk'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Trace frequency and values\n",
    "\n",
    "A first interesting insight is the trace's values, namely the values drawn during sampling that reflect the target posterior distribution.\n",
    "\n",
    "For each variable to be estimated in our probabilistic model, the `traceplot` function provides two kinds of plots:\n",
    "\n",
    "* a *frequency plot* (left) that shows the output of a kernel density estimation (KDE) operation on the drawn samples for each chain. You should check that all chains follow the same trend and that the estimated density is unimodal. If the latter is not verified, you may need to run more draws.\n",
    "\n",
    "* a series of values taken by every chain at each sampling step (right): you should check that all chains explore efficiently the search space (absence of stale sequences where the chain keeps the same value for a while)\n",
    "\n",
    "It is also a good practice to discard the first samples that are not representative of the target distribution we aim at estimating (a.k.a. *burn-in* samples)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_burnin = 1000\n",
    "\n",
    "pm.plots.traceplot(trace=trace,\n",
    "                   skip_first=n_burnin,\n",
    "                   varnames=[\"mu_lk\"],\n",
    "                   figsize=(10, 4));"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Posterior density histogram\n",
    "\n",
    "This detailed illustration of the drawn samples' distribution provides a good visual overview of the posterior distribution(s) as well as some statistical indicators."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pm.plots.plot_posterior(trace=trace[\"mu_lk\"],\n",
    "                        figsize=(10, 4))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Matching observations against inferred distribution\n",
    "\n",
    "Recall that our main goal is to provide a probabilistic estimation of the parameters that define the generative process from which the observations should be coming from. \n",
    "\n",
    "To verify the quality of our inference, we can use the average of the samples to produce a density estimation that we can overlay with the actual observations:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.hist(obs, bins=32, histtype=\"step\", normed=True, color=\"k\", lw=2, label=\"Observations\");\n",
    "\n",
    "n_pts_kde = 200\n",
    "color_kde = \"#A60628\"\n",
    "if lk_type==\"normal\":\n",
    "    x = np.linspace(np.min(obs), np.max(obs), n_pts_kde)\n",
    "    y = scipy.stats.norm.pdf(x, loc=trace[\"mu_lk\"].mean(axis=0), scale=prior_sigma)\n",
    "if lk_type==\"poisson\":\n",
    "    x = np.arange(1, int(np.floor(np.max(obs))))\n",
    "    y = scipy.stats.poisson.pmf(x, mu=np.floor(trace[\"mu_lk\"].mean(axis=0)))\n",
    "plt.plot(x, y, label=\"Inferred distribution\", lw=3, color=color_kde)\n",
    "plt.fill_between(x, y, color=color_kde, alpha=0.3)\n",
    "plt.legend(loc=\"upper right\")\n",
    "plt.title(\"Observation histogram vs inferred distribution (averaging over samples)\")\n",
    "plt.xlabel(obs_column);"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
